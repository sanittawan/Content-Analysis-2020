{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8-Deep-Learning-and-text-training-models-Sanittawan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "88d068d97a5a4695af0b099d3f5b7986": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f7089aaeb35341819d6f5bb2608d7ebd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a00d8bc4d184484fb8f906767823ab2d",
              "IPY_MODEL_9a0fa8d6aa924bbfa7a78bd15eb3ffd6"
            ]
          }
        },
        "f7089aaeb35341819d6f5bb2608d7ebd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a00d8bc4d184484fb8f906767823ab2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7dbcb2d159eb4eccbd38f4537bacfb49",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_21fe0c5655334ea8b01df711ee812c4e"
          }
        },
        "9a0fa8d6aa924bbfa7a78bd15eb3ffd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3d4a57b6ff39421eb879e3de6e315b72",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 232k/232k [00:00&lt;00:00, 416kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eb01df2353ae4151816788ba5061ecc6"
          }
        },
        "7dbcb2d159eb4eccbd38f4537bacfb49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "21fe0c5655334ea8b01df711ee812c4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3d4a57b6ff39421eb879e3de6e315b72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eb01df2353ae4151816788ba5061ecc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "851aa99fede8479da11f89b2f9bdf99a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7a7e6141211e42119f35a714eb018513",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ca951d3b2a4a4837b5e1b7a60f797d50",
              "IPY_MODEL_f95bf9b8e3e64a48b7ff741241c8a3a9"
            ]
          }
        },
        "7a7e6141211e42119f35a714eb018513": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ca951d3b2a4a4837b5e1b7a60f797d50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_14052019c7fa4b7bad5d0e4bc0499122",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_91ec94c24a7e445193a73bfbe9b59efe"
          }
        },
        "f95bf9b8e3e64a48b7ff741241c8a3a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6f39d35827e54d37a0697df0f2e1ae93",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 361/361 [00:00&lt;00:00, 11.4kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_78e4d4b13e6841b48c12ed78c66446ec"
          }
        },
        "14052019c7fa4b7bad5d0e4bc0499122": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "91ec94c24a7e445193a73bfbe9b59efe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6f39d35827e54d37a0697df0f2e1ae93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "78e4d4b13e6841b48c12ed78c66446ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6ea074c2b0ca46adbe9e8292e003305c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_de4b41c394db4a14bbcb158593dbcc0d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_49f80788540247b48067b440b9a6f952",
              "IPY_MODEL_36fb10cc72894307929d82b451b8b046"
            ]
          }
        },
        "de4b41c394db4a14bbcb158593dbcc0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "49f80788540247b48067b440b9a6f952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9c1d9fe4a4dd480fb8aa37701dea1d51",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6a82dbc89d8c40ee9666676cfc830e01"
          }
        },
        "36fb10cc72894307929d82b451b8b046": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cb08745838f443dab07f27560b880dd0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 440M/440M [00:36&lt;00:00, 12.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bd39497b66e74cb4aca27db9fb659398"
          }
        },
        "9c1d9fe4a4dd480fb8aa37701dea1d51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6a82dbc89d8c40ee9666676cfc830e01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cb08745838f443dab07f27560b880dd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bd39497b66e74cb4aca27db9fb659398": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "95da6890cc7e425c9fecd7743da62efb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_325b874051bd45af952f4c86cb8c77cf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_034351173a33494d8690a8f79f47e573",
              "IPY_MODEL_3d9f6d9f71294d81ae39397d9c8fa707"
            ]
          }
        },
        "325b874051bd45af952f4c86cb8c77cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "034351173a33494d8690a8f79f47e573": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3e43f498a1134f278e688b90e42899d8",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d0a9b17471984263ad7749052a094dcc"
          }
        },
        "3d9f6d9f71294d81ae39397d9c8fa707": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_420642c18ea64f84983a6b326fa5325b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 361/361 [00:00&lt;00:00, 19.3kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ec5d3111b2b64706ad3f2164a0eebad3"
          }
        },
        "3e43f498a1134f278e688b90e42899d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d0a9b17471984263ad7749052a094dcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "420642c18ea64f84983a6b326fa5325b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ec5d3111b2b64706ad3f2164a0eebad3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a34dc2a44c0e45b7b5d2d0dde9795e86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1a73743058334dd4bcb830be1675904f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_00494cae92194b8b8fc17f6542fc5a76",
              "IPY_MODEL_82573624322547e393e19f0082f6f9a0"
            ]
          }
        },
        "1a73743058334dd4bcb830be1675904f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "00494cae92194b8b8fc17f6542fc5a76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8988a63920f446c0abb78613fc03690a",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1138bfec334842c3b690d045777e89a3"
          }
        },
        "82573624322547e393e19f0082f6f9a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d45ca511348841448ff16fa04def10c0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 440M/440M [00:05&lt;00:00, 83.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_33620f0d00a54e14908bd99966313090"
          }
        },
        "8988a63920f446c0abb78613fc03690a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1138bfec334842c3b690d045777e89a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d45ca511348841448ff16fa04def10c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "33620f0d00a54e14908bd99966313090": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6f91403349c945e9be020bcde378a8e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c1b627b4d0e84080b47784e9ab4927c9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b2362cfa61cf4c689d708eb121d0a249",
              "IPY_MODEL_103b5442d96f4b62bbc421ce6aaa044a"
            ]
          }
        },
        "c1b627b4d0e84080b47784e9ab4927c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b2362cfa61cf4c689d708eb121d0a249": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7152e720a6b24caf80a56343009da5dc",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 230,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 230,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eacc819057db4022961696a2234fcde9"
          }
        },
        "103b5442d96f4b62bbc421ce6aaa044a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_edee39b4bd544b8895df85911c5b8b7b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 230/230 [00:00&lt;00:00, 11.5kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8d27b7b1332a4fa5b42563de21d3287d"
          }
        },
        "7152e720a6b24caf80a56343009da5dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eacc819057db4022961696a2234fcde9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "edee39b4bd544b8895df85911c5b8b7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8d27b7b1332a4fa5b42563de21d3287d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fd7c86a8aca44dffb94f2eda3c2c40b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d2e3bf5406df40f7b0201151d0f2b9f1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_af94522f0fb143f58eddeb53e7ef2d6c",
              "IPY_MODEL_d51fef7b9b184a489c25f108474c0d3c"
            ]
          }
        },
        "d2e3bf5406df40f7b0201151d0f2b9f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "af94522f0fb143f58eddeb53e7ef2d6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1dc21f23432249f08833a328af71ab32",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 224,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 224,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ad73acf1dbfe47d9b39fe13d78e31c69"
          }
        },
        "d51fef7b9b184a489c25f108474c0d3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_10f78ecba47a446da73b454d8ae72b84",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 224/224 [00:00&lt;00:00, 10.8kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_60bc63df83ca40fb98cc222164d4b424"
          }
        },
        "1dc21f23432249f08833a328af71ab32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ad73acf1dbfe47d9b39fe13d78e31c69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "10f78ecba47a446da73b454d8ae72b84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "60bc63df83ca40fb98cc222164d4b424": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4455246b7dc24704a3b6789fafa3044d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_db1aa3a044eb4730b18bd783a3e0c127",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_72f36f33d1984ccdb248f54dbc5b227e",
              "IPY_MODEL_62aeb102bca4452fbaff5178c73877a9"
            ]
          }
        },
        "db1aa3a044eb4730b18bd783a3e0c127": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "72f36f33d1984ccdb248f54dbc5b227e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_915231ce2268490c90e597ac4b937cfe",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1042301,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1042301,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d69f3d0480844487aad39c7de34e6e53"
          }
        },
        "62aeb102bca4452fbaff5178c73877a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d4a9236ed02e49148f3bb1cbe381606f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 1.04M/1.04M [00:00&lt;00:00, 14.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f864a1b6db9b4f4e8ad05adf2762a541"
          }
        },
        "915231ce2268490c90e597ac4b937cfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d69f3d0480844487aad39c7de34e6e53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d4a9236ed02e49148f3bb1cbe381606f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f864a1b6db9b4f4e8ad05adf2762a541": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "666e2cb8186044268bc4195ccbc5b41e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7abcaec5113942a9818bfff825824065",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_97d600b6b100496f813e2ad38bc87cbb",
              "IPY_MODEL_e0cd43f5c050416f8440db5633b01d86"
            ]
          }
        },
        "7abcaec5113942a9818bfff825824065": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "97d600b6b100496f813e2ad38bc87cbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3f2cf2ec95ff42bc8f6937465bbfb7d0",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_939073a0df6448b2909a1f96f2c92005"
          }
        },
        "e0cd43f5c050416f8440db5633b01d86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_02885d8580d641f6bce485e9f458db81",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 456k/456k [00:00&lt;00:00, 8.18MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_615835a3a1404a06bdb5e2639c2f8054"
          }
        },
        "3f2cf2ec95ff42bc8f6937465bbfb7d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "939073a0df6448b2909a1f96f2c92005": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "02885d8580d641f6bce485e9f458db81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "615835a3a1404a06bdb5e2639c2f8054": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e508b4ef18a9411182ceea6408539924": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ae009213a8e4494b96d18369eb6709d1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_be62fce520b44b0099ff34f8e0179e5a",
              "IPY_MODEL_3f4687568d064f8ebf11792d6d8837ee"
            ]
          }
        },
        "ae009213a8e4494b96d18369eb6709d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "be62fce520b44b0099ff34f8e0179e5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f16199604d2d4c7eb79dc22e7e159c69",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 548118077,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 548118077,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a2722aa955b34d71b4fe17c8227f1f9d"
          }
        },
        "3f4687568d064f8ebf11792d6d8837ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1cb57fe2c22e4170b7df11f758b5653e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 548M/548M [00:07&lt;00:00, 75.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_286a29d3a98c4cdba08e93e0b897e882"
          }
        },
        "f16199604d2d4c7eb79dc22e7e159c69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a2722aa955b34d71b4fe17c8227f1f9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1cb57fe2c22e4170b7df11f758b5653e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "286a29d3a98c4cdba08e93e0b897e882": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanittawan/CAhomework/blob/master/8_Deep_Learning_and_text_training_models_Sanittawan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nFkME77f1Zh",
        "colab_type": "text"
      },
      "source": [
        "## Week - 8 - Deep Neural Nets and Text - Training Models\n",
        "\n",
        "### Sanittawan Tan's notebook\n",
        "\n",
        "In this week we will be introduced to using Deep Neural Networks to work with text. We have already seen some uses of neural networks for text in our classification HW, where we used a simple neural network to classify text - it performs quite well, but they can come up short in more sophisticated classification tasks, such as in predicting intent. We have also seen neural nets in the form of word embeddings such as Word2Vec - and while they certainly work well, they have some drawbacks, such as dealing with words with multiple meanings. \n",
        "\n",
        "BERT, which is a language model built using bidirectional encoders, allows us to have a powerful pre-trained model which we can then use to perform our own tasks based on the data we are analysing. \n",
        "\n",
        "In this notebook we will use ```huggingface/transformers```, which is a python package which allows for an easy interface to use pre-trained BERT models. It is built using Tensorflow and PyTorch, two computational graph packages which are built specifically for creating powerful neural networks. We will also be introducing Keras, which allows us to easily build Neural Networks in an abstracted way. Keras is a popular way to understand how we can stack layers to create such Neural Networks, though to reach state-of-the-art results we will stick with using BERT and similar models.\n",
        "\n",
        "To demonstrate this, we will use the [Corpus of Linguistic Acceptability](https://nyu-mll.github.io/CoLA/). We will also be using BERT by learning how to extract embeddings from such a model and use it to semantically probe sentences.\n",
        "There are a bunch of new packages and methods we will be using so be sure to update lucem_illud_2020.\n",
        "\n",
        "Note that this notebook is different to take advantage of the GPU here. We only run the GPU heavy tasks here, where we need models to be fine-tuned.\n",
        "\n",
        "The first section contains the CoLA classification task, the second contains training a model on Trump tweets, and the last bit has us training a model on US and UK blog posts.\n",
        "\n",
        "To switch on your GPU, go to edit -> notebook settings -> hardware accelerator -> enable GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTCP80HqWCZs",
        "colab_type": "code",
        "outputId": "e359b185-5b9b-4860-b2f2-2ffaccaea496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DkVY2HIWRPC",
        "colab_type": "code",
        "outputId": "48ef845c-58a8-43bd-b411-079e35ed0fc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vzt0EKCqWVsN",
        "colab_type": "code",
        "outputId": "ef302c21-c78d-4e68-8dfc-0583146d3063",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 501kB 2.7MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.7MB 52.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.0MB 39.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 870kB 35.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=4e1103f94decff31bae17ae72825586dde164873595b9ff36eaab41661f52277\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pwp7W3BWWERu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from transformers import AdamW, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPbf8Yvyf-Hn",
        "colab_type": "text"
      },
      "source": [
        "We'll now load the CoLA dataset up here to Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZH0HWkNgbho",
        "colab_type": "code",
        "outputId": "521016f8-8656-4f1f-c6b2-49534ac9af8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=90f643eb7815e4ea9d3453f9248566ae33818931de94f83ee468d582bc4ae116\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRqSO7DUWXmR",
        "colab_type": "code",
        "outputId": "4688c3e1-a5eb-436f-b0fa-fe1687d7671b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "print('Downloading dataset...')\n",
        "\n",
        "# The URL for the dataset zip file.\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "\n",
        "# Download the file (if we haven't already)\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "    wget.download(url, './cola_public_1.1.zip')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dewGhdzgEkH",
        "colab_type": "code",
        "outputId": "4bbf314e-aade-419e-b41e-e068fc9f4689",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Unzip the dataset (if we haven't already)\n",
        "if not os.path.exists('./cola_public/'):\n",
        "    !unzip cola_public_1.1.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  cola_public_1.1.zip\n",
            "   creating: cola_public/\n",
            "  inflating: cola_public/README      \n",
            "   creating: cola_public/tokenized/\n",
            "  inflating: cola_public/tokenized/in_domain_dev.tsv  \n",
            "  inflating: cola_public/tokenized/in_domain_train.tsv  \n",
            "  inflating: cola_public/tokenized/out_of_domain_dev.tsv  \n",
            "   creating: cola_public/raw/\n",
            "  inflating: cola_public/raw/in_domain_dev.tsv  \n",
            "  inflating: cola_public/raw/in_domain_train.tsv  \n",
            "  inflating: cola_public/raw/out_of_domain_dev.tsv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzyIo3XfgVxJ",
        "colab_type": "code",
        "outputId": "fff628b7-84d4-4095-8f42-051adfaf1465",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 8,551\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3128</th>\n",
              "      <td>l-93</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Paul yawned at Mary.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4609</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>John has taken Bill to the library.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5135</th>\n",
              "      <td>ks08</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>John that we are looking for showed up.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1532</th>\n",
              "      <td>r-67</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Mary scratched her arm too.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6541</th>\n",
              "      <td>g_81</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Who did you wonder saw Kim?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1734</th>\n",
              "      <td>r-67</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>They said that Tom would pay up, and pay up he...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8391</th>\n",
              "      <td>ad03</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>I thought she is pregnant</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2011</th>\n",
              "      <td>rhl07</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Fred kicked the ball over the fence.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1788</th>\n",
              "      <td>r-67</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>This rock is too heavy for me to begin to deci...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>cj99</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>I don't plan to lock the door, no matter how f...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "3128            l-93  ...                               Paul yawned at Mary.\n",
              "4609            ks08  ...                John has taken Bill to the library.\n",
              "5135            ks08  ...            John that we are looking for showed up.\n",
              "1532            r-67  ...                        Mary scratched her arm too.\n",
              "6541            g_81  ...                        Who did you wonder saw Kim?\n",
              "1734            r-67  ...  They said that Tom would pay up, and pay up he...\n",
              "8391            ad03  ...                          I thought she is pregnant\n",
              "2011           rhl07  ...               Fred kicked the ball over the fence.\n",
              "1788            r-67  ...  This rock is too heavy for me to begin to deci...\n",
              "179             cj99  ...  I don't plan to lock the door, no matter how f...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onaOGdjxgiyG",
        "colab_type": "code",
        "outputId": "fd00408f-1225-4dcb-aff3-97338acc0bd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4753</th>\n",
              "      <td>To which man did you talk to?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1214</th>\n",
              "      <td>Rutherford is understood. by himself.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>533</th>\n",
              "      <td>Fruit hit the roof against the ground.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2724</th>\n",
              "      <td>We contributed her our paycheck.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4474</th>\n",
              "      <td>I do not be happy.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    sentence  label\n",
              "4753           To which man did you talk to?      0\n",
              "1214   Rutherford is understood. by himself.      0\n",
              "533   Fruit hit the roof against the ground.      0\n",
              "2724        We contributed her our paycheck.      0\n",
              "4474                      I do not be happy.      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saNk183jgjVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = df.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOLRljp3gr9T",
        "colab_type": "code",
        "outputId": "1cec8959-fd38-442f-be9e-3f23a87322f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "88d068d97a5a4695af0b099d3f5b7986",
            "f7089aaeb35341819d6f5bb2608d7ebd",
            "a00d8bc4d184484fb8f906767823ab2d",
            "9a0fa8d6aa924bbfa7a78bd15eb3ffd6",
            "7dbcb2d159eb4eccbd38f4537bacfb49",
            "21fe0c5655334ea8b01df711ee812c4e",
            "3d4a57b6ff39421eb879e3de6e315b72",
            "eb01df2353ae4151816788ba5061ecc6"
          ]
        }
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88d068d97a5a4695af0b099d3f5b7986",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_widâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Tokenize the first sentence:\n",
            "['[CLS]', 'our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wywdbBISguFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfXhgejEgxRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko6fHIlAiHm7",
        "colab_type": "code",
        "outputId": "eb3689e0-763c-4e5b-faa1-fcc66ac948ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSaU7DbkgzWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEUBxUm8g0ov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4QPyBRDiIx1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2020, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2020, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUSKUh9JHiRc",
        "colab_type": "text"
      },
      "source": [
        "### Introducing Deep Neural Nets\n",
        "\n",
        "A popular, simplified package for introducing deep neural networks is [Keras](https://keras.io). It is a high level package in that we don't bother with every detail or hyper-parameter associated with the neural network (e.g., regularizers), and can stack on layers directly. For a rapid tutorial on neural networks for text such as the LSTM or the Recurrent Neural Network, Colah's blog is a great start. [LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) is an article on LSTMs, and if you'd like to  learn about RNN, Andrej Karpathy does a great job in [this blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), in addition to our reading from the newest online verion of Jurafsky & Martin's review of deep learning methods in their book on speech and language processing, chapters [6,7,9,10](https://web.stanford.edu/~jurafsky/slp3/), and the [*Deep Learning*](https://www.deeplearningbook.org/) book by Goodfellow, Bengio & Courville.\n",
        "\n",
        "In the following cells we build a basic deep net that has an embedding layer and an LSTM to perform classification. This is to illustrate the process of using Keras, which is a very popular library for such work. It may not yield state of the art performance because it constrains the hyperparameters you can tune, but is nonetheless an useful tool and works well on some datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWLfSUPjiLJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ0A0HbAiMpc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_in_size = tokenizer.vocab_size\n",
        "embedding_dim = 32\n",
        "unit = 100\n",
        "no_labels = len(np.unique(train_labels))\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POlslC2AiPAI",
        "colab_type": "code",
        "outputId": "8b0cac03-ef35-4829-9b91-ddf90292ae93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
        "model_lstm.add(LSTM(unit))\n",
        "model_lstm.add(Dense(no_labels, activation='softmax'))\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_lstm.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 128, 32)           976704    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 1,030,106\n",
            "Trainable params: 1,030,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu2c6CWSiP3D",
        "colab_type": "code",
        "outputId": "e961d659-ba6b-4c39-f12c-575999815c2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "history_lstm = model_lstm.fit(train_inputs, train_labels, \n",
        "                              epochs=10,batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "7695/7695 [==============================] - 47s 6ms/step - loss: 0.6081 - acc: 0.7038\n",
            "Epoch 2/10\n",
            "7695/7695 [==============================] - 47s 6ms/step - loss: 0.6084 - acc: 0.7038\n",
            "Epoch 3/10\n",
            "7695/7695 [==============================] - 47s 6ms/step - loss: 0.6086 - acc: 0.7038\n",
            "Epoch 4/10\n",
            "7695/7695 [==============================] - 47s 6ms/step - loss: 0.6084 - acc: 0.7038\n",
            "Epoch 5/10\n",
            "7695/7695 [==============================] - 46s 6ms/step - loss: 0.6083 - acc: 0.7038\n",
            "Epoch 6/10\n",
            "7695/7695 [==============================] - 46s 6ms/step - loss: 0.6080 - acc: 0.7038\n",
            "Epoch 7/10\n",
            "7695/7695 [==============================] - 47s 6ms/step - loss: 0.6083 - acc: 0.7038\n",
            "Epoch 8/10\n",
            "7695/7695 [==============================] - 46s 6ms/step - loss: 0.6084 - acc: 0.7038\n",
            "Epoch 9/10\n",
            "7695/7695 [==============================] - 47s 6ms/step - loss: 0.6081 - acc: 0.7038\n",
            "Epoch 10/10\n",
            "7695/7695 [==============================] - 46s 6ms/step - loss: 0.6081 - acc: 0.7038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMYHpnVmJcer",
        "colab_type": "code",
        "outputId": "94db26eb-7b02-459f-f817-7ef5309a1420",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "model_lstm2 = Sequential()\n",
        "model_lstm2.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
        "model_lstm2.add(LSTM(32))\n",
        "model_lstm2.add(LSTM(32))\n",
        "model_lstm2.add(Dense(1, activation='sigmoid'))\n",
        "model_lstm2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "history_lstm2 = model_lstm2.fit(train_inputs, train_labels, epochs=10, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "7695/7695 [==============================] - 49s 6ms/step - loss: 0.6143 - acc: 0.7025\n",
            "Epoch 2/10\n",
            "7695/7695 [==============================] - 48s 6ms/step - loss: 0.6084 - acc: 0.7038\n",
            "Epoch 3/10\n",
            "7695/7695 [==============================] - 48s 6ms/step - loss: 0.6087 - acc: 0.7038\n",
            "Epoch 4/10\n",
            "7695/7695 [==============================] - 48s 6ms/step - loss: 0.6089 - acc: 0.7038\n",
            "Epoch 5/10\n",
            "7695/7695 [==============================] - 47s 6ms/step - loss: 0.6083 - acc: 0.7038\n",
            "Epoch 6/10\n",
            "7695/7695 [==============================] - 47s 6ms/step - loss: 0.6086 - acc: 0.7038\n",
            "Epoch 7/10\n",
            "7695/7695 [==============================] - 47s 6ms/step - loss: 0.6085 - acc: 0.7038\n",
            "Epoch 8/10\n",
            "7695/7695 [==============================] - 47s 6ms/step - loss: 0.6084 - acc: 0.7038\n",
            "Epoch 9/10\n",
            "7695/7695 [==============================] - 47s 6ms/step - loss: 0.6083 - acc: 0.7038\n",
            "Epoch 10/10\n",
            "7695/7695 [==============================] - 47s 6ms/step - loss: 0.6081 - acc: 0.7038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6muAlDeiac7",
        "colab_type": "text"
      },
      "source": [
        "### On with BERT!\n",
        "\n",
        "So while Neural Networks can do a good job with some kind of classification tasks, they don't perform too well on intent classification. Let us see how BERT might do. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJDljt39iddQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgmJ-E_VigTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX6t1SIUi22s",
        "colab_type": "text"
      },
      "source": [
        "## Loading our Models\n",
        "\n",
        "### Train Model\n",
        "Now that our input data is properly formatted, it's time to fine tune the BERT model.\n",
        "For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task. Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.\n",
        "We'll load BertForSequenceClassification. This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
        "\n",
        "### Structure of Fine-Tuning Model\n",
        "\n",
        "As we've showed beforehand, the first token of every sequence is the special classification token ([CLS]). Unlike the hidden state vector corresponding to a normal word token, the hidden state corresponding to this special token is designated by the authors of BERT as an aggregate representation of the whole sentence used for classification tasks. As such, when we feed in an input sentence to our model during training, the output is the length 768 hidden state vector corresponding to this token. The additional layer that we've added on top consists of untrained linear neurons of size [hidden_state, number_of_labels], so [768,2], meaning that the output of BERT plus our classification layer is a vector of two numbers representing the \"score\" for \"grammatical/non-grammatical\" that are then fed into cross-entropy loss.\n",
        "\n",
        "### The Fine-Tuning Process\n",
        "\n",
        "Because the pre-trained BERT layers already encode a lot of information about the language, training the classifier is relatively inexpensive. Rather than training every layer in a large model from scratch, it's as if we have already trained the bottom layers 95% of where they need to be, and only really need to train the top layer, with a bit of tweaking going on in the lower levels to accomodate our task.\n",
        "Sometimes practicioners will opt to \"freeze\" certain layers when fine-tuning, or to apply different learning rates, apply diminishing learning rates, etc. all in an effort to preserve the good quality weights in the network and speed up training (often considerably). In fact, recent research on BERT specifically has demonstrated that freezing the majority of the weights results in only minimal accuracy declines, but there are exceptions and broader rules of transfer learning that should also be considered. For example, if your task and fine-tuning dataset is very different from the dataset used to train the transfer learning model, freezing the weights may not be a good idea. We'll cover the broader scope of transfer learning in NLP in a future post.\n",
        "OK, let's load BERT! There are a few different pre-trained BERT models available. \"bert-base-uncased\" means the version that has only lowercase letters (\"uncased\") and is the smaller version of the two (\"base\" vs \"large\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYkR2tJxiiJk",
        "colab_type": "code",
        "outputId": "a2814973-2040-4726-f955-7612578c6759",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "851aa99fede8479da11f89b2f9bdf99a",
            "7a7e6141211e42119f35a714eb018513",
            "ca951d3b2a4a4837b5e1b7a60f797d50",
            "f95bf9b8e3e64a48b7ff741241c8a3a9",
            "14052019c7fa4b7bad5d0e4bc0499122",
            "91ec94c24a7e445193a73bfbe9b59efe",
            "6f39d35827e54d37a0697df0f2e1ae93",
            "78e4d4b13e6841b48c12ed78c66446ec",
            "6ea074c2b0ca46adbe9e8292e003305c",
            "de4b41c394db4a14bbcb158593dbcc0d",
            "49f80788540247b48067b440b9a6f952",
            "36fb10cc72894307929d82b451b8b046",
            "9c1d9fe4a4dd480fb8aa37701dea1d51",
            "6a82dbc89d8c40ee9666676cfc830e01",
            "cb08745838f443dab07f27560b880dd0",
            "bd39497b66e74cb4aca27db9fb659398"
          ]
        }
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "851aa99fede8479da11f89b2f9bdf99a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=361, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ea074c2b0ca46adbe9e8292e003305c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfwPyoh9ii0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "likqi1-FimRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJcXKErgioEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-fvXJW_ip1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtCJt0A0irEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnwi-bj1itbL",
        "colab_type": "code",
        "outputId": "d9d2658b-c5dc-4c16-ae76-2ea378ee27cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:30.\n",
            "  Batch    80  of    241.    Elapsed: 0:01:00.\n",
            "  Batch   120  of    241.    Elapsed: 0:01:30.\n",
            "  Batch   160  of    241.    Elapsed: 0:02:01.\n",
            "  Batch   200  of    241.    Elapsed: 0:02:32.\n",
            "  Batch   240  of    241.    Elapsed: 0:03:02.\n",
            "\n",
            "  Average training loss: 0.50\n",
            "  Training epcoh took: 0:03:03\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.83\n",
            "  Validation took: 0:00:07\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:31.\n",
            "  Batch    80  of    241.    Elapsed: 0:01:01.\n",
            "  Batch   120  of    241.    Elapsed: 0:01:32.\n",
            "  Batch   160  of    241.    Elapsed: 0:02:02.\n",
            "  Batch   200  of    241.    Elapsed: 0:02:33.\n",
            "  Batch   240  of    241.    Elapsed: 0:03:04.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epcoh took: 0:03:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:07\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:31.\n",
            "  Batch    80  of    241.    Elapsed: 0:01:01.\n",
            "  Batch   120  of    241.    Elapsed: 0:01:32.\n",
            "  Batch   160  of    241.    Elapsed: 0:02:02.\n",
            "  Batch   200  of    241.    Elapsed: 0:02:33.\n",
            "  Batch   240  of    241.    Elapsed: 0:03:04.\n",
            "\n",
            "  Average training loss: 0.20\n",
            "  Training epcoh took: 0:03:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation took: 0:00:07\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:31.\n",
            "  Batch    80  of    241.    Elapsed: 0:01:01.\n",
            "  Batch   120  of    241.    Elapsed: 0:01:32.\n",
            "  Batch   160  of    241.    Elapsed: 0:02:03.\n",
            "  Batch   200  of    241.    Elapsed: 0:02:34.\n",
            "  Batch   240  of    241.    Elapsed: 0:03:04.\n",
            "\n",
            "  Average training loss: 0.14\n",
            "  Training epcoh took: 0:03:05\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:07\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZPeyGKYtoCn",
        "colab_type": "code",
        "outputId": "f062da5c-cb42-4f3d-8bd8-a3b4fd9b9004",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "loss_values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.49987454996811403,\n",
              " 0.3096145743280031,\n",
              " 0.199634012762318,\n",
              " 0.14039200691977724]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFogz7jeiycc",
        "colab_type": "code",
        "outputId": "e5425419-ffa0-4d66-af42-7a0142d2f115",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 516\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3S6GaadjK48",
        "colab_type": "code",
        "outputId": "06645ce0-262a-4a40-bdad-dee0876ca952",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 516 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_fqVB6CtzTo",
        "colab_type": "code",
        "outputId": "2dc41a02-41ce-42d8-cc79-f133df735546",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive samples: 354 of 516 (68.60%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIoseqqzt1pc",
        "colab_type": "code",
        "outputId": "2e7b9650-1edd-4e4d-b9f0-5219ff424e74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "\n",
        "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "    # in to a list of 0s and 1s.\n",
        "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "\n",
        "    # Calculate and store the coef for this batch.  \n",
        "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "    matthews_set.append(matthews)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BreKUJ9t25l",
        "colab_type": "code",
        "outputId": "82f99b68-ffa9-47c1-dab1-23507ee901e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "matthews_set"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.14856415213808927,\n",
              " -0.050964719143762556,\n",
              " 0.4732058754737091,\n",
              " 0.5096447293679518,\n",
              " 0.5945883900105632,\n",
              " 0.7410010097502685,\n",
              " 0.5555555555555556,\n",
              " 0.47519096331149147,\n",
              " 0.9165151389911681,\n",
              " 0.7704873741021288,\n",
              " 0.9229582069908973,\n",
              " 0.647150228929434,\n",
              " 0.8150678894028793,\n",
              " 0.647150228929434,\n",
              " 0.3268228676411533,\n",
              " 0.5716350506349809,\n",
              " 0.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20s0Poipt6XT",
        "colab_type": "code",
        "outputId": "772c7939-c335-49aa-84fd-84356a62bc71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpLhjm3D9-vA",
        "colab_type": "text"
      },
      "source": [
        "We now want to save this model to disk. Once you save it to disk, right click on the file and save it to your local computer to use it on your notebook. You would have maybe trained your model on your own dataset, in which case you would also need to upload your data, or load it using wget as we did before.\n",
        "\n",
        "NOTE: the files are accessible, both uploading and downloading, on the left hand side of the page, under the \"folder\" section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ1Y-b7-90S9",
        "colab_type": "code",
        "outputId": "56b521c5-3bdd-4ba2-9c47-68a3fbd64907",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to ./model_save/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./model_save/vocab.txt',\n",
              " './model_save/special_tokens_map.json',\n",
              " './model_save/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX_wIqvUP6zO",
        "colab_type": "text"
      },
      "source": [
        "## <span style=\"color:red\">*Exercise 1*</span>\n",
        "\n",
        "<span style=\"color:red\">Construct cells immediately below this that estimate a deep classification model with Keras (and LSTM) and also BERT in order to predict pre-established data labels relevant to your final project (as for week 3's homework). Which works better? Are the errors the same or different?\n",
        "\n",
        "<span style=\"color:red\">***Stretch***</span>: <span style=\"color:red\">Now alter the neural network by stacking network layers, adjusting the embedding dimension, compare its performance with your model above, and interpret why it might be different."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVhLm8QiQJ4x",
        "colab_type": "text"
      },
      "source": [
        "#### Load and Prepare Data\n",
        "\n",
        "Since the data for my final project does not involve classification as much, I am using a given data on Obama and Clinton press releases used in week 3 homework. I am going to classify these press releases if they belong to Obama or Clinton. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdKSiH5GQJl7",
        "colab_type": "code",
        "outputId": "20d03ceb-5f54-4d46-99c1-6f0b24e860bb",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-23210d91-8454-4bbc-9070-15c417312136\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-23210d91-8454-4bbc-9070-15c417312136\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving ObamaClintonReleases.csv to ObamaClintonReleases.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1l3pLQSSRBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "df = pd.read_csv(io.BytesIO(uploaded['ObamaClintonReleases.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JVKX0iMSYj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['label'] = [s == 'Obama' for s in df['targetSenator']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ildOrvpfVQaJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.rename(columns={\"text\": \"sentence\"}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQs9X2_5UJGK",
        "colab_type": "code",
        "outputId": "68100a5c-3b21-4c60-bcbf-cae81d48b1d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(df.shape)\n",
        "print(df.shape[0]*0.15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1709, 8)\n",
            "256.34999999999997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqdnpY7EVfMQ",
        "colab_type": "code",
        "outputId": "ea571852-89d7-4518-fdd8-aa35b852a31c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'download_url', 'html_url', 'name', 'path', 'sentence',\n",
              "       'targetSenator', 'label'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlhfnKwjSYmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a test set\n",
        "train_set = df.sample(frac=0.85, random_state=2020)\n",
        "test_set = df.drop(train_set.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-Bg0NfxSYp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_set.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNntzcU0UpIW",
        "colab_type": "code",
        "outputId": "4e1a8a98-26b5-4756-8076-89cbbe798611",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# check size\n",
        "print(train_set.shape)\n",
        "print(test_set.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1453, 8)\n",
            "(256, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_DkuiqxVws_",
        "colab_type": "code",
        "outputId": "6cfa2597-1064-4a9c-c681-29a35c6f537d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print(train_set.columns)\n",
        "print(test_set.columns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['Unnamed: 0', 'download_url', 'html_url', 'name', 'path', 'sentence',\n",
            "       'targetSenator', 'label'],\n",
            "      dtype='object')\n",
            "Index(['Unnamed: 0', 'download_url', 'html_url', 'name', 'path', 'sentence',\n",
            "       'targetSenator', 'label'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpMH_F0sUpOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use train set to split into train and validation set as in the example\n",
        "# begin with preprocessing\n",
        "\n",
        "train_df = train_set.copy()\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = train_df.sentence.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = train_df.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUq1od9jWBvI",
        "colab_type": "code",
        "outputId": "583b3e20-10fd-4e3f-e5da-afdb781d20a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "p_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "p_tokenized_texts = [p_tokenizer.tokenize(sent) for sent in sentences]\n",
        "print(\"Tokenize the first sentence:\")\n",
        "print(p_tokenized_texts[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'may', '12', '2005', 'senator', 'clinton', 'co', 'sponsors', 'legislation', 'to', 'ensure', 'homeland', 'security', 'funding', 'allocated', 'based', 'on', 'threat', 'level', 'legislation', 'a', 'good', 'first', 'step', 'toward', 'targeting', 'federal', 'dollars', 'to', 'highest', 'risk', 'communities', 'washington', 'dc', 'building', 'on', 'her', 'long', 'standing', 'commitment', 'to', 'ensuring', 'that', 'homeland', 'security', 'funding', 'is', 'allocated', 'based', 'on', 'threat', 'level', 'and', 'risk', 'senator', 'hillary', 'rod', '##ham', 'clinton', 'announced', 'that', 'she', 'is', 'co', 'sponsoring', 'legislation', 'introduced', 'today', 'to', 'require', 'that', 'the', 'federal', 'government', 'use', 'a', 'threat', 'based', 'formula', 'as', 'recommended', 'by', 'the', '9', '11', 'commission', 'the', 'bi', '##partisan', 'legislation', 'the', 'homeland', 'security', 'forward', 'funding', 'act', 'of', '2005', 'will', 'require', 'that', 'federal', 'dollars', 'be', 'directed', 'to', 'areas', 'at', 'highest', 'risk', 'for', 'a', 'terrorist', 'attack', 'and', 'where', 'an', 'attack', 'would', 'cause', 'the', 'most', 'damage', 'we', 'need', 'to', 'implement', 'threat', 'and', 'risk', 'based', 'funding', 'in', 'our', 'homeland', 'security', 'programs', 'said', 'senator', 'clinton', 'from', 'rail', 'safety', 'to', 'port', 'security', 'to', 'protecting', 'our', 'borders', 'we', 'have', 'much', 'work', 'to', 'do', 'but', 'we', 'should', 'all', 'be', 'able', 'to', 'agree', 'that', 'we', 'must', 'do', 'everything', 'we', 'can', 'to', 'safeguard', 'the', 'areas', 'at', 'highest', 'risk', 'the', 'bill', 'will', 'correct', 'in', '##ef', '##fi', '##ciency', 'in', 'current', 'law', 'that', 'distribute', '##s', 'scarce', 'homeland', 'security', 'funds', 'without', 'regard', 'to', 'actual', 'vu', '##ln', '##era', '##bilities', 'at', 'the', 'expense', 'of', 'communities', 'where', 'the', 'risk', 'of', 'terrorist', 'attacks', 'is', 'highest', 'the', 'legislation', 'also', 'will', 'help', 'speed', 'funding', 'directly', 'to', 'communities', 'by', 'requiring', 'that', 'states', 'distribute', '80', 'percent', 'of', 'the', 'money', 'they', 'receive', 'to', 'grant', '##ees', 'within', '45', 'days', 'states', 'that', 'fail', 'to', 'meet', 'these', 'time', 'requirements', 'can', 'face', 'a', 'reduction', 'in', 'funding', 'and', 'restrictions', 'on', 'use', 'and', 'grant', '##ees', 'in', 'these', 'states', 'would', 'be', 'allowed', 'to', 'petition', 'for', 'direct', 'funding', 'senator', 'clinton', 'has', 'repeatedly', 'urged', 'that', 'homeland', 'security', 'funds', 'be', 'distributed', 'quickly', 'and', 'efficiently', 'to', 'the', 'communities', 'that', 'need', 'them', 'she', 'has', 'introduced', 'legislation', 'the', 'domestic', 'defense', 'fund', 'of', '2005', 'which', 'would', 'provide', 'threat', 'based', 'funding', 'directly', 'to', 'cities', 'and', 'towns', 'as', 'well', 'as', 'states', 'to', 'help', 'them', 'improve', 'security', 'and', 'public', 'safety', 'locally', 'last', 'month', 'senator', 'clinton', 'met', 'with', 'homeland', 'security', 'secretary', 'michael', 'cher', '##to', '##ff', 'and', 'under', '##sco', '##red', 'the', 'critical', 'need', 'to', 'use', 'threat', 'and', 'risk', 'based', 'formulas', 'to', 'all', '##oca', '##te', 'funding', 'to', 'local', 'communities', 'states', 'and', 'first', 'respond', '##ers', 'senator', 'clinton', 'has', 'criticized', 'the', 'bush', 'administration', 'and', 'former', 'dh', '##s', 'secretary', 'ridge', 'for', 'acknowledging', 'the', 'need', 'for', 'threat', 'and', 'risk', 'based', 'funding', 'but', 'failing', 'to', 'act', 'the', 'lead', 'sponsors', 'of', 'the', 'homeland', 'security', 'forward', 'funding', 'act', 'of', '2005', 'are', 'senators', 'diane', 'fein', '##stein', 'd', 'cal', '##if', 'and', 'john', 'cory', '##n', 'r', 'texas', 'other', 'sponsors', 'include', 'senators', 'frank', 'lau', '##tenberg', 'd', 'n', 'j', 'kay', 'bailey', 'hut', '##chison', 'r', 'texas', 'barbara', 'boxer', 'd', 'cal', '##if', 'jon', 'co', '##rz', '##ine', 'd', 'n', 'j', 'chuck', 'sc', '##hum', '##er', 'd', 'n', 'y', 'and', 'bill', 'nelson', 'd', 'fl', '##a', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4LPVFySWCK6",
        "colab_type": "code",
        "outputId": "3d4d5c32-d175-447e-9794-9ccfa14cc22b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# find the longest sequence\n",
        "max_length = 0\n",
        "for t in p_tokenized_texts:\n",
        "  length = len(t)\n",
        "  if length > max_length:\n",
        "    max_length = length\n",
        "print(max_length)\n",
        "\n",
        "# the press releases can be very long. I will use max length of 512. "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6230\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0JqZNa1WCOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCkoecqxYTor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = [p_tokenizer.convert_tokens_to_ids(x) for x in p_tokenized_texts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58mYeklGYTq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvFwGr5qYTtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAh0IJs4YTyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APq4Pa-HYTv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2020, test_size=0.1)\n",
        "X_train_masks, X_validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2020, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDhKoETRQB3_",
        "colab_type": "text"
      },
      "source": [
        "#### Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71W9004TP-Qj",
        "colab_type": "code",
        "outputId": "9a7d50ef-4ab5-48a6-bee8-d191510d3749",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "vocab_in_size = p_tokenizer.vocab_size\n",
        "embedding_dim = 32\n",
        "unit = 100\n",
        "no_labels = len(np.unique(y_train))\n",
        "batch_size = 32\n",
        "print(no_labels)\n",
        "print(vocab_in_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "30522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xSIdBGmQAAz",
        "colab_type": "code",
        "outputId": "9800e990-c49d-42df-fb75-afadd5dc0f83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model_lstm1 = Sequential()\n",
        "model_lstm1.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
        "model_lstm1.add(LSTM(unit))\n",
        "model_lstm1.add(Dense(no_labels, activation='softmax'))\n",
        "model_lstm1.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_lstm1.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (None, 512, 32)           976704    \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 1,030,106\n",
            "Trainable params: 1,030,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIBpQGsGQAoW",
        "colab_type": "code",
        "outputId": "a2a7db09-75d7-4c87-95f8-7c7537eb16c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "history_lstm1 = model_lstm1.fit(X_train, y_train, \n",
        "                              epochs=10,batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1307/1307 [==============================] - 32s 24ms/step - loss: 0.6799 - acc: 0.5891\n",
            "Epoch 2/10\n",
            "1307/1307 [==============================] - 30s 23ms/step - loss: 0.6589 - acc: 0.5945\n",
            "Epoch 3/10\n",
            "1307/1307 [==============================] - 31s 23ms/step - loss: 0.6689 - acc: 0.6144\n",
            "Epoch 4/10\n",
            "1307/1307 [==============================] - 30s 23ms/step - loss: 0.6317 - acc: 0.7674\n",
            "Epoch 5/10\n",
            "1307/1307 [==============================] - 30s 23ms/step - loss: 0.5688 - acc: 0.7789\n",
            "Epoch 6/10\n",
            "1307/1307 [==============================] - 29s 22ms/step - loss: 0.2341 - acc: 0.9151\n",
            "Epoch 7/10\n",
            "1307/1307 [==============================] - 31s 24ms/step - loss: 0.0341 - acc: 0.9931\n",
            "Epoch 8/10\n",
            "1307/1307 [==============================] - 30s 23ms/step - loss: 0.0319 - acc: 0.9931\n",
            "Epoch 9/10\n",
            "1307/1307 [==============================] - 30s 23ms/step - loss: 0.0245 - acc: 0.9939\n",
            "Epoch 10/10\n",
            "1307/1307 [==============================] - 29s 23ms/step - loss: 0.3016 - acc: 0.9044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No9CPiYxbljl",
        "colab_type": "text"
      },
      "source": [
        "The accuracy rate of my LSTM on the training set is really good! It started from ~60% to more than ~90%. Let's switch to BERT and see how it does."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5s3AiasQE5g",
        "colab_type": "text"
      },
      "source": [
        "#### BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAqbcCoOby5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "train_inputs = torch.tensor(X_train)\n",
        "validation_inputs = torch.tensor(X_valid)\n",
        "train_labels = torch.tensor(y_train.astype(int))\n",
        "validation_labels = torch.tensor(y_valid.astype(int))\n",
        "train_masks = torch.tensor(X_train_masks)\n",
        "validation_masks = torch.tensor(X_validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gh5cXW5by7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 16\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRGPXskqby-O",
        "colab_type": "code",
        "outputId": "362774e5-1def-42ad-d7f6-14ffc6c0ce5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "95da6890cc7e425c9fecd7743da62efb",
            "325b874051bd45af952f4c86cb8c77cf",
            "034351173a33494d8690a8f79f47e573",
            "3d9f6d9f71294d81ae39397d9c8fa707",
            "3e43f498a1134f278e688b90e42899d8",
            "d0a9b17471984263ad7749052a094dcc",
            "420642c18ea64f84983a6b326fa5325b",
            "ec5d3111b2b64706ad3f2164a0eebad3",
            "a34dc2a44c0e45b7b5d2d0dde9795e86",
            "1a73743058334dd4bcb830be1675904f",
            "00494cae92194b8b8fc17f6542fc5a76",
            "82573624322547e393e19f0082f6f9a0",
            "8988a63920f446c0abb78613fc03690a",
            "1138bfec334842c3b690d045777e89a3",
            "d45ca511348841448ff16fa04def10c0",
            "33620f0d00a54e14908bd99966313090"
          ]
        }
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95da6890cc7e425c9fecd7743da62efb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=361, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a34dc2a44c0e45b7b5d2d0dde9795e86",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slqvt5vJbzA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSLNMWfpbzDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHDfABKGfAvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01EjqfADfA0Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use ready-made function\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPJUJ58tfA26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Useful function to print time \n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfVwnF3sgG3z",
        "colab_type": "text"
      },
      "source": [
        "I am going to modify the given code to train a BERT model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "934YGZsil61j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a hack to increase RAM on Colab\n",
        "# https://towardsdatascience.com/upgrade-your-memory-on-google-colab-for-free-1b8b18e8791d\n",
        "# a = []\n",
        "# while(1):\n",
        "#     a.append('1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnGwlPeSrwkH",
        "colab_type": "code",
        "outputId": "8b148952-ad92-49e6-88af-e3411ab41968",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.tensor(y_train.astype(int))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 0,  ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll4oy7W8fBDg",
        "colab_type": "code",
        "outputId": "0d74758f-a33e-4c83-e1e3-fa425a371f1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     82.    Elapsed: 0:00:34.\n",
            "  Batch    80  of     82.    Elapsed: 0:01:08.\n",
            "\n",
            "  Average training loss: 0.22\n",
            "  Training epcoh took: 0:01:09\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     82.    Elapsed: 0:00:34.\n",
            "  Batch    80  of     82.    Elapsed: 0:01:07.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:01:09\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.99\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     82.    Elapsed: 0:00:34.\n",
            "  Batch    80  of     82.    Elapsed: 0:01:07.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:09\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.99\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     82.    Elapsed: 0:00:34.\n",
            "  Batch    80  of     82.    Elapsed: 0:01:07.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:09\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.99\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMrCXMAmvHD-",
        "colab_type": "text"
      },
      "source": [
        "The BERT model does so well that the validation accuracy is 1 in each iteration which is very impressive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlCYpOlvfAyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# validate with the test set\n",
        "# Create sentence and label lists\n",
        "df = test_set\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "MAX_LEN = 512\n",
        "\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "p_tokenized_texts = [p_tokenizer.tokenize(sent) for sent in sentences]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "# input_ids = []\n",
        "\n",
        "# # For every sentence...\n",
        "# for sent in sentences:\n",
        "#     # `encode` will:\n",
        "#     #   (1) Tokenize the sentence.\n",
        "#     #   (2) Prepend the `[CLS]` token to the start.\n",
        "#     #   (3) Append the `[SEP]` token to the end.\n",
        "#     #   (4) Map tokens to their IDs.\n",
        "#     encoded_sent = p_tokenizer.encode(\n",
        "#                         sent,                      # Sentence to encode.\n",
        "#                         add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "#                    )\n",
        "    \n",
        "#     input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4OzGvTKw6nN",
        "colab_type": "code",
        "outputId": "fee708db-2dda-4e20-b965-4b8c4030f6ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 256 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q12XKQy6w6vN",
        "colab_type": "code",
        "outputId": "b771e3cb-b01f-47d5-f9e6-3d7e20e70040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "\n",
        "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "    # in to a list of 0s and 1s.\n",
        "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "\n",
        "    # Calculate and store the coef for this batch.  \n",
        "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "    matthews_set.append(matthews)\n",
        "\n",
        "print(matthews_set)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n",
            "[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUmFg4QFw6x_",
        "colab_type": "code",
        "outputId": "4ee01fc2-b712-4f4f-f85b-8d629d25d0c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLrE7fCR2KMp",
        "colab_type": "text"
      },
      "source": [
        "As we can notice, the matthews correlation coefficient is very high, i.e., ~99% on this particular data set. This means that BERT actually does very well even on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOfAXH4L25UF",
        "colab_type": "code",
        "outputId": "2635d0bc-b115-4a51-af18-b50558c6d633",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# save model to file\n",
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "p_tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to ./model_save/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./model_save/vocab.txt',\n",
              " './model_save/special_tokens_map.json',\n",
              " './model_save/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFEVsKOK3qAo",
        "colab_type": "code",
        "outputId": "e15749b9-a255-4e51-e099-087cfcc389a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print( os.getcwd() )\n",
        "print( os.listdir() )\n",
        "\n",
        "from google.colab import files\n",
        "files.download( \"./model_save/vocab.txt\" )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "['.config', 'ObamaClintonReleases.csv', 'model_save', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmSUwRpz4awR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download( \"./model_save/special_tokens_map.json\" )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajBarPqM4e3L",
        "colab_type": "code",
        "outputId": "d3ed0f81-c1ec-493e-be56-7ef28db08a28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print( os.listdir('model_save') )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['pytorch_model.bin', 'tokenizer_config.json', 'special_tokens_map.json', 'vocab.txt', 'config.json']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qlr3-v0A49XE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download( \"./model_save/pytorch_model.bin\" )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dMGHPb649aJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download( \"./model_save/tokenizer_config.json\" )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzjMnN984-Rl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download( \"./model_save/config.json\" )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWWMEc4879S8",
        "colab_type": "text"
      },
      "source": [
        "## <span style=\"color:red\">*Exercise 2*</span>\n",
        "\n",
        "<span style=\"color:red\">In the cells immediately following, use the pipeline functions or the word or sentence vector functions (e.g., similarity) to explore the social game underlying the production and meaning of texts associated with your final project. You have used similar, but often weaker versions in previous weeks. How does BERT help you gain insight regarding your research question that is similar and different from prior methods?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKlzNgVGJAb3",
        "colab_type": "text"
      },
      "source": [
        "#### Similarities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbhfwM5x9AIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfCRuCD79APi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJm6nnksHzhy",
        "colab_type": "text"
      },
      "source": [
        "Since the texts pertaining to my final project would not give a good example for this task, I decided to use a verse from Kustis Blow's the Breaks (a song)!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6Xh5BatIrgT",
        "colab_type": "code",
        "outputId": "60dad192-ffa6-4dfc-d45f-26ca8c7ad556",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "raw_text = \"Breaks on a bus brakes on a car \\\n",
        "Breaks to make you a superstar \\\n",
        "Breaks to win and breaks to lose \\\n",
        "But these here breaks will rock your shoes \\\n",
        "And these are the breaks \\\n",
        "Break it up, break it up, break it up\"\n",
        "\n",
        "text = []\n",
        "tmp = raw_text.split()\n",
        "for t in tmp:\n",
        "  if t != \"\":\n",
        "    text.append(t.lower())\n",
        "text = \" \".join(text)\n",
        "print(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "breaks on a bus brakes on a car breaks to make you a superstar breaks to win and breaks to lose but these here breaks will rock your shoes and these are the breaks break it up, break it up, break it up\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7TtGMy09AVI",
        "colab_type": "code",
        "outputId": "5ae46fec-ea44-4f6a-a231-dbc5a41b349f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "# Add the special tokens.\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Split the sentence into tokens.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# Map the token strings to their vocabulary indeces.\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "# Display the words with their indices.\n",
        "for tup in zip(tokenized_text, indexed_tokens):\n",
        "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS]           101\n",
            "breaks        7,807\n",
            "on            2,006\n",
            "a             1,037\n",
            "bus           3,902\n",
            "brakes       13,627\n",
            "on            2,006\n",
            "a             1,037\n",
            "car           2,482\n",
            "breaks        7,807\n",
            "to            2,000\n",
            "make          2,191\n",
            "you           2,017\n",
            "a             1,037\n",
            "superstar    18,795\n",
            "breaks        7,807\n",
            "to            2,000\n",
            "win           2,663\n",
            "and           1,998\n",
            "breaks        7,807\n",
            "to            2,000\n",
            "lose          4,558\n",
            "but           2,021\n",
            "these         2,122\n",
            "here          2,182\n",
            "breaks        7,807\n",
            "will          2,097\n",
            "rock          2,600\n",
            "your          2,115\n",
            "shoes         6,007\n",
            "and           1,998\n",
            "these         2,122\n",
            "are           2,024\n",
            "the           1,996\n",
            "breaks        7,807\n",
            "break         3,338\n",
            "it            2,009\n",
            "up            2,039\n",
            ",             1,010\n",
            "break         3,338\n",
            "it            2,009\n",
            "up            2,039\n",
            ",             1,010\n",
            "break         3,338\n",
            "it            2,009\n",
            "up            2,039\n",
            "[SEP]           102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNbcPrj29AYZ",
        "colab_type": "code",
        "outputId": "939e422a-eda2-4606-9c25-c54637885ffa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "print (segments_ids)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIawhNhk9Abc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert inputs to PyTorch tensors\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ3sM_JOI7WG",
        "colab_type": "code",
        "outputId": "cc13b310-0ba5-445d-ec7a-af2896cc3a08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model_embedding = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model_embedding.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cyHffNMI7Y9",
        "colab_type": "code",
        "outputId": "748d8239-2cd2-402e-e27d-627de8941932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "output = model_embedding(tokens_tensor)\n",
        "print(len(output[0][0][0]), len(output[1][0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "768 768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4aggC20I7bx",
        "colab_type": "code",
        "outputId": "4cca8502-34ad-4550-ea03-6a87a88436f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "word_embeddings, sentence_embedding = output\n",
        "word_embeddings[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0800,  0.2664,  0.1553,  ..., -0.5823,  0.0486,  0.2360],\n",
              "        [ 0.7006,  0.5506, -0.1479,  ..., -0.1412,  0.3838, -0.1449],\n",
              "        [-0.5792,  0.3457, -0.3020,  ..., -0.0965, -0.0448,  0.0825],\n",
              "        ...,\n",
              "        [ 0.0129, -0.7089,  0.9638,  ...,  0.1003, -0.0223,  0.4040],\n",
              "        [ 0.3847, -0.2060,  0.2848,  ...,  0.6658,  0.2594, -0.3116],\n",
              "        [ 0.7675,  0.5152, -0.0658,  ..., -0.4304, -0.5382, -0.2533]],\n",
              "       grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_ZVXcFoI7ew",
        "colab_type": "code",
        "outputId": "4b5ea19e-5063-480e-f17b-28c860adcbcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        }
      },
      "source": [
        "vec = word_embeddings[0][0]\n",
        "vec = vec.detach().numpy()\n",
        "# Plot the values as a histogram to show their distribution.\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.hist(vec, bins=200)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAAI/CAYAAAC4QOfKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAWjUlEQVR4nO3df4zkd33f8de7PlOiQmWot67lH13U\noERW2hjp6lJRqdSEyOVQcCRUhbbIVagulYIEKm1ykD+aqK10URucSq1SOTHFUkkI4odAHGnjEkcI\nqXVyBuPYmBSXXlpbBh8iCPiHyubdP26uPcied967szezd4+HtNqZ73xn532es/z0Z2bnU90dAACW\n96fWPQAAwGEjoAAAhgQUAMCQgAIAGBJQAABDAgoAYOjIpXywa6+9tre3ty/lQwIA7MlDDz301e7e\n2um2SxpQ29vbOX369KV8SACAPamqP7rYbV7CAwAYElAAAEMCCgBgSEABAAwJKACAIQEFADAkoAAA\nhpYOqKq6qqo+W1UfX1x/WVU9WFVPVNVvVtULDm5MAIDNMVmBeluSxy+4/otJ7u7u70/yx0nessrB\nAAA21VIBVVU3JjmW5NcW1yvJ7Uk+uDjlviR3HsSAAACbZtkVqF9O8jNJvrO4/ueSfL27n11cfzLJ\nDSueDQBgI+0aUFX1+iTPdPdDe3mAqjpeVaer6vTZs2f38iMAADbKMitQr0ryY1V1Jsn7c+6lu3+T\n5JqqOr8Z8Y1Jntrpzt19T3cf7e6jW1s7bmgMAHCo7BpQ3f3O7r6xu7eT/ESS3+nuv5fkgSRvXJx2\nV5KPHtiUAAAbZD+fA/WzSf5xVT2Rc++Junc1IwEAbLYju5/y/3X37yb53cXlLyW5bfUjAQBsNp9E\nDgAwJKAAAIYEFADAkIACABgSUAAAQwIKAGBIQAEADAkoAIAhAQVwCWyfOJXtE6fWPQawIgIKAGBI\nQAEADAkoAIAhAQUAMCSgAACGBBQAwJCAAgAYElAAAEMCCgBgSEABAAwJKACAIQEFADAkoAAAhgQU\nAMCQgAIAGBJQAABDAgoAYEhAAQAMCSgAgCEBBQAwJKAAAIYEFADAkIACABgSUAAAQwIKAGBIQAEA\nDAkoAIAhAQUAMCSgAACGBBQAwJCAAgAYElAAAEMCCgBgSEABAAwJKACAIQEFADAkoAAAhgQUAMCQ\ngAIAGBJQAABDAgoAYEhAAQAMCSgAgCEBBQAwJKAAAIYEFADAkIACABgSUAAAQ7sGVFW9sKp+r6o+\nV1WPVdUvLI6/t6r+Z1U9vPi69eDHBQBYvyNLnPPtJLd397eq6uokn66q31rc9k+7+4MHNx4AwObZ\nNaC6u5N8a3H16sVXH+RQAACbbKn3QFXVVVX1cJJnktzf3Q8ubvqXVfVIVd1dVX/6wKYEANggSwVU\ndz/X3bcmuTHJbVX1Q0nemeQHk/zVJC9N8rM73beqjlfV6ao6ffbs2RWNDXB52j5xKtsnTq17DGAX\no9/C6+6vJ3kgyR3d/XSf8+0k/yHJbRe5zz3dfbS7j25tbe1/YgCANVvmt/C2quqaxeXvS/LaJF+o\nqusXxyrJnUkePchBAQA2xTK/hXd9kvuq6qqcC64PdPfHq+p3qmorSSV5OMk/OsA5AQA2xjK/hfdI\nklfscPz2A5kIAGDD+SRyAIAhAQUAMCSgAACGBBQAwJCAAgAYElAAAEMCCgBgSEABAAwJKACAIQEF\nADAkoAAAhgQUAMCQgAIAGBJQAABDAgoAYEhAAQAMCSgAgCEBBQAwJKAAAIYEFADAkIACABgSUAAA\nQ0fWPQDAlW77xKl1jwAMWYECABgSUAAAQwIKAGBIQAEADAkoAIAhAQUAMCSgAACGBBQAwJCAAgAY\nElAAAEMCCgBgSEABAAzZTBhgBXbaEPjMyWNrmAS4FKxAAQAMCSgAgCEBBQAwJKAAAIYEFADAkIAC\nABgSUAAAQwIKAGBIQAEADAkoAIAhAQUAMCSgAACGbCYMcEhcuGGxjYphvaxAAQAMCSgAgCEBBQAw\nJKAAAIYEFADAkIACABgSUAAAQwIKAGBIQAEADO0aUFX1wqr6var6XFU9VlW/sDj+sqp6sKqeqKrf\nrKoXHPy4AADrt8wK1LeT3N7dP5zk1iR3VNUrk/xikru7+/uT/HGStxzcmAAAm2PXgOpzvrW4evXi\nq5PcnuSDi+P3JbnzQCYEANgwS70HqqquqqqHkzyT5P4k/yPJ17v72cUpTya54WBGBADYLEeWOam7\nn0tya1Vdk+QjSX5w2QeoquNJjifJzTffvJcZAS5L2ydOrXsEYI9Gv4XX3V9P8kCSv57kmqo6H2A3\nJnnqIve5p7uPdvfRra2tfQ0LALAJlvktvK3FylOq6vuSvDbJ4zkXUm9cnHZXko8e1JAAAJtkmZfw\nrk9yX1VdlXPB9YHu/nhVfT7J+6vqXyT5bJJ7D3BOAICNsWtAdfcjSV6xw/EvJbntIIYCANhkPokc\nAGBIQAEADAkoAIAhAQUAMCSgAACGBBQAwJCAAgAYElAAAEMCCgBgSEABAAwJKACAIQEFADAkoAAA\nhgQUAMCQgAIAGBJQAABDAgoAYEhAAQAMCSgAgCEBBQAwJKAAAIaOrHsAgMNg+8Sp/3f5zMlja5wE\n2ARWoAAAhgQUAMCQgAIAGBJQAABDAgoAYEhAAQAMCSgAgCEBBQAwJKAAAIYEFADAkIACABgSUAAA\nQzYTBthwF25kDGwGK1AAAEMCCgBgSEABAAwJKACAIQEFADAkoAAAhgQUAMCQgAIAGBJQAABDAgoA\nYEhAAQAMCSgAgCEBBQAwdGTdAwBcrrZPnFr3CMABsQIFADAkoAAAhgQUAMCQgAIAGBJQAABDAgoA\nYEhAAQAMCSgAgKFdA6qqbqqqB6rq81X1WFW9bXH856vqqap6ePH1uoMfFwBg/Zb5JPJnk7yjuz9T\nVS9O8lBV3b+47e7u/tcHNx4AwObZNaC6++kkTy8uf7OqHk9yw0EPBgCwqUbvgaqq7SSvSPLg4tBb\nq+qRqnpPVb1kxbMBAGykpTcTrqoXJflQkrd39zeq6leS/PMkvfj+S0l+cof7HU9yPEluvvnmVcwM\ncGgtu8GwjYhhsy21AlVVV+dcPL2vuz+cJN39le5+rru/k+RXk9y20327+57uPtrdR7e2tlY1NwDA\n2izzW3iV5N4kj3f3uy84fv0Fp/14kkdXPx4AwOZZ5iW8VyV5c5I/qKqHF8feleRNVXVrzr2EdybJ\nTx3IhAAAG2aZ38L7dJLa4aZPrH4cAIDN55PIAQCGBBQAwJCAAgAYElAAAEMCCgBgSEABAAwJKACA\nIQEFADC09GbCAHw3G/7ClcsKFADAkIACABgSUAAAQwIKAGBIQAEADAkoAIAhAQUAMCSgAACGBBQA\nwJCAAgAYElAAAEMCCgBgyGbCAM/DhsHATqxAAQAMCSgAgCEBBQAwJKAAAIYEFADAkIACABgSUAAA\nQwIKAGBIQAEADAkoAIAhAQUAMCSgAACGBBQAwNCRdQ8AcNhsnzi17hGANbMCBQAwJKAAAIYEFADA\nkIACABgSUAAAQwIKAGBIQAEADAkoAIAhAQUAMCSgAACGBBQAwJCAAgAYspkwwCF24cbGZ04eW+Mk\ncGWxAgUAMCSgAACGBBQAwJCAAgAYElAAAEMCCgBgSEABAAwJKACAoV0DqqpuqqoHqurzVfVYVb1t\ncfylVXV/VX1x8f0lBz8uAMD6LbMC9WySd3T3LUlemeSnq+qWJCeSfLK7X57kk4vrAACXvV0Dqruf\n7u7PLC5/M8njSW5I8oYk9y1Ouy/JnQc1JADAJhm9B6qqtpO8IsmDSa7r7qcXN305yXUrnQwAYEMt\nHVBV9aIkH0ry9u7+xoW3dXcn6Yvc73hVna6q02fPnt3XsAAAm2CpgKqqq3Munt7X3R9eHP5KVV2/\nuP36JM/sdN/uvqe7j3b30a2trVXMDACwVsv8Fl4luTfJ49397gtu+liSuxaX70ry0dWPBwCweY4s\ncc6rkrw5yR9U1cOLY+9KcjLJB6rqLUn+KMnfOZgRAQA2y64B1d2fTlIXufk1qx0HAGDz+SRyAIAh\nAQUAMCSgAACGBBQAwJCAAgAYElAAAEMCCgBgSEABAAwJKACAoWW2cgG4omyfOLXuEVbmwj/LmZPH\n1jgJXF6sQAEADAkoAIAhAQUAMCSgAACGBBQAwJCAAgAYElAAAEMCCgBgSEABAAwJKACAIQEFADAk\noAAAhmwmDHCZudhmyOeP21QY9s8KFADAkIACABgSUAAAQwIKAGBIQAEADAkoAIAhAQUAMCSgAACG\nBBQAwJCAAgAYElAAAEMCCgBgyGbCAIfQThsGX2wTYWD1rEABAAwJKACAIQEFADAkoAAAhgQUAMCQ\ngAIAGBJQAABDAgoAYEhAAQAMCSgAgCEBBQAwJKAAAIZsJgxc0S7cgPfMyWNrnAQ4TKxAAQAMCSgA\ngCEBBQAwJKAAAIYEFADAkIACABgSUAAAQwIKAGBIQAEADO0aUFX1nqp6pqoeveDYz1fVU1X18OLr\ndQc7JgDA5lhmBeq9Se7Y4fjd3X3r4usTqx0LAGBz7RpQ3f2pJF+7BLMAABwK+3kP1Fur6pHFS3wv\nWdlEAAAbbq8B9StJ/lKSW5M8neSXLnZiVR2vqtNVdfrs2bN7fDiA1do+cSrbJ06tewzgkNpTQHX3\nV7r7ue7+TpJfTXLb85x7T3cf7e6jW1tbe50TAGBj7Cmgqur6C67+eJJHL3YuAMDl5shuJ1TVbyR5\ndZJrq+rJJP8syaur6tYkneRMkp86wBkBADbKrgHV3W/a4fC9BzALAMCh4JPIAQCGBBQAwJCAAgAY\nElAAAEMCCgBgSEABAAwJKACAIQEFADC06wdpAlwudts8+ErcXPj8n/nMyWNrngQOFytQAABDAgoA\nYEhAAQAMCSgAgCEBBQAwJKAAAIYEFADAkIACABgSUAAAQwIKAGBIQAEADAkoAIAhmwkDXGGuxE2T\nYdWsQAEADAkoAIAhAQUAMCSgAACGBBQAwJCAAgAYElAAAEMCCgBgSEABAAwJKACAIQEFADAkoAAA\nhgQUAMCQgAIAGBJQAABDAgoAYEhAAQAMCSgAgCEBBQAwJKAAAIYEFADAkIACABgSUAAAQwIKAGBI\nQAEADAkoAIChI+seAOCgbZ84te4RgMuMFSgAgCEBBQAwJKAAAIYEFADAkIACABgSUAAAQwIKAGBI\nQAEADO0aUFX1nqp6pqoeveDYS6vq/qr64uL7Sw52TACAzbHMCtR7k9zxPcdOJPlkd788yScX1wEA\nrgi7BlR3fyrJ177n8BuS3Le4fF+SO1c8FwDAxtrre6Cu6+6nF5e/nOS6Fc0DALDx9v0m8u7uJH2x\n26vqeFWdrqrTZ8+e3e/DAQCs3V4D6itVdX2SLL4/c7ETu/ue7j7a3Ue3trb2+HAAAJtjrwH1sSR3\nLS7fleSjqxkHAGDzLfMxBr+R5L8m+YGqerKq3pLkZJLXVtUXk/zI4joAwBXhyG4ndPebLnLTa1Y8\nCwDAoeCTyAEAhgQUAMCQgAIAGBJQAABDAgoAYEhAAQAMCSgAgCEBBQAwJKAA+C7bJ05l+8SpdY8B\nG01AAQAMCSgAgCEBBQAwJKAAAIYEFADAkIACABgSUAAAQwIKAGBIQAEADAkoAIAhAQUAMCSgAACG\nBBQAwNCRdQ8AwPptnzi17hHgULECBQAwJKAAAIYEFADAkIACABgSUAAAQwIKAGBIQAEADAkoAIAh\nAQUAMCSgAACGBBQAwJCAAgAYspkwcFmyOS5wkKxAAQAMCSgAgCEBBQAwJKAAAIYEFADAkIACABgS\nUAAAQwIKAGBIQAEADAkoAIAhAQUAMCSgAACGbCYMXDZsILxay/7zPHPy2AFPApvHChQAwJCAAgAY\nElAAAEMCCgBgSEABAAwJKACAIQEFADAkoAAAhvb1QZpVdSbJN5M8l+TZ7j66iqEAADbZKj6J/G91\n91dX8HMAAA4FL+EBAAztN6A6yW9X1UNVdXwVAwEAbLr9voT3N7r7qar680nur6ovdPenLjxhEVbH\nk+Tmm2/e58MBV5Lzm9nutFmtjYOBddrXClR3P7X4/kySjyS5bYdz7unuo919dGtraz8PBwCwEfYc\nUFX1Z6rqxecvJ/nRJI+uajAAgE21n5fwrkvykao6/3N+vbv/00qmAgDYYHsOqO7+UpIfXuEsAACH\ngo8xAAAYElAAAEMCCgBgSEABAAwJKACAIQEFADAkoAAAhgQUAMCQgAIAGNrPVi4AK7N94lSS5MzJ\nY0udx+Fz4XO32/MMm84KFADAkIACABgSUAAAQwIKAGBIQAEADAkoAIAhAQUAMCSgAACGBBQAwJCA\nAgAYElAAAEMCCgBgyGbCwMazgTCwaaxAAQAMCSgAgCEBBQAwJKAAAIYEFADAkIACABgSUAAAQwIK\nAGBIQAEADAkoAIAhAQUAMCSgAACGbCYMXHLnNwc+c/LYmidh1Va18bO/I2w6K1AAAEMCCgBgSEAB\nAAwJKACAIQEFADAkoAAAhgQUAMCQgAIAGBJQAABDAgoAYEhAAQAMCSgAgCEBBQAwdGTdA6zahTuB\n28X7yrXTTu772d19Fffd6/2nj7PTn/lCy86wn/vux06Py2Zb1XO2289Z199JNscm/TfeChQAwJCA\nAgAYElAAAEMCCgBgSEABAAwJKACAIQEFADAkoAAAhvYVUFV1R1X9YVU9UVUnVjUUAMAm23NAVdVV\nSf5dkr+d5JYkb6qqW1Y1GADAptrPCtRtSZ7o7i919/9J8v4kb1jNWAAAm2s/AXVDkv99wfUnF8cA\nAC5r1d17u2PVG5Pc0d3/cHH9zUn+Wne/9XvOO57k+OLqDyT5w72PyyVwbZKvrnsIRjxnh5Pn7fDx\nnB1O+3ne/mJ3b+10w5G9z5Onktx0wfUbF8e+S3ffk+SefTwOl1BVne7uo+ueg+V5zg4nz9vh4zk7\nnA7qedvPS3i/n+TlVfWyqnpBkp9I8rHVjAUAsLn2vALV3c9W1VuT/OckVyV5T3c/trLJAAA21H5e\nwkt3fyLJJ1Y0C5vBy62Hj+fscPK8HT6es8PpQJ63Pb+JHADgSmUrFwCAIQHFRVXVO6qqq+radc/C\n86uqf1VVX6iqR6rqI1V1zbpnYme2wDp8quqmqnqgqj5fVY9V1dvWPRPLqaqrquqzVfXxVf9sAcWO\nquqmJD+a5H+texaWcn+SH+ruv5Lkvyd555rnYQe2wDq0nk3yju6+Jckrk/y05+3QeFuSxw/iBwso\nLubuJD+TxJvkDoHu/u3ufnZx9b/l3OeysXlsgXUIdffT3f2ZxeVv5tx/kO28seGq6sYkx5L82kH8\nfAHFn1BVb0jyVHd/bt2zsCc/meS31j0EO7IF1iFXVdtJXpHkwfVOwhJ+OecWAr5zED98Xx9jwOFV\nVf8lyV/Y4aafS/KunHv5jg3yfM9Zd390cc7P5dzLDe+7lLPBlaCqXpTkQ0ne3t3fWPc8XFxVvT7J\nM939UFW9+iAeQ0Bdobr7R3Y6XlV/OcnLknyuqpJzLwV9pqpu6+4vX8IR+R4Xe87Oq6p/kOT1SV7T\nPp9kUy21BRabp6quzrl4el93f3jd87CrVyX5sap6XZIXJvmzVfUfu/vvr+oBfA4Uz6uqziQ52t02\n0NxgVXVHkncn+ZvdfXbd87CzqjqSc2/yf03OhdPvJ/m7dnHYbHXu/ybvS/K17n77uudhZrEC9U+6\n+/Wr/LneAwWXh3+b5MVJ7q+qh6vq3697IP6kxRv9z2+B9XiSD4inQ+FVSd6c5PbFv18PL1Y2uIJZ\ngQIAGLICBQAwJKAAAIYEFADAkIACABgSUAAAQwIKAGBIQAEADAkoAICh/wtB6NHZOlhgfAAAAABJ\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZPEWy_wI7pT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word vectors \n",
        "token_vecs = []\n",
        "# For each token in the sentence...\n",
        "for embedding in word_embeddings[0]:\n",
        "    cat_vec = embedding.detach().numpy()\n",
        "    # Use `cat_vec` to represent `token`.\n",
        "    token_vecs.append(cat_vec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0m4DN5e9AeN",
        "colab_type": "code",
        "outputId": "438078bb-76e4-4e2b-b904-b6c42a38771a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "# sectence vecs\n",
        "for i, token_str in enumerate(tokenized_text):\n",
        "    print(i, token_str)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [CLS]\n",
            "1 breaks\n",
            "2 on\n",
            "3 a\n",
            "4 bus\n",
            "5 brakes\n",
            "6 on\n",
            "7 a\n",
            "8 car\n",
            "9 breaks\n",
            "10 to\n",
            "11 make\n",
            "12 you\n",
            "13 a\n",
            "14 superstar\n",
            "15 breaks\n",
            "16 to\n",
            "17 win\n",
            "18 and\n",
            "19 breaks\n",
            "20 to\n",
            "21 lose\n",
            "22 but\n",
            "23 these\n",
            "24 here\n",
            "25 breaks\n",
            "26 will\n",
            "27 rock\n",
            "28 your\n",
            "29 shoes\n",
            "30 and\n",
            "31 these\n",
            "32 are\n",
            "33 the\n",
            "34 breaks\n",
            "35 break\n",
            "36 it\n",
            "37 up\n",
            "38 ,\n",
            "39 break\n",
            "40 it\n",
            "41 up\n",
            "42 ,\n",
            "43 break\n",
            "44 it\n",
            "45 up\n",
            "46 [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pcf5E4rlKKDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.spatial.distance import cosine\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj0w8vdDKKIk",
        "colab_type": "code",
        "outputId": "4b6df3dd-8b04-4784-c1c4-7e727e699b8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Calculate the cosine similarity between the word breaks\n",
        "# in \"these are the breaks\" vs \"breaks to win\" (different meanings).\n",
        "diff_bank = 1 - cosine(token_vecs[34], token_vecs[15])\n",
        "\n",
        "# Calculate the cosine similarity between the word bank\n",
        "# in \"break it up\" vs \"break it up\" (same meaning).\n",
        "same_bank = 1 - cosine(token_vecs[35], token_vecs[39])\n",
        "\n",
        "print('Vector similarity for *different* meanings:  {:.2f}'.format(diff_bank))\n",
        "print('Vector similarity for  *similar*  meanings:  {:.2f}'.format(same_bank))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vector similarity for *different* meanings:  0.74\n",
            "Vector similarity for  *similar*  meanings:  0.88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEHfENb2La2K",
        "colab_type": "text"
      },
      "source": [
        "Indeed, the words that are supposed to be similar turn out to be more similar than \"breaks\" with different meanings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF85lw2bKKGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_vector(text, word_id, model, tokenizer):\n",
        "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "    tokenized_text = tokenizer.tokenize(marked_text)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    word_embeddings, sentence_embeddings = model(tokens_tensor)   \n",
        "    vector = word_embeddings[0][word_id].detach().numpy()\n",
        "    return vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3m8VCPKKKBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_6 = word_vector(text, 6, model_embedding, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMIX-BakLR6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_vector(text, model, tokenizer, method=\"average\"):\n",
        "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "    tokenized_text = tokenizer.tokenize(marked_text)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    word_embeddings, sentence_embeddings = model(tokens_tensor)\n",
        "    token_vecs = []\n",
        "    \n",
        "    for embedding in word_embeddings[0]:\n",
        "        cat_vec = embedding.detach().numpy()\n",
        "        token_vecs.append(cat_vec)\n",
        "        \n",
        "    if method == \"average\":\n",
        "        sentence_embedding = np.mean(token_vecs, axis=0)\n",
        "    if method == \"model\":\n",
        "        sentence_embedding = sentence_embeddings\n",
        "    # do something\n",
        "    return sentence_embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFZiehj6LWYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sen_vec_1 = sentence_vector(text, model_embedding, tokenizer, method=\"model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPRiDaYVI8RP",
        "colab_type": "text"
      },
      "source": [
        "#### Try pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4UEO4zf8RYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUeoKfNp8Rcc",
        "colab_type": "code",
        "outputId": "fec95e42-e0bb-4aa7-c57e-c91d341ceb24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "6f91403349c945e9be020bcde378a8e0",
            "c1b627b4d0e84080b47784e9ab4927c9",
            "b2362cfa61cf4c689d708eb121d0a249",
            "103b5442d96f4b62bbc421ce6aaa044a",
            "7152e720a6b24caf80a56343009da5dc",
            "eacc819057db4022961696a2234fcde9",
            "edee39b4bd544b8895df85911c5b8b7b",
            "8d27b7b1332a4fa5b42563de21d3287d"
          ]
        }
      },
      "source": [
        "# Allocate a pipeline for sentiment-analysis\n",
        "nlp_sentiment = pipeline('sentiment-analysis')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f91403349c945e9be020bcde378a8e0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=230, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3x4k9VkFowh",
        "colab_type": "text"
      },
      "source": [
        "Take Donald Trump's 2016 inaugural address as an example for sentiment analysis. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SU_XFngE-xb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "speech = '           Chief Justice Roberts, President Carter, President Clinton, President Bush, President Obama, fellow Americans and people of the world, thank you.      We, the citizens of America, are now joined in a great national effort to rebuild our country and restore its promise for all of our people. Together we will determine the course of America and the world for many, many years to come.      We will face challenges. We will confront hardships. But we will get the job done. Every four years, we gather on these steps to carry out the orderly and peaceful transfer of power.      And we are grateful to President Obama and First Lady Michelle Obama for their gracious aid throughout this transition. They have been magnificent. Thank you.      Today<U+0092>s ceremony, however, has very special meaning. Because today, we are not merely transferring power from one administration to another or from one party to another, but we are transferring power from Washington, D.C., and giving it back to you, the people.      For too long, a small group in our nation<U+0092>s capital has reaped the rewards of government while the people have borne the cost. Washington flourished, but the people did not share in its wealth. Politicians prospered, but the jobs left and the factories closed.      The establishment protected itself but not the citizens of our country. Their victories have not been your victories. Their triumphs have not been your triumphs. And while they celebrated in our nation<U+0092>s capital, there was little to celebrate for struggling families all across our land. That all changes starting right here and right now, because this moment is your moment: it belongs to you. It belongs to everyone gathered here today and everyone watching all across America. This is your day, this is your celebration, and this, the United States of America, is your country.      What truly matters is not which party controls our government but whether our government is controlled by the people. January Twentieth, Two Thousand and Seventeen will be remembered as the day the people became the rulers of this nation again. The forgotten men and women of our country will be forgotten no longer.      Everyone is listening to you now. You came by the tens of millions to become part of a historic movement, the likes of which the world has never seen before. At the center of this movement is a crucial conviction<U+0097>that a nation exists to serve its citizens.      Americans want great schools for their children, safe neighborhoods for their families and good jobs for themselves. These are just and reasonable demands of righteous people and a righteous public. But for too many of our citizens, a different reality exists: mothers and children trapped in poverty in our inner cities, rusted out factories scattered like tombstones across the landscape of our nation, an education system flushed with cash but which leaves our young and beautiful students deprived of all knowledge<U+0097>and the crime, and the gangs, and the drugs that have stolen too many lives and robbed our country of so much unrealized potential. This American carnage stops right here and stops right now.       We are one nation, and their pain is our pain, their dreams are our dreams, and their success will be our success. We share one heart, one home and one glorious destiny.       The oath of office I take today is an oath of allegiance to all Americans. For many decades, we<U+0092>ve enriched foreign industry at the expense of American industry, subsidized the armies of other countries while allowing for the very sad depletion of our military. We defended other nation<U+0092>s borders while refusing to defend our own<U+0097>and spent trillions and trillions of dollars overseas while America<U+0092>s infrastructure has fallen into disrepair and decay. We<U+0092>ve made other countries rich while the wealth, strength and confidence of our country has dissipated over the horizon. One by one, the factories shuttered and left our shores with not even a thought about the millions and millions of American workers that were left behind. The wealth of our middle class has been ripped from their homes and then redistributed all across the world. But, that is the past and now we are looking only to the future.       We, assembled here today, are issuing a new decree to be heard in every city, in every foreign capital and in every hall of power. From this day forward, a new vision will govern our land. From this day forward, it<U+0092>s going to be only America first. America first!       Every decision<U+0097>on trade, on taxes, on immigration, on foreign affairs<U+0097>will be made to benefit American workers and American families. We must protect our borders from the ravages of other countries making our products, stealing our companies and destroying our jobs. Protection will lead to great prosperity and strength.       I will fight for you with every breath in my body; and I will never, ever let you down. America will start winning again<U+0097>winning like never before.       We will bring back our jobs. We will bring back our borders. We will bring back our wealth, and we will bring back our dreams. We will build new roads and highways and bridges and airports and tunnels and railways all across our wonderful nation. We will get our people off of welfare and back to work, rebuilding our country with American hands and American labor. We will follow two simple rules: buy American and hire American.       We will seek friendship and goodwill with the nations of the world; but, we do so with the understanding that it is the right of all nations to put their own interests first. We do not seek to impose our way of life on anyone but rather to let it shine as an example. We will shine for everyone to follow.       We will reinforce old alliances and form new ones, and unite the civilized world against radical Islamic terrorism, which we will eradicate completely from the face of the earth.       At the bedrock of our politics will be a total allegiance to the United States of America; and through our loyalty to our country, we will rediscover our loyalty to each other. When you open your heart to patriotism, there is no room for prejudice.       The Bible tells us how good and pleasant it is when God<U+0092>s people live together in unity. We must speak our minds openly, debate our disagreements honestly, but always pursue solidarity. When America is united, America is totally unstoppable.       There should be no fear: we are protected, and we will always be protected. We will be protected by the great men and women of our military and law enforcement; and most importantly, we will be protected by God.       Finally, we must think big and dream even bigger. In America, we understand that a nation is only living as long as it is striving. We will no longer accept politicians who are all talk and no action, constantly complaining but never doing anything about it. The time for empty talk is over. Now arrives the hour of action.       Do not allow anyone to tell you that it cannot be done. No challenge can match the heart and fight and spirit of America. We will not fail. Our country will thrive and prosper again.       We stand at the birth of a new millennium: ready to unlock the mysteries of space; to free the earth from the miseries of disease; and to harness the energies, industries and technologies of tomorrow. A new national pride will stir ourselves, lift our sights and heal our divisions. It<U+0092>s time to remember that old wisdom our soldiers will never forget<U+0097>that whether we are black or brown or white, we all bleed the same red blood of patriots. We all enjoy the same glorious freedoms, and we all salute the same great American flag. And whether a child is born in the urban sprawl of Detroit or the wind-swept plains of Nebraska, they look up at the same night sky, they fill their heart with the same dreams and they are infused with the breath of life by the same Almighty Creator.       So to all Americans in every city near and far, small and large, from mountain to mountain, from ocean to ocean, hear these words: you will never be ignored again. Your voice, your hopes and your dreams will define our American destiny. And your courage and goodness and love will forever guide us along the way.       Together, we will make America strong again. We will make America wealthy again. We will make America proud again. We will make America safe again. And yes, together, we will make America great again!       Thank you. God bless you. And God bless America.       Thank you. God bless America.                '\n",
        "tmt = speech.split()\n",
        "new_speech = []\n",
        "for s in tmt:\n",
        "  if s != '':\n",
        "    new_speech.append(s.lower())\n",
        "new_speech = ' '.join(new_speech)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TPvXmLP8Rgu",
        "colab_type": "code",
        "outputId": "4e40c12f-4549-4b6a-e880-68355ab533f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "nlp_sentiment(new_speech)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.998232}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1CiP3VgF2I0",
        "colab_type": "text"
      },
      "source": [
        "The result is that the speech is very positive which is expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73U5ANlAMC6N",
        "colab_type": "text"
      },
      "source": [
        "#### End of exercise 2\n",
        "I test the given code here before making my own chatbot below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR1hUAJfMp-t",
        "colab_type": "code",
        "outputId": "f5a07eca-09df-487c-c51d-4028b15d98e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213,
          "referenced_widgets": [
            "fd7c86a8aca44dffb94f2eda3c2c40b1",
            "d2e3bf5406df40f7b0201151d0f2b9f1",
            "af94522f0fb143f58eddeb53e7ef2d6c",
            "d51fef7b9b184a489c25f108474c0d3c",
            "1dc21f23432249f08833a328af71ab32",
            "ad73acf1dbfe47d9b39fe13d78e31c69",
            "10f78ecba47a446da73b454d8ae72b84",
            "60bc63df83ca40fb98cc222164d4b424",
            "4455246b7dc24704a3b6789fafa3044d",
            "db1aa3a044eb4730b18bd783a3e0c127",
            "72f36f33d1984ccdb248f54dbc5b227e",
            "62aeb102bca4452fbaff5178c73877a9",
            "915231ce2268490c90e597ac4b937cfe",
            "d69f3d0480844487aad39c7de34e6e53",
            "d4a9236ed02e49148f3bb1cbe381606f",
            "f864a1b6db9b4f4e8ad05adf2762a541",
            "666e2cb8186044268bc4195ccbc5b41e",
            "7abcaec5113942a9818bfff825824065",
            "97d600b6b100496f813e2ad38bc87cbb",
            "e0cd43f5c050416f8440db5633b01d86",
            "3f2cf2ec95ff42bc8f6937465bbfb7d0",
            "939073a0df6448b2909a1f96f2c92005",
            "02885d8580d641f6bce485e9f458db81",
            "615835a3a1404a06bdb5e2639c2f8054",
            "e508b4ef18a9411182ceea6408539924",
            "ae009213a8e4494b96d18369eb6709d1",
            "be62fce520b44b0099ff34f8e0179e5a",
            "3f4687568d064f8ebf11792d6d8837ee",
            "f16199604d2d4c7eb79dc22e7e159c69",
            "a2722aa955b34d71b4fe17c8227f1f9d",
            "1cb57fe2c22e4170b7df11f758b5653e",
            "286a29d3a98c4cdba08e93e0b897e882"
          ]
        }
      },
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "\n",
        "tokenizer_gpt = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model_gpt = AutoModelWithLMHead.from_pretrained(\"gpt2\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd7c86a8aca44dffb94f2eda3c2c40b1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=224, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4455246b7dc24704a3b6789fafa3044d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=1042301, style=ProgressStyle(description_wiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "666e2cb8186044268bc4195ccbc5b41e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=456318, style=ProgressStyle(description_widâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e508b4ef18a9411182ceea6408539924",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=548118077, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpRmQH_pMqCJ",
        "colab_type": "code",
        "outputId": "cf98dc84-f623-4c63-c5a8-d033f75106dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# see what BERT will give us from Gen 1:1\n",
        "sequence = \"In the beginning God created the heavens and the earth\"\n",
        "\n",
        "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
        "generated = model_gpt.generate(input, max_length=50)\n",
        "\n",
        "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
        "print(resulting_string)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In the beginning God created the heavens and the earthâ€”where there was a center, and to no one there was a central body; he that has seen God create the earth is also His central body; from thence the seven heavens were created, and\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ6EekfVOZqY",
        "colab_type": "text"
      },
      "source": [
        "Ah... pretty interesting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq7G7JbAt9sn",
        "colab_type": "text"
      },
      "source": [
        "### Results\n",
        "\n",
        "We get the same result as our Jupyter Notebook, but our model was trained in just 10 mins, as opposed to 12 hours without GPU. Super neat! We now skip the parts which just use the pre-trained model, as you can follow those in the Jupyter Notebook. The purpose of this notebook is to get you to upload your data, train your models, download them, and use them in your Jupyter Notebooks.\n",
        "\n",
        "### Fine-tuning BERT and GPT\n",
        "\n",
        "We now do our other model fine-tuning.\n",
        "You have to upload your files to the colab file - on the left of the screen, on the files section, click the upload section, and upload test_text_trump, train_text_trump, run_generation.py, and run_language_modelling.py. These files would be on the GitHub repository.\n",
        "\n",
        "We start with training a model on Trump tweets, and then save the model to disk and load it in the Jupyter notebook. The following two lines of code does the language training and then text generation. The important part here is training your model, and then downloading that by right clicking the file name in the files section on the top left of the screen. In my xase, all the files were saved in /content/output_gpt_trump"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDFdfcqQPIMG",
        "colab_type": "code",
        "outputId": "d437510c-658d-4d27-92e8-37a4db4ebf24",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# upload manually\n",
        "test_text_trump = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-33d7c6c4-84dc-4a49-a3af-221e2240a6ec\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-33d7c6c4-84dc-4a49-a3af-221e2240a6ec\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test_text_trump to test_text_trump\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xESDcKm8PIRl",
        "colab_type": "code",
        "outputId": "9598bff4-4499-40ef-9e23-c6dd636e05fc",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# upload manually\n",
        "train_text_trump = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6dc5c760-911f-42e0-ab7a-ef2b85b91273\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-6dc5c760-911f-42e0-ab7a-ef2b85b91273\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving train_text_trump to train_text_trump\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpqZUK1YPIPY",
        "colab_type": "code",
        "outputId": "1be40d65-85c5-49b0-9a93-f625a6355bc6",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# upload manually\n",
        "run_generation = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c9678ee7-ddc0-4d2d-b25e-82afd7228262\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-c9678ee7-ddc0-4d2d-b25e-82afd7228262\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving run_generation.py to run_generation.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uNcZXWgPZL_",
        "colab_type": "code",
        "outputId": "66112137-9ff6-41e3-881a-4174454dcb44",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# upload manually\n",
        "run_language_modelling = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-99142a63-0a5f-4c6f-9de2-ed4df8389e0c\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-99142a63-0a5f-4c6f-9de2-ed4df8389e0c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving run_language_modelling.py to run_language_modelling.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEeCxckrPtnY",
        "colab_type": "code",
        "outputId": "621d106f-b0ac-4970-deea-7601f8b354a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# check if files are properly uploaded\n",
        "print( os.listdir() )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['.config', 'ObamaClintonReleases.csv', 'run_generation.py', 'run_language_modelling.py', 'test_text_trump', 'train_text_trump', 'model_save', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XdVsM_cL_3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqiVuxFuLLvu",
        "colab_type": "code",
        "outputId": "d5b10259-c853-43aa-b745-c014f4976953",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python /content/run_language_modelling.py --output_dir=output_gpt_trump --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=/content/train_text_trump --do_eval --eval_data_file=/content/test_text_trump --per_gpu_train_batch_size=1 --per_gpu_eval_batch_size=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/05/2020 05:50:55 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "03/05/2020 05:50:55 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942\n",
            "03/05/2020 05:50:55 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "03/05/2020 05:50:55 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "03/05/2020 05:50:55 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/05/2020 05:50:55 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "03/05/2020 05:51:02 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=1024, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='/content/test_text_trump', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_name_or_path='gpt2', model_type='gpt2', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='output_gpt_trump', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=1, per_gpu_train_batch_size=1, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='/content/train_text_trump', warmup_steps=0, weight_decay=0.0)\n",
            "03/05/2020 05:51:02 - INFO - __main__ -   Creating features from dataset file at /content\n",
            "03/05/2020 05:51:05 - INFO - __main__ -   Saving features into cached file /content/gpt2_cached_lm_1024_train_text_trump\n",
            "03/05/2020 05:51:05 - INFO - __main__ -   ***** Running training *****\n",
            "03/05/2020 05:51:05 - INFO - __main__ -     Num examples = 669\n",
            "03/05/2020 05:51:05 - INFO - __main__ -     Num Epochs = 1\n",
            "03/05/2020 05:51:05 - INFO - __main__ -     Instantaneous batch size per GPU = 1\n",
            "03/05/2020 05:51:05 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "03/05/2020 05:51:05 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "03/05/2020 05:51:05 - INFO - __main__ -     Total optimization steps = 669\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/669 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/669 [00:00<02:49,  3.95it/s]\u001b[A\n",
            "Iteration:   0% 2/669 [00:00<02:41,  4.12it/s]\u001b[A\n",
            "Iteration:   0% 3/669 [00:00<02:37,  4.22it/s]\u001b[A\n",
            "Iteration:   1% 4/669 [00:00<02:35,  4.28it/s]\u001b[A\n",
            "Iteration:   1% 5/669 [00:01<02:33,  4.32it/s]\u001b[A\n",
            "Iteration:   1% 6/669 [00:01<02:32,  4.36it/s]\u001b[A\n",
            "Iteration:   1% 7/669 [00:01<02:30,  4.39it/s]\u001b[A\n",
            "Iteration:   1% 8/669 [00:01<02:30,  4.41it/s]\u001b[A\n",
            "Iteration:   1% 9/669 [00:02<02:29,  4.41it/s]\u001b[A\n",
            "Iteration:   1% 10/669 [00:02<02:29,  4.42it/s]\u001b[A\n",
            "Iteration:   2% 11/669 [00:02<02:28,  4.43it/s]\u001b[A\n",
            "Iteration:   2% 12/669 [00:02<02:28,  4.44it/s]\u001b[A\n",
            "Iteration:   2% 13/669 [00:02<02:27,  4.43it/s]\u001b[A\n",
            "Iteration:   2% 14/669 [00:03<02:27,  4.44it/s]\u001b[A\n",
            "Iteration:   2% 15/669 [00:03<02:27,  4.44it/s]\u001b[A\n",
            "Iteration:   2% 16/669 [00:03<02:27,  4.44it/s]\u001b[A\n",
            "Iteration:   3% 17/669 [00:03<02:26,  4.44it/s]\u001b[A\n",
            "Iteration:   3% 18/669 [00:04<02:26,  4.44it/s]\u001b[A\n",
            "Iteration:   3% 19/669 [00:04<02:26,  4.45it/s]\u001b[A\n",
            "Iteration:   3% 20/669 [00:04<02:26,  4.44it/s]\u001b[A\n",
            "Iteration:   3% 21/669 [00:04<02:25,  4.44it/s]\u001b[A\n",
            "Iteration:   3% 22/669 [00:04<02:25,  4.45it/s]\u001b[A\n",
            "Iteration:   3% 23/669 [00:05<02:25,  4.45it/s]\u001b[A\n",
            "Iteration:   4% 24/669 [00:05<02:25,  4.44it/s]\u001b[A\n",
            "Iteration:   4% 25/669 [00:05<02:24,  4.45it/s]\u001b[A\n",
            "Iteration:   4% 26/669 [00:05<02:24,  4.44it/s]\u001b[A\n",
            "Iteration:   4% 27/669 [00:06<02:24,  4.44it/s]\u001b[A\n",
            "Iteration:   4% 28/669 [00:06<02:24,  4.44it/s]\u001b[A\n",
            "Iteration:   4% 29/669 [00:06<02:24,  4.44it/s]\u001b[A\n",
            "Iteration:   4% 30/669 [00:06<02:24,  4.41it/s]\u001b[A\n",
            "Iteration:   5% 31/669 [00:07<02:23,  4.43it/s]\u001b[A\n",
            "Iteration:   5% 32/669 [00:07<02:23,  4.44it/s]\u001b[A\n",
            "Iteration:   5% 33/669 [00:07<02:23,  4.43it/s]\u001b[A\n",
            "Iteration:   5% 34/669 [00:07<02:23,  4.42it/s]\u001b[A\n",
            "Iteration:   5% 35/669 [00:07<02:22,  4.44it/s]\u001b[A\n",
            "Iteration:   5% 36/669 [00:08<02:23,  4.43it/s]\u001b[A\n",
            "Iteration:   6% 37/669 [00:08<02:22,  4.45it/s]\u001b[A\n",
            "Iteration:   6% 38/669 [00:08<02:22,  4.43it/s]\u001b[A\n",
            "Iteration:   6% 39/669 [00:08<02:21,  4.45it/s]\u001b[A\n",
            "Iteration:   6% 40/669 [00:09<02:21,  4.45it/s]\u001b[A\n",
            "Iteration:   6% 41/669 [00:09<02:21,  4.45it/s]\u001b[A\n",
            "Iteration:   6% 42/669 [00:09<02:20,  4.45it/s]\u001b[A\n",
            "Iteration:   6% 43/669 [00:09<02:21,  4.44it/s]\u001b[A\n",
            "Iteration:   7% 44/669 [00:09<02:20,  4.44it/s]\u001b[A\n",
            "Iteration:   7% 45/669 [00:10<02:20,  4.44it/s]\u001b[A\n",
            "Iteration:   7% 46/669 [00:10<02:20,  4.44it/s]\u001b[A\n",
            "Iteration:   7% 47/669 [00:10<02:20,  4.41it/s]\u001b[A\n",
            "Iteration:   7% 48/669 [00:10<02:19,  4.45it/s]\u001b[A\n",
            "Iteration:   7% 49/669 [00:11<02:19,  4.43it/s]\u001b[A\n",
            "Iteration:   7% 50/669 [00:11<02:19,  4.45it/s]\u001b[A\n",
            "Iteration:   8% 51/669 [00:11<02:19,  4.43it/s]\u001b[A\n",
            "Iteration:   8% 52/669 [00:11<02:18,  4.44it/s]\u001b[A\n",
            "Iteration:   8% 53/669 [00:11<02:18,  4.45it/s]\u001b[A\n",
            "Iteration:   8% 54/669 [00:12<02:18,  4.45it/s]\u001b[A\n",
            "Iteration:   8% 55/669 [00:12<02:18,  4.44it/s]\u001b[A\n",
            "Iteration:   8% 56/669 [00:12<02:17,  4.45it/s]\u001b[A\n",
            "Iteration:   9% 57/669 [00:12<02:17,  4.44it/s]\u001b[A\n",
            "Iteration:   9% 58/669 [00:13<02:17,  4.44it/s]\u001b[A\n",
            "Iteration:   9% 59/669 [00:13<02:17,  4.45it/s]\u001b[A\n",
            "Iteration:   9% 60/669 [00:13<02:18,  4.41it/s]\u001b[A\n",
            "Iteration:   9% 61/669 [00:13<02:16,  4.45it/s]\u001b[A\n",
            "Iteration:   9% 62/669 [00:13<02:16,  4.44it/s]\u001b[A\n",
            "Iteration:   9% 63/669 [00:14<02:16,  4.44it/s]\u001b[A\n",
            "Iteration:  10% 64/669 [00:14<02:16,  4.44it/s]\u001b[A\n",
            "Iteration:  10% 65/669 [00:14<02:15,  4.45it/s]\u001b[A\n",
            "Iteration:  10% 66/669 [00:14<02:15,  4.44it/s]\u001b[A\n",
            "Iteration:  10% 67/669 [00:15<02:15,  4.44it/s]\u001b[A\n",
            "Iteration:  10% 68/669 [00:15<02:15,  4.43it/s]\u001b[A\n",
            "Iteration:  10% 69/669 [00:15<02:15,  4.43it/s]\u001b[A\n",
            "Iteration:  10% 70/669 [00:15<02:15,  4.44it/s]\u001b[A\n",
            "Iteration:  11% 71/669 [00:16<02:15,  4.41it/s]\u001b[A\n",
            "Iteration:  11% 72/669 [00:16<02:15,  4.41it/s]\u001b[A\n",
            "Iteration:  11% 73/669 [00:16<02:14,  4.43it/s]\u001b[A\n",
            "Iteration:  11% 74/669 [00:16<02:13,  4.44it/s]\u001b[A\n",
            "Iteration:  11% 75/669 [00:16<02:13,  4.45it/s]\u001b[A\n",
            "Iteration:  11% 76/669 [00:17<02:13,  4.44it/s]\u001b[A\n",
            "Iteration:  12% 77/669 [00:17<02:12,  4.45it/s]\u001b[A\n",
            "Iteration:  12% 78/669 [00:17<02:13,  4.44it/s]\u001b[A\n",
            "Iteration:  12% 79/669 [00:17<02:12,  4.45it/s]\u001b[A\n",
            "Iteration:  12% 80/669 [00:18<02:12,  4.43it/s]\u001b[A\n",
            "Iteration:  12% 81/669 [00:18<02:12,  4.45it/s]\u001b[A\n",
            "Iteration:  12% 82/669 [00:18<02:12,  4.44it/s]\u001b[A\n",
            "Iteration:  12% 83/669 [00:18<02:11,  4.45it/s]\u001b[A\n",
            "Iteration:  13% 84/669 [00:18<02:12,  4.43it/s]\u001b[A\n",
            "Iteration:  13% 85/669 [00:19<02:11,  4.45it/s]\u001b[A\n",
            "Iteration:  13% 86/669 [00:19<02:11,  4.43it/s]\u001b[A\n",
            "Iteration:  13% 87/669 [00:19<02:10,  4.45it/s]\u001b[A\n",
            "Iteration:  13% 88/669 [00:19<02:10,  4.44it/s]\u001b[A\n",
            "Iteration:  13% 89/669 [00:20<02:11,  4.42it/s]\u001b[A\n",
            "Iteration:  13% 90/669 [00:20<02:10,  4.44it/s]\u001b[A\n",
            "Iteration:  14% 91/669 [00:20<02:10,  4.44it/s]\u001b[A\n",
            "Iteration:  14% 92/669 [00:20<02:09,  4.44it/s]\u001b[A\n",
            "Iteration:  14% 93/669 [00:20<02:10,  4.42it/s]\u001b[A\n",
            "Iteration:  14% 94/669 [00:21<02:09,  4.44it/s]\u001b[A\n",
            "Iteration:  14% 95/669 [00:21<02:09,  4.45it/s]\u001b[A\n",
            "Iteration:  14% 96/669 [00:21<02:08,  4.44it/s]\u001b[A\n",
            "Iteration:  14% 97/669 [00:21<02:09,  4.43it/s]\u001b[A\n",
            "Iteration:  15% 98/669 [00:22<02:08,  4.44it/s]\u001b[A\n",
            "Iteration:  15% 99/669 [00:22<02:08,  4.44it/s]\u001b[A\n",
            "Iteration:  15% 100/669 [00:22<02:08,  4.44it/s]\u001b[A\n",
            "Iteration:  15% 101/669 [00:22<02:07,  4.44it/s]\u001b[A\n",
            "Iteration:  15% 102/669 [00:22<02:07,  4.44it/s]\u001b[A\n",
            "Iteration:  15% 103/669 [00:23<02:07,  4.45it/s]\u001b[A\n",
            "Iteration:  16% 104/669 [00:23<02:07,  4.45it/s]\u001b[A\n",
            "Iteration:  16% 105/669 [00:23<02:07,  4.44it/s]\u001b[A\n",
            "Iteration:  16% 106/669 [00:23<02:06,  4.44it/s]\u001b[A\n",
            "Iteration:  16% 107/669 [00:24<02:06,  4.44it/s]\u001b[A\n",
            "Iteration:  16% 108/669 [00:24<02:06,  4.44it/s]\u001b[A\n",
            "Iteration:  16% 109/669 [00:24<02:06,  4.44it/s]\u001b[A\n",
            "Iteration:  16% 110/669 [00:24<02:06,  4.42it/s]\u001b[A\n",
            "Iteration:  17% 111/669 [00:25<02:05,  4.44it/s]\u001b[A\n",
            "Iteration:  17% 112/669 [00:25<02:05,  4.44it/s]\u001b[A\n",
            "Iteration:  17% 113/669 [00:25<02:05,  4.45it/s]\u001b[A\n",
            "Iteration:  17% 114/669 [00:25<02:05,  4.43it/s]\u001b[A\n",
            "Iteration:  17% 115/669 [00:25<02:04,  4.45it/s]\u001b[A\n",
            "Iteration:  17% 116/669 [00:26<02:04,  4.45it/s]\u001b[A\n",
            "Iteration:  17% 117/669 [00:26<02:04,  4.44it/s]\u001b[A\n",
            "Iteration:  18% 118/669 [00:26<02:04,  4.44it/s]\u001b[A\n",
            "Iteration:  18% 119/669 [00:26<02:03,  4.45it/s]\u001b[A\n",
            "Iteration:  18% 120/669 [00:27<02:03,  4.44it/s]\u001b[A\n",
            "Iteration:  18% 121/669 [00:27<02:03,  4.44it/s]\u001b[A\n",
            "Iteration:  18% 122/669 [00:27<02:03,  4.44it/s]\u001b[A\n",
            "Iteration:  18% 123/669 [00:27<02:02,  4.45it/s]\u001b[A\n",
            "Iteration:  19% 124/669 [00:27<02:03,  4.43it/s]\u001b[A\n",
            "Iteration:  19% 125/669 [00:28<02:02,  4.45it/s]\u001b[A\n",
            "Iteration:  19% 126/669 [00:28<02:02,  4.44it/s]\u001b[A\n",
            "Iteration:  19% 127/669 [00:28<02:02,  4.43it/s]\u001b[A\n",
            "Iteration:  19% 128/669 [00:28<02:02,  4.43it/s]\u001b[A\n",
            "Iteration:  19% 129/669 [00:29<02:01,  4.44it/s]\u001b[A\n",
            "Iteration:  19% 130/669 [00:29<02:01,  4.44it/s]\u001b[A\n",
            "Iteration:  20% 131/669 [00:29<02:01,  4.44it/s]\u001b[A\n",
            "Iteration:  20% 132/669 [00:29<02:01,  4.43it/s]\u001b[A\n",
            "Iteration:  20% 133/669 [00:29<02:00,  4.45it/s]\u001b[A\n",
            "Iteration:  20% 134/669 [00:30<02:00,  4.45it/s]\u001b[A\n",
            "Iteration:  20% 135/669 [00:30<02:00,  4.44it/s]\u001b[A\n",
            "Iteration:  20% 136/669 [00:30<01:59,  4.45it/s]\u001b[A\n",
            "Iteration:  20% 137/669 [00:30<01:59,  4.44it/s]\u001b[A\n",
            "Iteration:  21% 138/669 [00:31<01:59,  4.43it/s]\u001b[A\n",
            "Iteration:  21% 139/669 [00:31<01:59,  4.45it/s]\u001b[A\n",
            "Iteration:  21% 140/669 [00:31<01:58,  4.45it/s]\u001b[A\n",
            "Iteration:  21% 141/669 [00:31<01:58,  4.44it/s]\u001b[A\n",
            "Iteration:  21% 142/669 [00:32<01:58,  4.45it/s]\u001b[A\n",
            "Iteration:  21% 143/669 [00:32<01:59,  4.42it/s]\u001b[A\n",
            "Iteration:  22% 144/669 [00:32<01:58,  4.42it/s]\u001b[A\n",
            "Iteration:  22% 145/669 [00:32<01:58,  4.43it/s]\u001b[A\n",
            "Iteration:  22% 146/669 [00:32<01:57,  4.45it/s]\u001b[A\n",
            "Iteration:  22% 147/669 [00:33<01:57,  4.44it/s]\u001b[A\n",
            "Iteration:  22% 148/669 [00:33<01:57,  4.42it/s]\u001b[A\n",
            "Iteration:  22% 149/669 [00:33<01:57,  4.44it/s]\u001b[A\n",
            "Iteration:  22% 150/669 [00:33<01:56,  4.45it/s]\u001b[A\n",
            "Iteration:  23% 151/669 [00:34<01:56,  4.44it/s]\u001b[A\n",
            "Iteration:  23% 152/669 [00:34<01:56,  4.44it/s]\u001b[A\n",
            "Iteration:  23% 153/669 [00:34<01:55,  4.45it/s]\u001b[A\n",
            "Iteration:  23% 154/669 [00:34<01:55,  4.45it/s]\u001b[A\n",
            "Iteration:  23% 155/669 [00:34<01:55,  4.44it/s]\u001b[A\n",
            "Iteration:  23% 156/669 [00:35<01:55,  4.45it/s]\u001b[A\n",
            "Iteration:  23% 157/669 [00:35<01:55,  4.43it/s]\u001b[A\n",
            "Iteration:  24% 158/669 [00:35<01:54,  4.44it/s]\u001b[A\n",
            "Iteration:  24% 159/669 [00:35<01:54,  4.45it/s]\u001b[A\n",
            "Iteration:  24% 160/669 [00:36<01:54,  4.45it/s]\u001b[A\n",
            "Iteration:  24% 161/669 [00:36<01:54,  4.44it/s]\u001b[A\n",
            "Iteration:  24% 162/669 [00:36<01:54,  4.44it/s]\u001b[A\n",
            "Iteration:  24% 163/669 [00:36<01:53,  4.45it/s]\u001b[A\n",
            "Iteration:  25% 164/669 [00:36<01:53,  4.45it/s]\u001b[A\n",
            "Iteration:  25% 165/669 [00:37<01:54,  4.42it/s]\u001b[A\n",
            "Iteration:  25% 166/669 [00:37<01:53,  4.45it/s]\u001b[A\n",
            "Iteration:  25% 167/669 [00:37<01:52,  4.45it/s]\u001b[A\n",
            "Iteration:  25% 168/669 [00:37<01:52,  4.45it/s]\u001b[A\n",
            "Iteration:  25% 169/669 [00:38<01:52,  4.43it/s]\u001b[A\n",
            "Iteration:  25% 170/669 [00:38<01:52,  4.45it/s]\u001b[A\n",
            "Iteration:  26% 171/669 [00:38<01:52,  4.45it/s]\u001b[A\n",
            "Iteration:  26% 172/669 [00:38<01:51,  4.45it/s]\u001b[A\n",
            "Iteration:  26% 173/669 [00:38<01:51,  4.45it/s]\u001b[A\n",
            "Iteration:  26% 174/669 [00:39<01:51,  4.43it/s]\u001b[A\n",
            "Iteration:  26% 175/669 [00:39<01:51,  4.44it/s]\u001b[A\n",
            "Iteration:  26% 176/669 [00:39<01:50,  4.44it/s]\u001b[A\n",
            "Iteration:  26% 177/669 [00:39<01:50,  4.44it/s]\u001b[A\n",
            "Iteration:  27% 178/669 [00:40<01:50,  4.44it/s]\u001b[A\n",
            "Iteration:  27% 179/669 [00:40<01:50,  4.45it/s]\u001b[A\n",
            "Iteration:  27% 180/669 [00:40<01:49,  4.45it/s]\u001b[A\n",
            "Iteration:  27% 181/669 [00:40<01:49,  4.45it/s]\u001b[A\n",
            "Iteration:  27% 182/669 [00:41<01:49,  4.44it/s]\u001b[A\n",
            "Iteration:  27% 183/669 [00:41<01:49,  4.44it/s]\u001b[A\n",
            "Iteration:  28% 184/669 [00:41<01:49,  4.45it/s]\u001b[A\n",
            "Iteration:  28% 185/669 [00:41<01:49,  4.44it/s]\u001b[A\n",
            "Iteration:  28% 186/669 [00:41<01:48,  4.44it/s]\u001b[A\n",
            "Iteration:  28% 187/669 [00:42<01:48,  4.44it/s]\u001b[A\n",
            "Iteration:  28% 188/669 [00:42<01:48,  4.44it/s]\u001b[A\n",
            "Iteration:  28% 189/669 [00:42<01:48,  4.44it/s]\u001b[A\n",
            "Iteration:  28% 190/669 [00:42<01:47,  4.44it/s]\u001b[A\n",
            "Iteration:  29% 191/669 [00:43<01:47,  4.45it/s]\u001b[A\n",
            "Iteration:  29% 192/669 [00:43<01:47,  4.45it/s]\u001b[A\n",
            "Iteration:  29% 193/669 [00:43<01:47,  4.45it/s]\u001b[A\n",
            "Iteration:  29% 194/669 [00:43<01:46,  4.45it/s]\u001b[A\n",
            "Iteration:  29% 195/669 [00:43<01:46,  4.44it/s]\u001b[A\n",
            "Iteration:  29% 196/669 [00:44<01:46,  4.43it/s]\u001b[A\n",
            "Iteration:  29% 197/669 [00:44<01:46,  4.44it/s]\u001b[A\n",
            "Iteration:  30% 198/669 [00:44<01:45,  4.45it/s]\u001b[A\n",
            "Iteration:  30% 199/669 [00:44<01:46,  4.42it/s]\u001b[A\n",
            "Iteration:  30% 200/669 [00:45<01:45,  4.45it/s]\u001b[A\n",
            "Iteration:  30% 201/669 [00:45<01:45,  4.45it/s]\u001b[A\n",
            "Iteration:  30% 202/669 [00:45<01:44,  4.45it/s]\u001b[A\n",
            "Iteration:  30% 203/669 [00:45<01:45,  4.44it/s]\u001b[A\n",
            "Iteration:  30% 204/669 [00:45<01:44,  4.45it/s]\u001b[A\n",
            "Iteration:  31% 205/669 [00:46<01:44,  4.43it/s]\u001b[A\n",
            "Iteration:  31% 206/669 [00:46<01:43,  4.45it/s]\u001b[A\n",
            "Iteration:  31% 207/669 [00:46<01:43,  4.44it/s]\u001b[A\n",
            "Iteration:  31% 208/669 [00:46<01:43,  4.45it/s]\u001b[A\n",
            "Iteration:  31% 209/669 [00:47<01:43,  4.45it/s]\u001b[A\n",
            "Iteration:  31% 210/669 [00:47<01:43,  4.45it/s]\u001b[A\n",
            "Iteration:  32% 211/669 [00:47<01:43,  4.44it/s]\u001b[A\n",
            "Iteration:  32% 212/669 [00:47<01:43,  4.43it/s]\u001b[A\n",
            "Iteration:  32% 213/669 [00:47<01:42,  4.44it/s]\u001b[A\n",
            "Iteration:  32% 214/669 [00:48<01:42,  4.45it/s]\u001b[A\n",
            "Iteration:  32% 215/669 [00:48<01:42,  4.45it/s]\u001b[A\n",
            "Iteration:  32% 216/669 [00:48<01:42,  4.43it/s]\u001b[A\n",
            "Iteration:  32% 217/669 [00:48<01:41,  4.44it/s]\u001b[A\n",
            "Iteration:  33% 218/669 [00:49<01:41,  4.45it/s]\u001b[A\n",
            "Iteration:  33% 219/669 [00:49<01:41,  4.43it/s]\u001b[A\n",
            "Iteration:  33% 220/669 [00:49<01:41,  4.43it/s]\u001b[A\n",
            "Iteration:  33% 221/669 [00:49<01:40,  4.45it/s]\u001b[A\n",
            "Iteration:  33% 222/669 [00:50<01:40,  4.44it/s]\u001b[A\n",
            "Iteration:  33% 223/669 [00:50<01:40,  4.45it/s]\u001b[A\n",
            "Iteration:  33% 224/669 [00:50<01:40,  4.43it/s]\u001b[A\n",
            "Iteration:  34% 225/669 [00:50<01:39,  4.45it/s]\u001b[A\n",
            "Iteration:  34% 226/669 [00:50<01:39,  4.44it/s]\u001b[A\n",
            "Iteration:  34% 227/669 [00:51<01:39,  4.45it/s]\u001b[A\n",
            "Iteration:  34% 228/669 [00:51<01:39,  4.45it/s]\u001b[A\n",
            "Iteration:  34% 229/669 [00:51<01:39,  4.43it/s]\u001b[A\n",
            "Iteration:  34% 230/669 [00:51<01:38,  4.44it/s]\u001b[A\n",
            "Iteration:  35% 231/669 [00:52<01:39,  4.42it/s]\u001b[A\n",
            "Iteration:  35% 232/669 [00:52<01:44,  4.19it/s]\u001b[A\n",
            "Iteration:  35% 233/669 [00:52<01:42,  4.27it/s]\u001b[A\n",
            "Iteration:  35% 234/669 [00:52<01:40,  4.32it/s]\u001b[A\n",
            "Iteration:  35% 235/669 [00:52<01:39,  4.36it/s]\u001b[A\n",
            "Iteration:  35% 236/669 [00:53<01:38,  4.39it/s]\u001b[A\n",
            "Iteration:  35% 237/669 [00:53<01:37,  4.41it/s]\u001b[A\n",
            "Iteration:  36% 238/669 [00:53<01:37,  4.42it/s]\u001b[A\n",
            "Iteration:  36% 239/669 [00:53<01:37,  4.43it/s]\u001b[A\n",
            "Iteration:  36% 240/669 [00:54<01:36,  4.43it/s]\u001b[A\n",
            "Iteration:  36% 241/669 [00:54<01:36,  4.44it/s]\u001b[A\n",
            "Iteration:  36% 242/669 [00:54<01:36,  4.44it/s]\u001b[A\n",
            "Iteration:  36% 243/669 [00:54<01:35,  4.44it/s]\u001b[A\n",
            "Iteration:  36% 244/669 [00:55<01:35,  4.44it/s]\u001b[A\n",
            "Iteration:  37% 245/669 [00:55<01:35,  4.44it/s]\u001b[A\n",
            "Iteration:  37% 246/669 [00:55<01:35,  4.44it/s]\u001b[A\n",
            "Iteration:  37% 247/669 [00:55<01:34,  4.44it/s]\u001b[A\n",
            "Iteration:  37% 248/669 [00:55<01:35,  4.42it/s]\u001b[A\n",
            "Iteration:  37% 249/669 [00:56<01:34,  4.45it/s]\u001b[A\n",
            "Iteration:  37% 250/669 [00:56<01:34,  4.45it/s]\u001b[A\n",
            "Iteration:  38% 251/669 [00:56<01:34,  4.44it/s]\u001b[A\n",
            "Iteration:  38% 252/669 [00:56<01:33,  4.45it/s]\u001b[A\n",
            "Iteration:  38% 253/669 [00:57<01:33,  4.45it/s]\u001b[A\n",
            "Iteration:  38% 254/669 [00:57<01:33,  4.44it/s]\u001b[A\n",
            "Iteration:  38% 255/669 [00:57<01:33,  4.45it/s]\u001b[A\n",
            "Iteration:  38% 256/669 [00:57<01:32,  4.44it/s]\u001b[A\n",
            "Iteration:  38% 257/669 [00:57<01:32,  4.44it/s]\u001b[A\n",
            "Iteration:  39% 258/669 [00:58<01:32,  4.45it/s]\u001b[A\n",
            "Iteration:  39% 259/669 [00:58<01:32,  4.44it/s]\u001b[A\n",
            "Iteration:  39% 260/669 [00:58<01:32,  4.42it/s]\u001b[A\n",
            "Iteration:  39% 261/669 [00:58<01:31,  4.44it/s]\u001b[A\n",
            "Iteration:  39% 262/669 [00:59<01:31,  4.45it/s]\u001b[A\n",
            "Iteration:  39% 263/669 [00:59<01:31,  4.45it/s]\u001b[A\n",
            "Iteration:  39% 264/669 [00:59<01:31,  4.44it/s]\u001b[A\n",
            "Iteration:  40% 265/669 [00:59<01:30,  4.44it/s]\u001b[A\n",
            "Iteration:  40% 266/669 [00:59<01:30,  4.45it/s]\u001b[A\n",
            "Iteration:  40% 267/669 [01:00<01:30,  4.43it/s]\u001b[A\n",
            "Iteration:  40% 268/669 [01:00<01:30,  4.45it/s]\u001b[A\n",
            "Iteration:  40% 269/669 [01:00<01:29,  4.45it/s]\u001b[A\n",
            "Iteration:  40% 270/669 [01:00<01:29,  4.45it/s]\u001b[A\n",
            "Iteration:  41% 271/669 [01:01<01:29,  4.45it/s]\u001b[A\n",
            "Iteration:  41% 272/669 [01:01<01:29,  4.45it/s]\u001b[A\n",
            "Iteration:  41% 273/669 [01:01<01:29,  4.44it/s]\u001b[A\n",
            "Iteration:  41% 274/669 [01:01<01:29,  4.42it/s]\u001b[A\n",
            "Iteration:  41% 275/669 [01:01<01:28,  4.44it/s]\u001b[A\n",
            "Iteration:  41% 276/669 [01:02<01:28,  4.45it/s]\u001b[A\n",
            "Iteration:  41% 277/669 [01:02<01:28,  4.45it/s]\u001b[A\n",
            "Iteration:  42% 278/669 [01:02<01:27,  4.45it/s]\u001b[A\n",
            "Iteration:  42% 279/669 [01:02<01:27,  4.44it/s]\u001b[A\n",
            "Iteration:  42% 280/669 [01:03<01:27,  4.45it/s]\u001b[A\n",
            "Iteration:  42% 281/669 [01:03<01:27,  4.45it/s]\u001b[A\n",
            "Iteration:  42% 282/669 [01:03<01:27,  4.41it/s]\u001b[A\n",
            "Iteration:  42% 283/669 [01:03<01:26,  4.44it/s]\u001b[A\n",
            "Iteration:  42% 284/669 [01:04<01:26,  4.44it/s]\u001b[A\n",
            "Iteration:  43% 285/669 [01:04<01:26,  4.45it/s]\u001b[A\n",
            "Iteration:  43% 286/669 [01:04<01:26,  4.43it/s]\u001b[A\n",
            "Iteration:  43% 287/669 [01:04<01:25,  4.44it/s]\u001b[A\n",
            "Iteration:  43% 288/669 [01:04<01:25,  4.44it/s]\u001b[A\n",
            "Iteration:  43% 289/669 [01:05<01:25,  4.45it/s]\u001b[A\n",
            "Iteration:  43% 290/669 [01:05<01:25,  4.41it/s]\u001b[A\n",
            "Iteration:  43% 291/669 [01:05<01:25,  4.44it/s]\u001b[A\n",
            "Iteration:  44% 292/669 [01:05<01:24,  4.44it/s]\u001b[A\n",
            "Iteration:  44% 293/669 [01:06<01:24,  4.45it/s]\u001b[A\n",
            "Iteration:  44% 294/669 [01:06<01:24,  4.45it/s]\u001b[A\n",
            "Iteration:  44% 295/669 [01:06<01:24,  4.43it/s]\u001b[A\n",
            "Iteration:  44% 296/669 [01:06<01:24,  4.44it/s]\u001b[A\n",
            "Iteration:  44% 297/669 [01:06<01:23,  4.44it/s]\u001b[A\n",
            "Iteration:  45% 298/669 [01:07<01:23,  4.44it/s]\u001b[A\n",
            "Iteration:  45% 299/669 [01:07<01:23,  4.42it/s]\u001b[A\n",
            "Iteration:  45% 300/669 [01:07<01:23,  4.44it/s]\u001b[A\n",
            "Iteration:  45% 301/669 [01:07<01:22,  4.43it/s]\u001b[A\n",
            "Iteration:  45% 302/669 [01:08<01:22,  4.44it/s]\u001b[A\n",
            "Iteration:  45% 303/669 [01:08<01:22,  4.43it/s]\u001b[A\n",
            "Iteration:  45% 304/669 [01:08<01:22,  4.44it/s]\u001b[A\n",
            "Iteration:  46% 305/669 [01:08<01:21,  4.44it/s]\u001b[A\n",
            "Iteration:  46% 306/669 [01:08<01:21,  4.44it/s]\u001b[A\n",
            "Iteration:  46% 307/669 [01:09<01:21,  4.42it/s]\u001b[A\n",
            "Iteration:  46% 308/669 [01:09<01:21,  4.45it/s]\u001b[A\n",
            "Iteration:  46% 309/669 [01:09<01:20,  4.45it/s]\u001b[A\n",
            "Iteration:  46% 310/669 [01:09<01:20,  4.45it/s]\u001b[A\n",
            "Iteration:  46% 311/669 [01:10<01:20,  4.44it/s]\u001b[A\n",
            "Iteration:  47% 312/669 [01:10<01:20,  4.45it/s]\u001b[A\n",
            "Iteration:  47% 313/669 [01:10<01:20,  4.45it/s]\u001b[A\n",
            "Iteration:  47% 314/669 [01:10<01:19,  4.44it/s]\u001b[A\n",
            "Iteration:  47% 315/669 [01:10<01:19,  4.43it/s]\u001b[A\n",
            "Iteration:  47% 316/669 [01:11<01:19,  4.44it/s]\u001b[A\n",
            "Iteration:  47% 317/669 [01:11<01:19,  4.44it/s]\u001b[A\n",
            "Iteration:  48% 318/669 [01:11<01:18,  4.44it/s]\u001b[A\n",
            "Iteration:  48% 319/669 [01:11<01:18,  4.44it/s]\u001b[A\n",
            "Iteration:  48% 320/669 [01:12<01:18,  4.42it/s]\u001b[A\n",
            "Iteration:  48% 321/669 [01:12<01:18,  4.45it/s]\u001b[A\n",
            "Iteration:  48% 322/669 [01:12<01:18,  4.44it/s]\u001b[A\n",
            "Iteration:  48% 323/669 [01:12<01:17,  4.45it/s]\u001b[A\n",
            "Iteration:  48% 324/669 [01:13<01:17,  4.43it/s]\u001b[A\n",
            "Iteration:  49% 325/669 [01:13<01:17,  4.45it/s]\u001b[A\n",
            "Iteration:  49% 326/669 [01:13<01:17,  4.45it/s]\u001b[A\n",
            "Iteration:  49% 327/669 [01:13<01:16,  4.44it/s]\u001b[A\n",
            "Iteration:  49% 328/669 [01:13<01:16,  4.43it/s]\u001b[A\n",
            "Iteration:  49% 329/669 [01:14<01:16,  4.45it/s]\u001b[A\n",
            "Iteration:  49% 330/669 [01:14<01:16,  4.45it/s]\u001b[A\n",
            "Iteration:  49% 331/669 [01:14<01:16,  4.44it/s]\u001b[A\n",
            "Iteration:  50% 332/669 [01:14<01:15,  4.45it/s]\u001b[A\n",
            "Iteration:  50% 333/669 [01:15<01:15,  4.44it/s]\u001b[A\n",
            "Iteration:  50% 334/669 [01:15<01:15,  4.44it/s]\u001b[A\n",
            "Iteration:  50% 335/669 [01:15<01:15,  4.44it/s]\u001b[A\n",
            "Iteration:  50% 336/669 [01:15<01:14,  4.44it/s]\u001b[A\n",
            "Iteration:  50% 337/669 [01:15<01:15,  4.39it/s]\u001b[A\n",
            "Iteration:  51% 338/669 [01:16<01:14,  4.42it/s]\u001b[A\n",
            "Iteration:  51% 339/669 [01:16<01:14,  4.42it/s]\u001b[A\n",
            "Iteration:  51% 340/669 [01:16<01:14,  4.43it/s]\u001b[A\n",
            "Iteration:  51% 341/669 [01:16<01:14,  4.42it/s]\u001b[A\n",
            "Iteration:  51% 342/669 [01:17<01:13,  4.44it/s]\u001b[A\n",
            "Iteration:  51% 343/669 [01:17<01:13,  4.44it/s]\u001b[A\n",
            "Iteration:  51% 344/669 [01:17<01:13,  4.44it/s]\u001b[A\n",
            "Iteration:  52% 345/669 [01:17<01:13,  4.43it/s]\u001b[A\n",
            "Iteration:  52% 346/669 [01:17<01:12,  4.44it/s]\u001b[A\n",
            "Iteration:  52% 347/669 [01:18<01:12,  4.44it/s]\u001b[A\n",
            "Iteration:  52% 348/669 [01:18<01:12,  4.44it/s]\u001b[A\n",
            "Iteration:  52% 349/669 [01:18<01:12,  4.42it/s]\u001b[A\n",
            "Iteration:  52% 350/669 [01:18<01:12,  4.43it/s]\u001b[A\n",
            "Iteration:  52% 351/669 [01:19<01:11,  4.44it/s]\u001b[A\n",
            "Iteration:  53% 352/669 [01:19<01:11,  4.43it/s]\u001b[A\n",
            "Iteration:  53% 353/669 [01:19<01:11,  4.44it/s]\u001b[A\n",
            "Iteration:  53% 354/669 [01:19<01:10,  4.45it/s]\u001b[A\n",
            "Iteration:  53% 355/669 [01:20<01:10,  4.45it/s]\u001b[A\n",
            "Iteration:  53% 356/669 [01:20<01:10,  4.45it/s]\u001b[A\n",
            "Iteration:  53% 357/669 [01:20<01:10,  4.44it/s]\u001b[A\n",
            "Iteration:  54% 358/669 [01:20<01:09,  4.44it/s]\u001b[A\n",
            "Iteration:  54% 359/669 [01:20<01:09,  4.43it/s]\u001b[A\n",
            "Iteration:  54% 360/669 [01:21<01:09,  4.44it/s]\u001b[A\n",
            "Iteration:  54% 361/669 [01:21<01:09,  4.44it/s]\u001b[A\n",
            "Iteration:  54% 362/669 [01:21<01:09,  4.43it/s]\u001b[A\n",
            "Iteration:  54% 363/669 [01:21<01:09,  4.43it/s]\u001b[A\n",
            "Iteration:  54% 364/669 [01:22<01:08,  4.44it/s]\u001b[A\n",
            "Iteration:  55% 365/669 [01:22<01:08,  4.44it/s]\u001b[A\n",
            "Iteration:  55% 366/669 [01:22<01:08,  4.43it/s]\u001b[A\n",
            "Iteration:  55% 367/669 [01:22<01:07,  4.45it/s]\u001b[A\n",
            "Iteration:  55% 368/669 [01:22<01:07,  4.45it/s]\u001b[A\n",
            "Iteration:  55% 369/669 [01:23<01:07,  4.45it/s]\u001b[A\n",
            "Iteration:  55% 370/669 [01:23<01:07,  4.43it/s]\u001b[A\n",
            "Iteration:  55% 371/669 [01:23<01:06,  4.45it/s]\u001b[A\n",
            "Iteration:  56% 372/669 [01:23<01:06,  4.45it/s]\u001b[A\n",
            "Iteration:  56% 373/669 [01:24<01:06,  4.44it/s]\u001b[A\n",
            "Iteration:  56% 374/669 [01:24<01:06,  4.45it/s]\u001b[A\n",
            "Iteration:  56% 375/669 [01:24<01:06,  4.45it/s]\u001b[A\n",
            "Iteration:  56% 376/669 [01:24<01:06,  4.43it/s]\u001b[A\n",
            "Iteration:  56% 377/669 [01:24<01:05,  4.45it/s]\u001b[A\n",
            "Iteration:  57% 378/669 [01:25<01:05,  4.45it/s]\u001b[A\n",
            "Iteration:  57% 379/669 [01:25<01:05,  4.44it/s]\u001b[A\n",
            "Iteration:  57% 380/669 [01:25<01:04,  4.45it/s]\u001b[A\n",
            "Iteration:  57% 381/669 [01:25<01:04,  4.45it/s]\u001b[A\n",
            "Iteration:  57% 382/669 [01:26<01:04,  4.42it/s]\u001b[A\n",
            "Iteration:  57% 383/669 [01:26<01:04,  4.44it/s]\u001b[A\n",
            "Iteration:  57% 384/669 [01:26<01:04,  4.45it/s]\u001b[A\n",
            "Iteration:  58% 385/669 [01:26<01:03,  4.45it/s]\u001b[A\n",
            "Iteration:  58% 386/669 [01:26<01:03,  4.44it/s]\u001b[A\n",
            "Iteration:  58% 387/669 [01:27<01:03,  4.43it/s]\u001b[A\n",
            "Iteration:  58% 388/669 [01:27<01:03,  4.43it/s]\u001b[A\n",
            "Iteration:  58% 389/669 [01:27<01:02,  4.44it/s]\u001b[A\n",
            "Iteration:  58% 390/669 [01:27<01:02,  4.45it/s]\u001b[A\n",
            "Iteration:  58% 391/669 [01:28<01:02,  4.43it/s]\u001b[A\n",
            "Iteration:  59% 392/669 [01:28<01:02,  4.45it/s]\u001b[A\n",
            "Iteration:  59% 393/669 [01:28<01:02,  4.43it/s]\u001b[A\n",
            "Iteration:  59% 394/669 [01:28<01:01,  4.44it/s]\u001b[A\n",
            "Iteration:  59% 395/669 [01:29<01:01,  4.44it/s]\u001b[A\n",
            "Iteration:  59% 396/669 [01:29<01:01,  4.43it/s]\u001b[A\n",
            "Iteration:  59% 397/669 [01:29<01:01,  4.44it/s]\u001b[A\n",
            "Iteration:  59% 398/669 [01:29<01:01,  4.44it/s]\u001b[A\n",
            "Iteration:  60% 399/669 [01:29<01:00,  4.45it/s]\u001b[A\n",
            "Iteration:  60% 400/669 [01:30<01:00,  4.44it/s]\u001b[A\n",
            "Iteration:  60% 401/669 [01:30<01:00,  4.45it/s]\u001b[A\n",
            "Iteration:  60% 402/669 [01:30<01:00,  4.44it/s]\u001b[A\n",
            "Iteration:  60% 403/669 [01:30<01:00,  4.42it/s]\u001b[A\n",
            "Iteration:  60% 404/669 [01:31<00:59,  4.43it/s]\u001b[A\n",
            "Iteration:  61% 405/669 [01:31<00:59,  4.44it/s]\u001b[A\n",
            "Iteration:  61% 406/669 [01:31<00:59,  4.44it/s]\u001b[A\n",
            "Iteration:  61% 407/669 [01:31<00:58,  4.45it/s]\u001b[A\n",
            "Iteration:  61% 408/669 [01:31<00:58,  4.45it/s]\u001b[A\n",
            "Iteration:  61% 409/669 [01:32<00:58,  4.42it/s]\u001b[A\n",
            "Iteration:  61% 410/669 [01:32<00:58,  4.41it/s]\u001b[A\n",
            "Iteration:  61% 411/669 [01:32<00:58,  4.41it/s]\u001b[A\n",
            "Iteration:  62% 412/669 [01:32<00:58,  4.41it/s]\u001b[A\n",
            "Iteration:  62% 413/669 [01:33<00:57,  4.44it/s]\u001b[A\n",
            "Iteration:  62% 414/669 [01:33<00:57,  4.44it/s]\u001b[A\n",
            "Iteration:  62% 415/669 [01:33<00:57,  4.45it/s]\u001b[A\n",
            "Iteration:  62% 416/669 [01:33<00:56,  4.45it/s]\u001b[A\n",
            "Iteration:  62% 417/669 [01:33<00:56,  4.44it/s]\u001b[A\n",
            "Iteration:  62% 418/669 [01:34<00:56,  4.43it/s]\u001b[A\n",
            "Iteration:  63% 419/669 [01:34<00:56,  4.44it/s]\u001b[A\n",
            "Iteration:  63% 420/669 [01:34<00:56,  4.44it/s]\u001b[A\n",
            "Iteration:  63% 421/669 [01:34<00:55,  4.44it/s]\u001b[A\n",
            "Iteration:  63% 422/669 [01:35<00:55,  4.44it/s]\u001b[A\n",
            "Iteration:  63% 423/669 [01:35<00:55,  4.42it/s]\u001b[A\n",
            "Iteration:  63% 424/669 [01:35<00:55,  4.43it/s]\u001b[A\n",
            "Iteration:  64% 425/669 [01:35<00:54,  4.45it/s]\u001b[A\n",
            "Iteration:  64% 426/669 [01:36<00:54,  4.45it/s]\u001b[A\n",
            "Iteration:  64% 427/669 [01:36<00:54,  4.44it/s]\u001b[A\n",
            "Iteration:  64% 428/669 [01:36<00:54,  4.45it/s]\u001b[A\n",
            "Iteration:  64% 429/669 [01:36<00:54,  4.42it/s]\u001b[A\n",
            "Iteration:  64% 430/669 [01:36<00:53,  4.45it/s]\u001b[A\n",
            "Iteration:  64% 431/669 [01:37<00:53,  4.45it/s]\u001b[A\n",
            "Iteration:  65% 432/669 [01:37<00:53,  4.45it/s]\u001b[A\n",
            "Iteration:  65% 433/669 [01:37<00:53,  4.44it/s]\u001b[A\n",
            "Iteration:  65% 434/669 [01:37<00:52,  4.44it/s]\u001b[A\n",
            "Iteration:  65% 435/669 [01:38<00:52,  4.44it/s]\u001b[A\n",
            "Iteration:  65% 436/669 [01:38<00:52,  4.44it/s]\u001b[A\n",
            "Iteration:  65% 437/669 [01:38<00:52,  4.44it/s]\u001b[A\n",
            "Iteration:  65% 438/669 [01:38<00:52,  4.43it/s]\u001b[A\n",
            "Iteration:  66% 439/669 [01:38<00:51,  4.44it/s]\u001b[A\n",
            "Iteration:  66% 440/669 [01:39<00:51,  4.44it/s]\u001b[A\n",
            "Iteration:  66% 441/669 [01:39<00:51,  4.43it/s]\u001b[A\n",
            "Iteration:  66% 442/669 [01:39<00:51,  4.45it/s]\u001b[A\n",
            "Iteration:  66% 443/669 [01:39<00:50,  4.45it/s]\u001b[A\n",
            "Iteration:  66% 444/669 [01:40<00:50,  4.45it/s]\u001b[A\n",
            "Iteration:  67% 445/669 [01:40<00:50,  4.44it/s]\u001b[A\n",
            "Iteration:  67% 446/669 [01:40<00:50,  4.43it/s]\u001b[A\n",
            "Iteration:  67% 447/669 [01:40<00:49,  4.45it/s]\u001b[A\n",
            "Iteration:  67% 448/669 [01:40<00:49,  4.43it/s]\u001b[A\n",
            "Iteration:  67% 449/669 [01:41<00:49,  4.45it/s]\u001b[A\n",
            "Iteration:  67% 450/669 [01:41<00:49,  4.44it/s]\u001b[A\n",
            "Iteration:  67% 451/669 [01:41<00:49,  4.44it/s]\u001b[A\n",
            "Iteration:  68% 452/669 [01:41<00:48,  4.45it/s]\u001b[A\n",
            "Iteration:  68% 453/669 [01:42<00:48,  4.44it/s]\u001b[A\n",
            "Iteration:  68% 454/669 [01:42<00:48,  4.45it/s]\u001b[A\n",
            "Iteration:  68% 455/669 [01:42<00:48,  4.43it/s]\u001b[A\n",
            "Iteration:  68% 456/669 [01:42<00:48,  4.43it/s]\u001b[A\n",
            "Iteration:  68% 457/669 [01:42<00:47,  4.43it/s]\u001b[A\n",
            "Iteration:  68% 458/669 [01:43<00:47,  4.45it/s]\u001b[A\n",
            "Iteration:  69% 459/669 [01:43<00:47,  4.42it/s]\u001b[A\n",
            "Iteration:  69% 460/669 [01:43<00:47,  4.45it/s]\u001b[A\n",
            "Iteration:  69% 461/669 [01:43<00:46,  4.44it/s]\u001b[A\n",
            "Iteration:  69% 462/669 [01:44<00:46,  4.45it/s]\u001b[A\n",
            "Iteration:  69% 463/669 [01:44<00:46,  4.43it/s]\u001b[A\n",
            "Iteration:  69% 464/669 [01:44<00:46,  4.45it/s]\u001b[A\n",
            "Iteration:  70% 465/669 [01:44<00:45,  4.45it/s]\u001b[A\n",
            "Iteration:  70% 466/669 [01:45<00:45,  4.45it/s]\u001b[A\n",
            "Iteration:  70% 467/669 [01:45<00:45,  4.45it/s]\u001b[A\n",
            "Iteration:  70% 468/669 [01:45<00:45,  4.44it/s]\u001b[A\n",
            "Iteration:  70% 469/669 [01:45<00:45,  4.44it/s]\u001b[A\n",
            "Iteration:  70% 470/669 [01:45<00:44,  4.43it/s]\u001b[A\n",
            "Iteration:  70% 471/669 [01:46<00:44,  4.43it/s]\u001b[A\n",
            "Iteration:  71% 472/669 [01:46<00:44,  4.44it/s]\u001b[A\n",
            "Iteration:  71% 473/669 [01:46<00:44,  4.45it/s]\u001b[A\n",
            "Iteration:  71% 474/669 [01:46<00:43,  4.44it/s]\u001b[A\n",
            "Iteration:  71% 475/669 [01:47<00:43,  4.44it/s]\u001b[A\n",
            "Iteration:  71% 476/669 [01:47<00:43,  4.44it/s]\u001b[A\n",
            "Iteration:  71% 477/669 [01:47<00:43,  4.44it/s]\u001b[A\n",
            "Iteration:  71% 478/669 [01:47<00:42,  4.45it/s]\u001b[A\n",
            "Iteration:  72% 479/669 [01:47<00:42,  4.45it/s]\u001b[A\n",
            "Iteration:  72% 480/669 [01:48<00:42,  4.45it/s]\u001b[A\n",
            "Iteration:  72% 481/669 [01:48<00:42,  4.45it/s]\u001b[A\n",
            "Iteration:  72% 482/669 [01:48<00:42,  4.45it/s]\u001b[A\n",
            "Iteration:  72% 483/669 [01:48<00:41,  4.45it/s]\u001b[A\n",
            "Iteration:  72% 484/669 [01:49<00:41,  4.43it/s]\u001b[A\n",
            "Iteration:  72% 485/669 [01:49<00:41,  4.44it/s]\u001b[A\n",
            "Iteration:  73% 486/669 [01:49<00:41,  4.44it/s]\u001b[A\n",
            "Iteration:  73% 487/669 [01:49<00:40,  4.45it/s]\u001b[A\n",
            "Iteration:  73% 488/669 [01:49<00:40,  4.45it/s]\u001b[A\n",
            "Iteration:  73% 489/669 [01:50<00:40,  4.45it/s]\u001b[A\n",
            "Iteration:  73% 490/669 [01:50<00:40,  4.45it/s]\u001b[A\n",
            "Iteration:  73% 491/669 [01:50<00:40,  4.45it/s]\u001b[A\n",
            "Iteration:  74% 492/669 [01:50<00:39,  4.45it/s]\u001b[A\n",
            "Iteration:  74% 493/669 [01:51<00:39,  4.44it/s]\u001b[A\n",
            "Iteration:  74% 494/669 [01:51<00:39,  4.45it/s]\u001b[A\n",
            "Iteration:  74% 495/669 [01:51<00:39,  4.45it/s]\u001b[A\n",
            "Iteration:  74% 496/669 [01:51<00:38,  4.45it/s]\u001b[A\n",
            "Iteration:  74% 497/669 [01:51<00:38,  4.44it/s]\u001b[A\n",
            "Iteration:  74% 498/669 [01:52<00:38,  4.43it/s]\u001b[A\n",
            "Iteration:  75% 499/669 [01:52<00:38,  4.41it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "03/05/2020 05:52:58 - INFO - transformers.configuration_utils -   Configuration saved in output_gpt_trump/checkpoint-500/config.json\n",
            "03/05/2020 05:52:59 - INFO - transformers.modeling_utils -   Model weights saved in output_gpt_trump/checkpoint-500/pytorch_model.bin\n",
            "03/05/2020 05:52:59 - INFO - __main__ -   Saving model checkpoint to output_gpt_trump/checkpoint-500\n",
            "03/05/2020 05:53:02 - INFO - __main__ -   Saving optimizer and scheduler states to output_gpt_trump/checkpoint-500\n",
            "\n",
            "Iteration:  75% 500/669 [01:57<04:28,  1.59s/it]\u001b[A\n",
            "Iteration:  75% 501/669 [01:57<03:18,  1.18s/it]\u001b[A\n",
            "Iteration:  75% 502/669 [01:57<02:29,  1.12it/s]\u001b[A\n",
            "Iteration:  75% 503/669 [01:57<01:55,  1.44it/s]\u001b[A\n",
            "Iteration:  75% 504/669 [01:58<01:31,  1.81it/s]\u001b[A\n",
            "Iteration:  75% 505/669 [01:58<01:14,  2.19it/s]\u001b[A\n",
            "Iteration:  76% 506/669 [01:58<01:03,  2.58it/s]\u001b[A\n",
            "Iteration:  76% 507/669 [01:58<00:54,  2.95it/s]\u001b[A\n",
            "Iteration:  76% 508/669 [01:59<00:49,  3.28it/s]\u001b[A\n",
            "Iteration:  76% 509/669 [01:59<00:45,  3.55it/s]\u001b[A\n",
            "Iteration:  76% 510/669 [01:59<00:41,  3.79it/s]\u001b[A\n",
            "Iteration:  76% 511/669 [01:59<00:39,  3.96it/s]\u001b[A\n",
            "Iteration:  77% 512/669 [01:59<00:38,  4.09it/s]\u001b[A\n",
            "Iteration:  77% 513/669 [02:00<00:37,  4.18it/s]\u001b[A\n",
            "Iteration:  77% 514/669 [02:00<00:36,  4.26it/s]\u001b[A\n",
            "Iteration:  77% 515/669 [02:00<00:35,  4.31it/s]\u001b[A\n",
            "Iteration:  77% 516/669 [02:00<00:35,  4.35it/s]\u001b[A\n",
            "Iteration:  77% 517/669 [02:01<00:34,  4.37it/s]\u001b[A\n",
            "Iteration:  77% 518/669 [02:01<00:34,  4.40it/s]\u001b[A\n",
            "Iteration:  78% 519/669 [02:01<00:34,  4.41it/s]\u001b[A\n",
            "Iteration:  78% 520/669 [02:01<00:33,  4.41it/s]\u001b[A\n",
            "Iteration:  78% 521/669 [02:01<00:33,  4.43it/s]\u001b[A\n",
            "Iteration:  78% 522/669 [02:02<00:33,  4.43it/s]\u001b[A\n",
            "Iteration:  78% 523/669 [02:02<00:32,  4.43it/s]\u001b[A\n",
            "Iteration:  78% 524/669 [02:02<00:32,  4.43it/s]\u001b[A\n",
            "Iteration:  78% 525/669 [02:02<00:32,  4.43it/s]\u001b[A\n",
            "Iteration:  79% 526/669 [02:03<00:32,  4.41it/s]\u001b[A\n",
            "Iteration:  79% 527/669 [02:03<00:32,  4.44it/s]\u001b[A\n",
            "Iteration:  79% 528/669 [02:03<00:31,  4.44it/s]\u001b[A\n",
            "Iteration:  79% 529/669 [02:03<00:31,  4.44it/s]\u001b[A\n",
            "Iteration:  79% 530/669 [02:03<00:31,  4.42it/s]\u001b[A\n",
            "Iteration:  79% 531/669 [02:04<00:31,  4.44it/s]\u001b[A\n",
            "Iteration:  80% 532/669 [02:04<00:30,  4.44it/s]\u001b[A\n",
            "Iteration:  80% 533/669 [02:04<00:30,  4.44it/s]\u001b[A\n",
            "Iteration:  80% 534/669 [02:04<00:30,  4.43it/s]\u001b[A\n",
            "Iteration:  80% 535/669 [02:05<00:30,  4.44it/s]\u001b[A\n",
            "Iteration:  80% 536/669 [02:05<00:30,  4.42it/s]\u001b[A\n",
            "Iteration:  80% 537/669 [02:05<00:29,  4.44it/s]\u001b[A\n",
            "Iteration:  80% 538/669 [02:05<00:29,  4.44it/s]\u001b[A\n",
            "Iteration:  81% 539/669 [02:06<00:29,  4.45it/s]\u001b[A\n",
            "Iteration:  81% 540/669 [02:06<00:29,  4.43it/s]\u001b[A\n",
            "Iteration:  81% 541/669 [02:06<00:28,  4.44it/s]\u001b[A\n",
            "Iteration:  81% 542/669 [02:06<00:28,  4.43it/s]\u001b[A\n",
            "Iteration:  81% 543/669 [02:06<00:28,  4.43it/s]\u001b[A\n",
            "Iteration:  81% 544/669 [02:07<00:28,  4.43it/s]\u001b[A\n",
            "Iteration:  81% 545/669 [02:07<00:27,  4.44it/s]\u001b[A\n",
            "Iteration:  82% 546/669 [02:07<00:27,  4.44it/s]\u001b[A\n",
            "Iteration:  82% 547/669 [02:07<00:27,  4.43it/s]\u001b[A\n",
            "Iteration:  82% 548/669 [02:08<00:27,  4.44it/s]\u001b[A\n",
            "Iteration:  82% 549/669 [02:08<00:27,  4.43it/s]\u001b[A\n",
            "Iteration:  82% 550/669 [02:08<00:26,  4.43it/s]\u001b[A\n",
            "Iteration:  82% 551/669 [02:08<00:26,  4.43it/s]\u001b[A\n",
            "Iteration:  83% 552/669 [02:08<00:26,  4.43it/s]\u001b[A\n",
            "Iteration:  83% 553/669 [02:09<00:26,  4.44it/s]\u001b[A\n",
            "Iteration:  83% 554/669 [02:09<00:26,  4.40it/s]\u001b[A\n",
            "Iteration:  83% 555/669 [02:09<00:25,  4.43it/s]\u001b[A\n",
            "Iteration:  83% 556/669 [02:09<00:25,  4.43it/s]\u001b[A\n",
            "Iteration:  83% 557/669 [02:10<00:25,  4.43it/s]\u001b[A\n",
            "Iteration:  83% 558/669 [02:10<00:25,  4.44it/s]\u001b[A\n",
            "Iteration:  84% 559/669 [02:10<00:24,  4.43it/s]\u001b[A\n",
            "Iteration:  84% 560/669 [02:10<00:24,  4.45it/s]\u001b[A\n",
            "Iteration:  84% 561/669 [02:10<00:24,  4.43it/s]\u001b[A\n",
            "Iteration:  84% 562/669 [02:11<00:24,  4.45it/s]\u001b[A\n",
            "Iteration:  84% 563/669 [02:11<00:23,  4.43it/s]\u001b[A\n",
            "Iteration:  84% 564/669 [02:11<00:23,  4.45it/s]\u001b[A\n",
            "Iteration:  84% 565/669 [02:11<00:23,  4.45it/s]\u001b[A\n",
            "Iteration:  85% 566/669 [02:12<00:23,  4.44it/s]\u001b[A\n",
            "Iteration:  85% 567/669 [02:12<00:23,  4.42it/s]\u001b[A\n",
            "Iteration:  85% 568/669 [02:12<00:22,  4.45it/s]\u001b[A\n",
            "Iteration:  85% 569/669 [02:12<00:22,  4.45it/s]\u001b[A\n",
            "Iteration:  85% 570/669 [02:13<00:22,  4.44it/s]\u001b[A\n",
            "Iteration:  85% 571/669 [02:13<00:22,  4.45it/s]\u001b[A\n",
            "Iteration:  86% 572/669 [02:13<00:21,  4.43it/s]\u001b[A\n",
            "Iteration:  86% 573/669 [02:13<00:21,  4.43it/s]\u001b[A\n",
            "Iteration:  86% 574/669 [02:13<00:21,  4.45it/s]\u001b[A\n",
            "Iteration:  86% 575/669 [02:14<00:21,  4.44it/s]\u001b[A\n",
            "Iteration:  86% 576/669 [02:14<00:20,  4.43it/s]\u001b[A\n",
            "Iteration:  86% 577/669 [02:14<00:20,  4.45it/s]\u001b[A\n",
            "Iteration:  86% 578/669 [02:14<00:20,  4.45it/s]\u001b[A\n",
            "Iteration:  87% 579/669 [02:15<00:20,  4.45it/s]\u001b[A\n",
            "Iteration:  87% 580/669 [02:15<00:20,  4.44it/s]\u001b[A\n",
            "Iteration:  87% 581/669 [02:15<00:19,  4.44it/s]\u001b[A\n",
            "Iteration:  87% 582/669 [02:15<00:19,  4.45it/s]\u001b[A\n",
            "Iteration:  87% 583/669 [02:15<00:19,  4.42it/s]\u001b[A\n",
            "Iteration:  87% 584/669 [02:16<00:19,  4.42it/s]\u001b[A\n",
            "Iteration:  87% 585/669 [02:16<00:18,  4.42it/s]\u001b[A\n",
            "Iteration:  88% 586/669 [02:16<00:18,  4.43it/s]\u001b[A\n",
            "Iteration:  88% 587/669 [02:16<00:18,  4.45it/s]\u001b[A\n",
            "Iteration:  88% 588/669 [02:17<00:18,  4.44it/s]\u001b[A\n",
            "Iteration:  88% 589/669 [02:17<00:17,  4.45it/s]\u001b[A\n",
            "Iteration:  88% 590/669 [02:17<00:17,  4.45it/s]\u001b[A\n",
            "Iteration:  88% 591/669 [02:17<00:17,  4.44it/s]\u001b[A\n",
            "Iteration:  88% 592/669 [02:17<00:17,  4.44it/s]\u001b[A\n",
            "Iteration:  89% 593/669 [02:18<00:17,  4.44it/s]\u001b[A\n",
            "Iteration:  89% 594/669 [02:18<00:16,  4.44it/s]\u001b[A\n",
            "Iteration:  89% 595/669 [02:18<00:16,  4.44it/s]\u001b[A\n",
            "Iteration:  89% 596/669 [02:18<00:16,  4.45it/s]\u001b[A\n",
            "Iteration:  89% 597/669 [02:19<00:16,  4.44it/s]\u001b[A\n",
            "Iteration:  89% 598/669 [02:19<00:15,  4.45it/s]\u001b[A\n",
            "Iteration:  90% 599/669 [02:19<00:15,  4.45it/s]\u001b[A\n",
            "Iteration:  90% 600/669 [02:19<00:15,  4.45it/s]\u001b[A\n",
            "Iteration:  90% 601/669 [02:19<00:15,  4.44it/s]\u001b[A\n",
            "Iteration:  90% 602/669 [02:20<00:15,  4.44it/s]\u001b[A\n",
            "Iteration:  90% 603/669 [02:20<00:14,  4.45it/s]\u001b[A\n",
            "Iteration:  90% 604/669 [02:20<00:14,  4.45it/s]\u001b[A\n",
            "Iteration:  90% 605/669 [02:20<00:14,  4.45it/s]\u001b[A\n",
            "Iteration:  91% 606/669 [02:21<00:14,  4.44it/s]\u001b[A\n",
            "Iteration:  91% 607/669 [02:21<00:13,  4.44it/s]\u001b[A\n",
            "Iteration:  91% 608/669 [02:21<00:13,  4.45it/s]\u001b[A\n",
            "Iteration:  91% 609/669 [02:21<00:13,  4.45it/s]\u001b[A\n",
            "Iteration:  91% 610/669 [02:22<00:13,  4.45it/s]\u001b[A\n",
            "Iteration:  91% 611/669 [02:22<00:13,  4.44it/s]\u001b[A\n",
            "Iteration:  91% 612/669 [02:22<00:12,  4.44it/s]\u001b[A\n",
            "Iteration:  92% 613/669 [02:22<00:12,  4.44it/s]\u001b[A\n",
            "Iteration:  92% 614/669 [02:22<00:12,  4.44it/s]\u001b[A\n",
            "Iteration:  92% 615/669 [02:23<00:12,  4.43it/s]\u001b[A\n",
            "Iteration:  92% 616/669 [02:23<00:11,  4.44it/s]\u001b[A\n",
            "Iteration:  92% 617/669 [02:23<00:11,  4.44it/s]\u001b[A\n",
            "Iteration:  92% 618/669 [02:23<00:11,  4.44it/s]\u001b[A\n",
            "Iteration:  93% 619/669 [02:24<00:11,  4.44it/s]\u001b[A\n",
            "Iteration:  93% 620/669 [02:24<00:11,  4.43it/s]\u001b[A\n",
            "Iteration:  93% 621/669 [02:24<00:10,  4.44it/s]\u001b[A\n",
            "Iteration:  93% 622/669 [02:24<00:10,  4.45it/s]\u001b[A\n",
            "Iteration:  93% 623/669 [02:24<00:10,  4.44it/s]\u001b[A\n",
            "Iteration:  93% 624/669 [02:25<00:10,  4.44it/s]\u001b[A\n",
            "Iteration:  93% 625/669 [02:25<00:09,  4.44it/s]\u001b[A\n",
            "Iteration:  94% 626/669 [02:25<00:09,  4.44it/s]\u001b[A\n",
            "Iteration:  94% 627/669 [02:25<00:09,  4.45it/s]\u001b[A\n",
            "Iteration:  94% 628/669 [02:26<00:09,  4.44it/s]\u001b[A\n",
            "Iteration:  94% 629/669 [02:26<00:09,  4.43it/s]\u001b[A\n",
            "Iteration:  94% 630/669 [02:26<00:08,  4.44it/s]\u001b[A\n",
            "Iteration:  94% 631/669 [02:26<00:08,  4.44it/s]\u001b[A\n",
            "Iteration:  94% 632/669 [02:26<00:08,  4.45it/s]\u001b[A\n",
            "Iteration:  95% 633/669 [02:27<00:08,  4.44it/s]\u001b[A\n",
            "Iteration:  95% 634/669 [02:27<00:07,  4.42it/s]\u001b[A\n",
            "Iteration:  95% 635/669 [02:27<00:07,  4.44it/s]\u001b[A\n",
            "Iteration:  95% 636/669 [02:27<00:07,  4.45it/s]\u001b[A\n",
            "Iteration:  95% 637/669 [02:28<00:07,  4.44it/s]\u001b[A\n",
            "Iteration:  95% 638/669 [02:28<00:06,  4.45it/s]\u001b[A\n",
            "Iteration:  96% 639/669 [02:28<00:06,  4.43it/s]\u001b[A\n",
            "Iteration:  96% 640/669 [02:28<00:06,  4.45it/s]\u001b[A\n",
            "Iteration:  96% 641/669 [02:28<00:06,  4.45it/s]\u001b[A\n",
            "Iteration:  96% 642/669 [02:29<00:06,  4.43it/s]\u001b[A\n",
            "Iteration:  96% 643/669 [02:29<00:05,  4.44it/s]\u001b[A\n",
            "Iteration:  96% 644/669 [02:29<00:05,  4.45it/s]\u001b[A\n",
            "Iteration:  96% 645/669 [02:29<00:05,  4.44it/s]\u001b[A\n",
            "Iteration:  97% 646/669 [02:30<00:05,  4.45it/s]\u001b[A\n",
            "Iteration:  97% 647/669 [02:30<00:04,  4.45it/s]\u001b[A\n",
            "Iteration:  97% 648/669 [02:30<00:04,  4.44it/s]\u001b[A\n",
            "Iteration:  97% 649/669 [02:30<00:04,  4.45it/s]\u001b[A\n",
            "Iteration:  97% 650/669 [02:31<00:04,  4.44it/s]\u001b[A\n",
            "Iteration:  97% 651/669 [02:31<00:04,  4.44it/s]\u001b[A\n",
            "Iteration:  97% 652/669 [02:31<00:03,  4.43it/s]\u001b[A\n",
            "Iteration:  98% 653/669 [02:31<00:03,  4.45it/s]\u001b[A\n",
            "Iteration:  98% 654/669 [02:31<00:03,  4.44it/s]\u001b[A\n",
            "Iteration:  98% 655/669 [02:32<00:03,  4.45it/s]\u001b[A\n",
            "Iteration:  98% 656/669 [02:32<00:02,  4.40it/s]\u001b[A\n",
            "Iteration:  98% 657/669 [02:32<00:02,  4.42it/s]\u001b[A\n",
            "Iteration:  98% 658/669 [02:32<00:02,  4.42it/s]\u001b[A\n",
            "Iteration:  99% 659/669 [02:33<00:02,  4.45it/s]\u001b[A\n",
            "Iteration:  99% 660/669 [02:33<00:02,  4.44it/s]\u001b[A\n",
            "Iteration:  99% 661/669 [02:33<00:01,  4.45it/s]\u001b[A\n",
            "Iteration:  99% 662/669 [02:33<00:01,  4.44it/s]\u001b[A\n",
            "Iteration:  99% 663/669 [02:33<00:01,  4.45it/s]\u001b[A\n",
            "Iteration:  99% 664/669 [02:34<00:01,  4.45it/s]\u001b[A\n",
            "Iteration:  99% 665/669 [02:34<00:00,  4.43it/s]\u001b[A\n",
            "Iteration: 100% 666/669 [02:34<00:00,  4.44it/s]\u001b[A\n",
            "Iteration: 100% 667/669 [02:34<00:00,  4.45it/s]\u001b[A\n",
            "Iteration: 100% 668/669 [02:35<00:00,  4.44it/s]\u001b[A\n",
            "Iteration: 100% 669/669 [02:35<00:00,  4.42it/s]\u001b[A\n",
            "Epoch: 100% 1/1 [02:35<00:00, 155.31s/it]\n",
            "03/05/2020 05:53:41 - INFO - __main__ -    global_step = 669, average loss = 3.5980888189934475\n",
            "03/05/2020 05:53:41 - INFO - __main__ -   Saving model checkpoint to output_gpt_trump\n",
            "03/05/2020 05:53:41 - INFO - transformers.configuration_utils -   Configuration saved in output_gpt_trump/config.json\n",
            "03/05/2020 05:53:41 - INFO - transformers.modeling_utils -   Model weights saved in output_gpt_trump/pytorch_model.bin\n",
            "03/05/2020 05:53:41 - INFO - transformers.configuration_utils -   loading configuration file output_gpt_trump/config.json\n",
            "03/05/2020 05:53:41 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "03/05/2020 05:53:41 - INFO - transformers.modeling_utils -   loading weights file output_gpt_trump/pytorch_model.bin\n",
            "03/05/2020 05:53:45 - INFO - transformers.tokenization_utils -   Model name 'output_gpt_trump' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'output_gpt_trump' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/05/2020 05:53:45 - INFO - transformers.tokenization_utils -   Didn't find file output_gpt_trump/added_tokens.json. We won't load it.\n",
            "03/05/2020 05:53:45 - INFO - transformers.tokenization_utils -   loading file output_gpt_trump/vocab.json\n",
            "03/05/2020 05:53:45 - INFO - transformers.tokenization_utils -   loading file output_gpt_trump/merges.txt\n",
            "03/05/2020 05:53:45 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/05/2020 05:53:45 - INFO - transformers.tokenization_utils -   loading file output_gpt_trump/special_tokens_map.json\n",
            "03/05/2020 05:53:45 - INFO - transformers.tokenization_utils -   loading file output_gpt_trump/tokenizer_config.json\n",
            "03/05/2020 05:53:45 - INFO - __main__ -   Evaluate the following checkpoints: ['output_gpt_trump']\n",
            "03/05/2020 05:53:45 - INFO - transformers.configuration_utils -   loading configuration file output_gpt_trump/config.json\n",
            "03/05/2020 05:53:45 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "03/05/2020 05:53:45 - INFO - transformers.modeling_utils -   loading weights file output_gpt_trump/pytorch_model.bin\n",
            "03/05/2020 05:53:49 - INFO - __main__ -   Creating features from dataset file at /content\n",
            "03/05/2020 05:53:50 - INFO - __main__ -   Saving features into cached file /content/gpt2_cached_lm_1024_test_text_trump\n",
            "03/05/2020 05:53:50 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "03/05/2020 05:53:50 - INFO - __main__ -     Num examples = 166\n",
            "03/05/2020 05:53:50 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 166/166 [00:11<00:00, 15.03it/s]\n",
            "03/05/2020 05:54:01 - INFO - __main__ -   ***** Eval results  *****\n",
            "03/05/2020 05:54:01 - INFO - __main__ -     perplexity = tensor(28.9716)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hympHJXeMIhB",
        "colab_type": "code",
        "outputId": "786141bd-ea64-44f0-e519-75ff48dc5cdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python run_generation.py --model_type=gpt2 --model_name_or_path=/content/output_gpt_trump"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/05/2020 06:25:15 - INFO - transformers.tokenization_utils -   Model name '/content/output_gpt_trump' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/output_gpt_trump' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/05/2020 06:25:15 - INFO - transformers.tokenization_utils -   Didn't find file /content/output_gpt_trump/added_tokens.json. We won't load it.\n",
            "03/05/2020 06:25:15 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_trump/vocab.json\n",
            "03/05/2020 06:25:15 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_trump/merges.txt\n",
            "03/05/2020 06:25:15 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/05/2020 06:25:15 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_trump/special_tokens_map.json\n",
            "03/05/2020 06:25:15 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_trump/tokenizer_config.json\n",
            "03/05/2020 06:25:15 - INFO - transformers.configuration_utils -   loading configuration file /content/output_gpt_trump/config.json\n",
            "03/05/2020 06:25:15 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "03/05/2020 06:25:15 - INFO - transformers.modeling_utils -   loading weights file /content/output_gpt_trump/pytorch_model.bin\n",
            "03/05/2020 06:25:22 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=0, length=20, model_name_or_path='/content/output_gpt_trump', model_type='gpt2', n_gpu=1, no_cuda=False, num_return_sequences=1, p=0.9, padding_text='', prompt='', repetition_penalty=1.0, seed=42, stop_token=None, temperature=1.0, xlm_language='')\n",
            "Model prompt >>> National Security\n",
            "=== GENERATED SEQUENCE 1 ===\n",
            "National Security Advisor Jared Kushner has been very close to both of them. He is a smart politician, both on\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF07JuDLUV9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_trump = AutoTokenizer.from_pretrained(\"output_gpt_trump\")\n",
        "model_trump = AutoModelWithLMHead.from_pretrained(\"output_gpt_trump\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOdsBQahQtNH",
        "colab_type": "code",
        "outputId": "30b42533-777e-49ef-af8e-c5825f397bc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "sequence = \"Obama is going to\"\n",
        "\n",
        "input = tokenizer_trump.encode(sequence, return_tensors=\"pt\")\n",
        "generated = model_trump.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
        "\n",
        "resulting_string = tokenizer_trump.decode(generated.tolist()[0])\n",
        "print(resulting_string)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obama is going to have to show the world a little more backbone about the Iraq War. It will be tough, but with me there is. @foxandfriends,\" she said on @foxandfriends, via @nbcavs.\"I\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnFzkteQQtSO",
        "colab_type": "code",
        "outputId": "bd809648-2a5b-4f1f-abd3-4ce9c7551b27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "sequence = \"Obama is going to\"\n",
        "\n",
        "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
        "generated = model_gpt.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
        "\n",
        "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
        "print(resulting_string)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obama is going to be in charge. He's going to put his own people into place. \"I am the boss, so when people get scared when they get scared about something they need to learn from, I'll hold them accountable for that to\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nu-LHPsPLnEj",
        "colab_type": "text"
      },
      "source": [
        "## <span style=\"color:red\">*Exercise 3*</span>\n",
        "\n",
        "<span style=\"color:red\">Construct cells immediately below this that generate a BERT-powered chatbot tuned on text related to your final project. What is interesting about this model, and how to does it compare to an untrained model? What does it reveal about the social game involved with your dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orklpykQa0ai",
        "colab_type": "text"
      },
      "source": [
        "I am going to train a bot to write Obama's press releases using the press releases data set that I used before. Before training here, I generated train and test set using my local machine. The code is below, but it won't run on colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry-r3im7b8Y0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ObamaClintonReleases = pd.read_csv('../data/ObamaClintonReleases.csv', index_col=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqsDIn9Nb85f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ObamaClintonReleases['category'] = [s == 'Obama' for s in ObamaClintonReleases['targetSenator']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhrB1IZPb880",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filter1 = ObamaClintonReleases['category'] == 1\n",
        "df = ObamaClintonReleases[filter1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWsihfx0cCos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_text, test_text = train_test_split(df['text'], test_size=0.2)\n",
        "\n",
        "train_text.to_frame().to_csv(r'train_text_obama', header=None, index=None, sep=' ', mode='a')\n",
        "test_text.to_frame().to_csv(r'test_text_obama', header=None, index=None, sep=' ', mode='a')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B87RlKALU1P3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHKK4sIXU1S1",
        "colab_type": "code",
        "outputId": "0e5fdde3-5f99-49ad-bb42-f83462824544",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!python /content/run_language_modelling.py --output_dir=output_gpt_obama --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=/content/train_text_obama --do_eval --eval_data_file=/content/test_text_obama --per_gpu_train_batch_size=1 --per_gpu_eval_batch_size=1"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/run_language_modelling.py\", line 799, in <module>\n",
            "    main()\n",
            "  File \"/content/run_language_modelling.py\", line 650, in main\n",
            "    args.output_dir\n",
            "ValueError: Output directory (output_gpt_obama) already exists and is not empty. Use --overwrite_output_dir to overcome.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcvSfSLEU1Yv",
        "colab_type": "code",
        "outputId": "e19e2d95-9034-4055-ee76-b546d13d5a7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python run_generation.py --model_type=gpt2 --model_name_or_path=/content/output_gpt_obama"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/05/2020 06:40:32 - INFO - transformers.tokenization_utils -   Model name '/content/output_gpt_obama' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/output_gpt_obama' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/05/2020 06:40:32 - INFO - transformers.tokenization_utils -   Didn't find file /content/output_gpt_obama/added_tokens.json. We won't load it.\n",
            "03/05/2020 06:40:32 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_obama/vocab.json\n",
            "03/05/2020 06:40:32 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_obama/merges.txt\n",
            "03/05/2020 06:40:32 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/05/2020 06:40:32 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_obama/special_tokens_map.json\n",
            "03/05/2020 06:40:32 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_obama/tokenizer_config.json\n",
            "03/05/2020 06:40:32 - INFO - transformers.configuration_utils -   loading configuration file /content/output_gpt_obama/config.json\n",
            "03/05/2020 06:40:32 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "03/05/2020 06:40:32 - INFO - transformers.modeling_utils -   loading weights file /content/output_gpt_obama/pytorch_model.bin\n",
            "03/05/2020 06:40:39 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=0, length=20, model_name_or_path='/content/output_gpt_obama', model_type='gpt2', n_gpu=1, no_cuda=False, num_return_sequences=1, p=0.9, padding_text='', prompt='', repetition_penalty=1.0, seed=42, stop_token=None, temperature=1.0, xlm_language='')\n",
            "Model prompt >>> On Iraq war,\n",
            "=== GENERATED SEQUENCE 1 ===\n",
            "On Iraq war, Obama petitions 39 senators to bring on another AML member to hold the House committee oversight of war operations\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLOSCSyCU1cw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "tokenizer_obama = AutoTokenizer.from_pretrained(\"output_gpt_obama\")\n",
        "model_obama = AutoModelWithLMHead.from_pretrained(\"output_gpt_obama\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIW776-EU1WQ",
        "colab_type": "code",
        "outputId": "7248f346-fd01-4064-f025-e3b4fdc61f2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# fine tuned model\n",
        "sequence = \"Senator Obama announces on national security\"\n",
        "\n",
        "input = tokenizer_obama.encode(sequence, return_tensors=\"pt\")\n",
        "generated = model_obama.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
        "\n",
        "resulting_string = tokenizer_obama.decode(generated.tolist()[0])\n",
        "print(resulting_string)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Senator Obama announces on national security committee that he will be supporting legislation in the Senate to close the loopholes with military and intelligence contractors that allow them to circumvent taxes by claiming that they can do business  The bill must be signed into law  and Obama is\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfbLUBuiU1NK",
        "colab_type": "code",
        "outputId": "145ad194-1f8f-4e5a-ad1d-973410def246",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# non-fine tuned model\n",
        "\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "\n",
        "tokenizer_gpt = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model_gpt = AutoModelWithLMHead.from_pretrained(\"gpt2\")\n",
        "\n",
        "sequence = \"Senator Obama announces on national security and\"\n",
        "\n",
        "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
        "generated = model_gpt.generate(input, max_length=50)\n",
        "\n",
        "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
        "print(resulting_string)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Senator Obama announces on national security and budget he will sign an NDAA.\n",
            "\n",
            "Congress created the NDAA to address a problem associated with unchecked government spying on Americans.\n",
            "\n",
            "The NDAA includes language to ban the USA Freedom Act, a program\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znvxYNjckiFQ",
        "colab_type": "text"
      },
      "source": [
        "The fine-tuned model performs impressively by generating texts that almost reads like an actual press release on national security. When comparing it with the non fine-tuned model, the former obviously does a better job. However, the non fine-tuned model also did pretty well since it came up with a content that is related to national security issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwU7fgTlblUC",
        "colab_type": "text"
      },
      "source": [
        "#### End of exercise 3\n",
        "\n",
        "I am testing the given code here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-CDbhF8SQoN",
        "colab_type": "text"
      },
      "source": [
        "### Tuning RoBERTa on US and UK blog posts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwsrsQ2qSVnN",
        "colab_type": "code",
        "outputId": "0690fd5f-7da0-4cc0-87a7-3d58b748fc51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python run_language_modelling.py --output_dir=output_roberta_US --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=us_blog_train --do_eval --eval_data_file=us_blog_test --mlm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/05/2020 06:50:00 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "03/05/2020 06:50:00 - INFO - filelock -   Lock 139941156408512 acquired on /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6.lock\n",
            "03/05/2020 06:50:00 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmprq48qs5m\n",
            "Downloading: 100% 524/524 [00:00<00:00, 718kB/s]\n",
            "03/05/2020 06:50:00 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json in cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "03/05/2020 06:50:00 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "03/05/2020 06:50:00 - INFO - filelock -   Lock 139941156408512 released on /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6.lock\n",
            "03/05/2020 06:50:00 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "03/05/2020 06:50:00 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "03/05/2020 06:50:00 - INFO - filelock -   Lock 139941156371704 acquired on /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n",
            "03/05/2020 06:50:00 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpz4pld9rd\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 11.8MB/s]\n",
            "03/05/2020 06:50:01 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json in cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "03/05/2020 06:50:01 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "03/05/2020 06:50:01 - INFO - filelock -   Lock 139941156371704 released on /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n",
            "03/05/2020 06:50:01 - INFO - filelock -   Lock 139941156371704 acquired on /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "03/05/2020 06:50:01 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp_5do4nqd\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 7.07MB/s]\n",
            "03/05/2020 06:50:01 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt in cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/05/2020 06:50:01 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/05/2020 06:50:01 - INFO - filelock -   Lock 139941156371704 released on /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "03/05/2020 06:50:01 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "03/05/2020 06:50:01 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/05/2020 06:50:01 - INFO - filelock -   Lock 139941018198200 acquired on /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e.lock\n",
            "03/05/2020 06:50:01 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpue3hx40e\n",
            "Downloading: 100% 501M/501M [00:06<00:00, 81.8MB/s]\n",
            "03/05/2020 06:50:07 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin in cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "03/05/2020 06:50:07 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "03/05/2020 06:50:07 - INFO - filelock -   Lock 139941018198200 released on /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e.lock\n",
            "03/05/2020 06:50:07 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "03/05/2020 06:50:12 - INFO - transformers.modeling_utils -   Weights of RobertaForMaskedLM not initialized from pretrained model: ['lm_head.decoder.bias']\n",
            "03/05/2020 06:50:16 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='us_blog_test', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=True, mlm_probability=0.15, model_name_or_path='roberta-base', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='output_roberta_US', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='us_blog_train', warmup_steps=0, weight_decay=0.0)\n",
            "03/05/2020 06:50:16 - INFO - __main__ -   Creating features from dataset file at \n",
            "03/05/2020 06:50:44 - INFO - __main__ -   Saving features into cached file roberta_cached_lm_510_us_blog_train\n",
            "03/05/2020 06:50:44 - INFO - __main__ -   ***** Running training *****\n",
            "03/05/2020 06:50:44 - INFO - __main__ -     Num examples = 14967\n",
            "03/05/2020 06:50:44 - INFO - __main__ -     Num Epochs = 1\n",
            "03/05/2020 06:50:44 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
            "03/05/2020 06:50:44 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "03/05/2020 06:50:44 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "03/05/2020 06:50:44 - INFO - __main__ -     Total optimization steps = 3742\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/3742 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/3742 [00:00<22:15,  2.80it/s]\u001b[A\n",
            "Iteration:   0% 2/3742 [00:00<21:39,  2.88it/s]\u001b[A\n",
            "Iteration:   0% 3/3742 [00:01<21:12,  2.94it/s]\u001b[A\n",
            "Iteration:   0% 4/3742 [00:01<20:57,  2.97it/s]\u001b[A\n",
            "Iteration:   0% 5/3742 [00:01<20:45,  3.00it/s]\u001b[A\n",
            "Iteration:   0% 6/3742 [00:01<20:35,  3.02it/s]\u001b[A\n",
            "Iteration:   0% 7/3742 [00:02<20:29,  3.04it/s]\u001b[A\n",
            "Iteration:   0% 8/3742 [00:02<20:24,  3.05it/s]\u001b[A\n",
            "Iteration:   0% 9/3742 [00:02<20:19,  3.06it/s]\u001b[A\n",
            "Iteration:   0% 10/3742 [00:03<20:21,  3.06it/s]\u001b[A\n",
            "Iteration:   0% 11/3742 [00:03<20:16,  3.07it/s]\u001b[A\n",
            "Iteration:   0% 12/3742 [00:03<20:16,  3.07it/s]\u001b[A\n",
            "Iteration:   0% 13/3742 [00:04<20:15,  3.07it/s]\u001b[A\n",
            "Iteration:   0% 14/3742 [00:04<20:18,  3.06it/s]\u001b[A\n",
            "Iteration:   0% 15/3742 [00:04<20:15,  3.06it/s]\u001b[A\n",
            "Iteration:   0% 16/3742 [00:05<20:13,  3.07it/s]\u001b[A\n",
            "Iteration:   0% 17/3742 [00:05<20:11,  3.08it/s]\u001b[A\n",
            "Iteration:   0% 18/3742 [00:05<20:10,  3.08it/s]\u001b[A\n",
            "Iteration:   1% 19/3742 [00:06<20:10,  3.07it/s]\u001b[A\n",
            "Iteration:   1% 20/3742 [00:06<20:10,  3.08it/s]\u001b[A\n",
            "Iteration:   1% 21/3742 [00:06<20:08,  3.08it/s]\u001b[A\n",
            "Iteration:   1% 22/3742 [00:07<20:08,  3.08it/s]\u001b[A\n",
            "Iteration:   1% 23/3742 [00:07<20:08,  3.08it/s]\u001b[A\n",
            "Iteration:   1% 24/3742 [00:07<20:07,  3.08it/s]\u001b[A\n",
            "Iteration:   1% 25/3742 [00:08<20:06,  3.08it/s]\u001b[A\n",
            "Iteration:   1% 26/3742 [00:08<20:05,  3.08it/s]\u001b[A\n",
            "Iteration:   1% 27/3742 [00:08<20:05,  3.08it/s]\u001b[A\n",
            "Iteration:   1% 28/3742 [00:09<20:05,  3.08it/s]\u001b[A\n",
            "Iteration:   1% 29/3742 [00:09<20:05,  3.08it/s]\u001b[A\n",
            "Iteration:   1% 30/3742 [00:09<20:04,  3.08it/s]\u001b[A\n",
            "Iteration:   1% 31/3742 [00:10<20:03,  3.08it/s]\u001b[A\n",
            "Iteration:   1% 32/3742 [00:10<20:05,  3.08it/s]\u001b[A\n",
            "Iteration:   1% 33/3742 [00:10<20:03,  3.08it/s]\u001b[A\n",
            "Iteration:   1% 34/3742 [00:11<20:03,  3.08it/s]\u001b[A\n",
            "Iteration:   1% 35/3742 [00:11<20:03,  3.08it/s]\u001b[A\n",
            "Iteration:   1% 36/3742 [00:11<20:09,  3.07it/s]\u001b[A\n",
            "Iteration:   1% 37/3742 [00:12<20:07,  3.07it/s]\u001b[A\n",
            "Iteration:   1% 38/3742 [00:12<20:05,  3.07it/s]\u001b[A\n",
            "Iteration:   1% 39/3742 [00:12<20:04,  3.07it/s]\u001b[A\n",
            "Iteration:   1% 40/3742 [00:13<20:11,  3.06it/s]\u001b[A\n",
            "Iteration:   1% 41/3742 [00:13<20:13,  3.05it/s]\u001b[A\n",
            "Iteration:   1% 42/3742 [00:13<20:16,  3.04it/s]\u001b[A\n",
            "Iteration:   1% 43/3742 [00:14<20:12,  3.05it/s]\u001b[A\n",
            "Iteration:   1% 44/3742 [00:14<20:10,  3.05it/s]\u001b[A\n",
            "Iteration:   1% 45/3742 [00:14<20:07,  3.06it/s]\u001b[A\n",
            "Iteration:   1% 46/3742 [00:15<20:04,  3.07it/s]\u001b[A\n",
            "Iteration:   1% 47/3742 [00:15<20:03,  3.07it/s]\u001b[A\n",
            "Iteration:   1% 48/3742 [00:15<20:09,  3.05it/s]\u001b[A\n",
            "Iteration:   1% 49/3742 [00:15<20:06,  3.06it/s]\u001b[A\n",
            "Iteration:   1% 50/3742 [00:16<20:03,  3.07it/s]\u001b[A\n",
            "Iteration:   1% 51/3742 [00:16<20:01,  3.07it/s]\u001b[A\n",
            "Iteration:   1% 52/3742 [00:16<20:02,  3.07it/s]\u001b[A\n",
            "Iteration:   1% 53/3742 [00:17<20:00,  3.07it/s]\u001b[A\n",
            "Iteration:   1% 54/3742 [00:17<20:01,  3.07it/s]\u001b[A\n",
            "Iteration:   1% 55/3742 [00:17<19:59,  3.07it/s]\u001b[A\n",
            "Iteration:   1% 56/3742 [00:18<19:59,  3.07it/s]\u001b[A\n",
            "Iteration:   2% 57/3742 [00:18<20:03,  3.06it/s]\u001b[A\n",
            "Iteration:   2% 58/3742 [00:18<20:01,  3.07it/s]\u001b[A\n",
            "Iteration:   2% 59/3742 [00:19<19:58,  3.07it/s]\u001b[A\n",
            "Iteration:   2% 60/3742 [00:19<19:56,  3.08it/s]\u001b[A\n",
            "Iteration:   2% 61/3742 [00:19<19:58,  3.07it/s]\u001b[A\n",
            "Iteration:   2% 62/3742 [00:20<19:57,  3.07it/s]\u001b[A\n",
            "Iteration:   2% 63/3742 [00:20<19:56,  3.07it/s]\u001b[A\n",
            "Iteration:   2% 64/3742 [00:20<19:54,  3.08it/s]\u001b[A\n",
            "Iteration:   2% 65/3742 [00:21<19:57,  3.07it/s]\u001b[A\n",
            "Iteration:   2% 66/3742 [00:21<19:56,  3.07it/s]\u001b[A\n",
            "Iteration:   2% 67/3742 [00:21<19:57,  3.07it/s]\u001b[A\n",
            "Iteration:   2% 68/3742 [00:22<19:54,  3.07it/s]\u001b[A\n",
            "Iteration:   2% 69/3742 [00:22<19:52,  3.08it/s]\u001b[A\n",
            "Iteration:   2% 70/3742 [00:22<19:52,  3.08it/s]\u001b[A\n",
            "Iteration:   2% 71/3742 [00:23<19:54,  3.07it/s]\u001b[A\n",
            "Iteration:   2% 72/3742 [00:23<19:54,  3.07it/s]\u001b[A\n",
            "Iteration:   2% 73/3742 [00:23<19:53,  3.08it/s]\u001b[A\n",
            "Iteration:   2% 74/3742 [00:24<19:53,  3.07it/s]\u001b[A\n",
            "Iteration:   2% 75/3742 [00:24<19:54,  3.07it/s]\u001b[A\n",
            "Iteration:   2% 76/3742 [00:24<19:52,  3.07it/s]\u001b[A\n",
            "Iteration:   2% 77/3742 [00:25<19:50,  3.08it/s]\u001b[A\n",
            "Iteration:   2% 78/3742 [00:25<19:52,  3.07it/s]\u001b[A\n",
            "Iteration:   2% 79/3742 [00:25<20:12,  3.02it/s]\u001b[A\n",
            "Iteration:   2% 80/3742 [00:26<20:06,  3.04it/s]\u001b[A\n",
            "Iteration:   2% 81/3742 [00:26<20:00,  3.05it/s]\u001b[A\n",
            "Iteration:   2% 82/3742 [00:26<20:02,  3.04it/s]\u001b[A\n",
            "Iteration:   2% 83/3742 [00:27<20:00,  3.05it/s]\u001b[A\n",
            "Iteration:   2% 84/3742 [00:27<19:54,  3.06it/s]\u001b[A\n",
            "Iteration:   2% 85/3742 [00:27<19:53,  3.06it/s]\u001b[A\n",
            "Iteration:   2% 86/3742 [00:28<19:52,  3.07it/s]\u001b[A\n",
            "Iteration:   2% 87/3742 [00:28<19:50,  3.07it/s]\u001b[A\n",
            "Iteration:   2% 88/3742 [00:28<19:47,  3.08it/s]\u001b[A\n",
            "Iteration:   2% 89/3742 [00:29<19:47,  3.08it/s]\u001b[A\n",
            "Iteration:   2% 90/3742 [00:29<19:49,  3.07it/s]\u001b[A\n",
            "Iteration:   2% 91/3742 [00:29<19:48,  3.07it/s]\u001b[A\n",
            "Iteration:   2% 92/3742 [00:29<19:46,  3.08it/s]\u001b[A\n",
            "Iteration:   2% 93/3742 [00:30<19:44,  3.08it/s]\u001b[A\n",
            "Iteration:   3% 94/3742 [00:30<19:44,  3.08it/s]\u001b[A\n",
            "Iteration:   3% 95/3742 [00:30<19:42,  3.08it/s]\u001b[A\n",
            "Iteration:   3% 96/3742 [00:31<19:43,  3.08it/s]\u001b[A\n",
            "Iteration:   3% 97/3742 [00:31<19:43,  3.08it/s]\u001b[A\n",
            "Iteration:   3% 98/3742 [00:31<19:42,  3.08it/s]\u001b[A\n",
            "Iteration:   3% 99/3742 [00:32<19:41,  3.08it/s]\u001b[A\n",
            "Iteration:   3% 100/3742 [00:32<19:42,  3.08it/s]\u001b[A\n",
            "Iteration:   3% 101/3742 [00:32<19:42,  3.08it/s]\u001b[A\n",
            "Iteration:   3% 102/3742 [00:33<19:42,  3.08it/s]\u001b[A\n",
            "Iteration:   3% 103/3742 [00:33<19:44,  3.07it/s]\u001b[A\n",
            "Iteration:   3% 104/3742 [00:33<19:43,  3.07it/s]\u001b[A\n",
            "Iteration:   3% 105/3742 [00:34<19:42,  3.07it/s]\u001b[A\n",
            "Iteration:   3% 106/3742 [00:34<19:41,  3.08it/s]\u001b[A\n",
            "Iteration:   3% 107/3742 [00:34<19:40,  3.08it/s]\u001b[A\n",
            "Iteration:   3% 108/3742 [00:35<19:39,  3.08it/s]\u001b[A\n",
            "Iteration:   3% 109/3742 [00:35<19:39,  3.08it/s]\u001b[A\n",
            "Iteration:   3% 110/3742 [00:35<19:38,  3.08it/s]\u001b[A\n",
            "Iteration:   3% 111/3742 [00:36<19:37,  3.08it/s]\u001b[A\n",
            "Iteration:   3% 112/3742 [00:36<19:36,  3.08it/s]\u001b[A\n",
            "Iteration:   3% 113/3742 [00:36<19:42,  3.07it/s]\u001b[A\n",
            "Iteration:   3% 114/3742 [00:37<19:46,  3.06it/s]\u001b[A\n",
            "Iteration:   3% 115/3742 [00:37<19:47,  3.05it/s]\u001b[A\n",
            "Iteration:   3% 116/3742 [00:37<19:45,  3.06it/s]\u001b[A\n",
            "Iteration:   3% 117/3742 [00:38<19:41,  3.07it/s]\u001b[A\n",
            "Iteration:   3% 118/3742 [00:38<19:40,  3.07it/s]\u001b[A\n",
            "Iteration:   3% 119/3742 [00:38<19:38,  3.07it/s]\u001b[A\n",
            "Iteration:   3% 120/3742 [00:39<19:38,  3.07it/s]\u001b[A\n",
            "Iteration:   3% 121/3742 [00:39<19:38,  3.07it/s]\u001b[A\n",
            "Iteration:   3% 122/3742 [00:39<19:37,  3.07it/s]\u001b[A\n",
            "Iteration:   3% 123/3742 [00:40<19:35,  3.08it/s]\u001b[A\n",
            "Iteration:   3% 124/3742 [00:40<19:38,  3.07it/s]\u001b[A\n",
            "Iteration:   3% 125/3742 [00:40<19:37,  3.07it/s]\u001b[A\n",
            "Iteration:   3% 126/3742 [00:41<19:40,  3.06it/s]\u001b[A\n",
            "Iteration:   3% 127/3742 [00:41<19:38,  3.07it/s]\u001b[A\n",
            "Iteration:   3% 128/3742 [00:41<19:37,  3.07it/s]\u001b[A\n",
            "Iteration:   3% 129/3742 [00:42<19:43,  3.05it/s]\u001b[A\n",
            "Iteration:   3% 130/3742 [00:42<19:40,  3.06it/s]\u001b[A\n",
            "Iteration:   4% 131/3742 [00:42<19:38,  3.06it/s]\u001b[A\n",
            "Iteration:   4% 132/3742 [00:43<19:36,  3.07it/s]\u001b[A\n",
            "Iteration:   4% 133/3742 [00:43<19:35,  3.07it/s]\u001b[A\n",
            "Iteration:   4% 134/3742 [00:43<19:33,  3.07it/s]\u001b[A\n",
            "Iteration:   4% 135/3742 [00:43<19:31,  3.08it/s]\u001b[A\n",
            "Iteration:   4% 136/3742 [00:44<19:31,  3.08it/s]\u001b[A\n",
            "Iteration:   4% 137/3742 [00:44<19:32,  3.08it/s]\u001b[A\n",
            "Iteration:   4% 138/3742 [00:44<19:32,  3.07it/s]\u001b[A\n",
            "Iteration:   4% 139/3742 [00:45<19:30,  3.08it/s]\u001b[A\n",
            "Iteration:   4% 140/3742 [00:45<19:30,  3.08it/s]\u001b[A\n",
            "Iteration:   4% 141/3742 [00:45<19:33,  3.07it/s]\u001b[A\n",
            "Iteration:   4% 142/3742 [00:46<19:32,  3.07it/s]\u001b[A\n",
            "Iteration:   4% 143/3742 [00:46<19:33,  3.07it/s]\u001b[A\n",
            "Iteration:   4% 144/3742 [00:46<19:33,  3.07it/s]\u001b[A\n",
            "Iteration:   4% 145/3742 [00:47<19:34,  3.06it/s]\u001b[A\n",
            "Iteration:   4% 146/3742 [00:47<19:32,  3.07it/s]\u001b[A\n",
            "Iteration:   4% 147/3742 [00:47<19:30,  3.07it/s]\u001b[A\n",
            "Iteration:   4% 148/3742 [00:48<19:33,  3.06it/s]\u001b[A\n",
            "Iteration:   4% 149/3742 [00:48<19:32,  3.07it/s]\u001b[A\n",
            "Iteration:   4% 150/3742 [00:48<19:29,  3.07it/s]\u001b[A\n",
            "Iteration:   4% 151/3742 [00:49<19:27,  3.08it/s]\u001b[A\n",
            "Iteration:   4% 152/3742 [00:49<19:28,  3.07it/s]\u001b[A\n",
            "Iteration:   4% 153/3742 [00:49<19:26,  3.08it/s]\u001b[A\n",
            "Iteration:   4% 154/3742 [00:50<19:26,  3.08it/s]\u001b[A\n",
            "Iteration:   4% 155/3742 [00:50<19:28,  3.07it/s]\u001b[A\n",
            "Iteration:   4% 156/3742 [00:50<19:27,  3.07it/s]\u001b[A\n",
            "Iteration:   4% 157/3742 [00:51<19:25,  3.08it/s]\u001b[A\n",
            "Iteration:   4% 158/3742 [00:51<19:26,  3.07it/s]\u001b[A\n",
            "Iteration:   4% 159/3742 [00:51<19:24,  3.08it/s]\u001b[A\n",
            "Iteration:   4% 160/3742 [00:52<19:23,  3.08it/s]\u001b[A\n",
            "Iteration:   4% 161/3742 [00:52<19:22,  3.08it/s]\u001b[A\n",
            "Iteration:   4% 162/3742 [00:52<19:21,  3.08it/s]\u001b[A\n",
            "Iteration:   4% 163/3742 [00:53<19:25,  3.07it/s]\u001b[A\n",
            "Iteration:   4% 164/3742 [00:53<19:31,  3.06it/s]\u001b[A\n",
            "Iteration:   4% 165/3742 [00:53<19:33,  3.05it/s]\u001b[A\n",
            "Iteration:   4% 166/3742 [00:54<19:32,  3.05it/s]\u001b[A\n",
            "Iteration:   4% 167/3742 [00:54<19:28,  3.06it/s]\u001b[A\n",
            "Iteration:   4% 168/3742 [00:54<19:26,  3.06it/s]\u001b[A\n",
            "Iteration:   5% 169/3742 [00:55<19:24,  3.07it/s]\u001b[A\n",
            "Iteration:   5% 170/3742 [00:55<19:25,  3.07it/s]\u001b[A\n",
            "Iteration:   5% 171/3742 [00:55<19:23,  3.07it/s]\u001b[A\n",
            "Iteration:   5% 172/3742 [00:56<19:21,  3.07it/s]\u001b[A\n",
            "Iteration:   5% 173/3742 [00:56<19:20,  3.08it/s]\u001b[A\n",
            "Iteration:   5% 174/3742 [00:56<19:20,  3.07it/s]\u001b[A\n",
            "Iteration:   5% 175/3742 [00:57<19:20,  3.07it/s]\u001b[A\n",
            "Iteration:   5% 176/3742 [00:57<19:18,  3.08it/s]\u001b[A\n",
            "Iteration:   5% 177/3742 [00:57<19:18,  3.08it/s]\u001b[A\n",
            "Iteration:   5% 178/3742 [00:57<19:16,  3.08it/s]\u001b[A\n",
            "Iteration:   5% 179/3742 [00:58<19:16,  3.08it/s]\u001b[A\n",
            "Iteration:   5% 180/3742 [00:58<19:16,  3.08it/s]\u001b[A\n",
            "Iteration:   5% 181/3742 [00:58<19:25,  3.05it/s]\u001b[A\n",
            "Iteration:   5% 182/3742 [00:59<19:24,  3.06it/s]\u001b[A\n",
            "Iteration:   5% 183/3742 [00:59<19:27,  3.05it/s]\u001b[A\n",
            "Iteration:   5% 184/3742 [00:59<19:23,  3.06it/s]\u001b[A\n",
            "Iteration:   5% 185/3742 [01:00<19:20,  3.07it/s]\u001b[A\n",
            "Iteration:   5% 186/3742 [01:00<19:18,  3.07it/s]\u001b[A\n",
            "Iteration:   5% 187/3742 [01:00<19:16,  3.07it/s]\u001b[A\n",
            "Iteration:   5% 188/3742 [01:01<19:14,  3.08it/s]\u001b[A\n",
            "Iteration:   5% 189/3742 [01:01<19:13,  3.08it/s]\u001b[A\n",
            "Iteration:   5% 190/3742 [01:01<19:13,  3.08it/s]\u001b[A\n",
            "Iteration:   5% 191/3742 [01:02<19:12,  3.08it/s]\u001b[A\n",
            "Iteration:   5% 192/3742 [01:02<19:12,  3.08it/s]\u001b[A\n",
            "Iteration:   5% 193/3742 [01:02<19:11,  3.08it/s]\u001b[A\n",
            "Iteration:   5% 194/3742 [01:03<19:11,  3.08it/s]\u001b[A\n",
            "Iteration:   5% 195/3742 [01:03<19:12,  3.08it/s]\u001b[A\n",
            "Iteration:   5% 196/3742 [01:03<19:12,  3.08it/s]\u001b[A\n",
            "Iteration:   5% 197/3742 [01:04<19:12,  3.08it/s]\u001b[A\n",
            "Iteration:   5% 198/3742 [01:04<19:10,  3.08it/s]\u001b[A\n",
            "Iteration:   5% 199/3742 [01:04<19:10,  3.08it/s]\u001b[A\n",
            "Iteration:   5% 200/3742 [01:05<19:11,  3.08it/s]\u001b[A\n",
            "Iteration:   5% 201/3742 [01:05<19:10,  3.08it/s]\u001b[A\n",
            "Iteration:   5% 202/3742 [01:05<19:08,  3.08it/s]\u001b[A\n",
            "Iteration:   5% 203/3742 [01:06<19:09,  3.08it/s]\u001b[A\n",
            "Iteration:   5% 204/3742 [01:06<19:09,  3.08it/s]\u001b[A\n",
            "Iteration:   5% 205/3742 [01:06<19:09,  3.08it/s]\u001b[A\n",
            "Iteration:   6% 206/3742 [01:07<19:07,  3.08it/s]\u001b[A\n",
            "Iteration:   6% 207/3742 [01:07<19:08,  3.08it/s]\u001b[A\n",
            "Iteration:   6% 208/3742 [01:07<19:08,  3.08it/s]\u001b[A\n",
            "Iteration:   6% 209/3742 [01:08<19:09,  3.07it/s]\u001b[A\n",
            "Iteration:   6% 210/3742 [01:08<19:09,  3.07it/s]\u001b[A\n",
            "Iteration:   6% 211/3742 [01:08<19:08,  3.07it/s]\u001b[A\n",
            "Iteration:   6% 212/3742 [01:09<19:06,  3.08it/s]\u001b[A\n",
            "Iteration:   6% 213/3742 [01:09<19:05,  3.08it/s]\u001b[A\n",
            "Iteration:   6% 214/3742 [01:09<19:07,  3.08it/s]\u001b[A\n",
            "Iteration:   6% 215/3742 [01:10<19:05,  3.08it/s]\u001b[A\n",
            "Iteration:   6% 216/3742 [01:10<19:04,  3.08it/s]\u001b[A\n",
            "Iteration:   6% 217/3742 [01:10<19:04,  3.08it/s]\u001b[A\n",
            "Iteration:   6% 218/3742 [01:10<19:03,  3.08it/s]\u001b[A\n",
            "Iteration:   6% 219/3742 [01:11<19:03,  3.08it/s]\u001b[A\n",
            "Iteration:   6% 220/3742 [01:11<19:02,  3.08it/s]\u001b[A\n",
            "Iteration:   6% 221/3742 [01:11<19:02,  3.08it/s]\u001b[A\n",
            "Iteration:   6% 222/3742 [01:12<19:04,  3.08it/s]\u001b[A\n",
            "Iteration:   6% 223/3742 [01:12<19:03,  3.08it/s]\u001b[A\n",
            "Iteration:   6% 224/3742 [01:12<19:07,  3.07it/s]\u001b[A\n",
            "Iteration:   6% 225/3742 [01:13<19:09,  3.06it/s]\u001b[A\n",
            "Iteration:   6% 226/3742 [01:13<19:12,  3.05it/s]\u001b[A\n",
            "Iteration:   6% 227/3742 [01:13<19:11,  3.05it/s]\u001b[A\n",
            "Iteration:   6% 228/3742 [01:14<19:07,  3.06it/s]\u001b[A\n",
            "Iteration:   6% 229/3742 [01:14<19:04,  3.07it/s]\u001b[A\n",
            "Iteration:   6% 230/3742 [01:14<19:01,  3.08it/s]\u001b[A\n",
            "Iteration:   6% 231/3742 [01:15<19:00,  3.08it/s]\u001b[A\n",
            "Iteration:   6% 232/3742 [01:15<18:59,  3.08it/s]\u001b[A\n",
            "Iteration:   6% 233/3742 [01:15<18:59,  3.08it/s]\u001b[A\n",
            "Iteration:   6% 234/3742 [01:16<18:59,  3.08it/s]\u001b[A\n",
            "Iteration:   6% 235/3742 [01:16<19:04,  3.06it/s]\u001b[A\n",
            "Iteration:   6% 236/3742 [01:16<19:02,  3.07it/s]\u001b[A\n",
            "Iteration:   6% 237/3742 [01:17<19:00,  3.07it/s]\u001b[A\n",
            "Iteration:   6% 238/3742 [01:17<18:59,  3.08it/s]\u001b[A\n",
            "Iteration:   6% 239/3742 [01:17<18:59,  3.07it/s]\u001b[A\n",
            "Iteration:   6% 240/3742 [01:18<19:02,  3.06it/s]\u001b[A\n",
            "Iteration:   6% 241/3742 [01:18<19:01,  3.07it/s]\u001b[A\n",
            "Iteration:   6% 242/3742 [01:18<18:59,  3.07it/s]\u001b[A\n",
            "Iteration:   6% 243/3742 [01:19<19:00,  3.07it/s]\u001b[A\n",
            "Iteration:   7% 244/3742 [01:19<19:01,  3.07it/s]\u001b[A\n",
            "Iteration:   7% 245/3742 [01:19<18:58,  3.07it/s]\u001b[A\n",
            "Iteration:   7% 246/3742 [01:20<18:57,  3.07it/s]\u001b[A\n",
            "Iteration:   7% 247/3742 [01:20<18:56,  3.07it/s]\u001b[A\n",
            "Iteration:   7% 248/3742 [01:20<18:54,  3.08it/s]\u001b[A\n",
            "Iteration:   7% 249/3742 [01:21<18:55,  3.08it/s]\u001b[A\n",
            "Iteration:   7% 250/3742 [01:21<18:54,  3.08it/s]\u001b[A\n",
            "Iteration:   7% 251/3742 [01:21<18:54,  3.08it/s]\u001b[A\n",
            "Iteration:   7% 252/3742 [01:22<18:52,  3.08it/s]\u001b[A\n",
            "Iteration:   7% 253/3742 [01:22<18:51,  3.08it/s]\u001b[A\n",
            "Iteration:   7% 254/3742 [01:22<18:51,  3.08it/s]\u001b[A\n",
            "Iteration:   7% 255/3742 [01:23<18:51,  3.08it/s]\u001b[A\n",
            "Iteration:   7% 256/3742 [01:23<18:52,  3.08it/s]\u001b[A\n",
            "Iteration:   7% 257/3742 [01:23<18:51,  3.08it/s]\u001b[A\n",
            "Iteration:   7% 258/3742 [01:24<18:50,  3.08it/s]\u001b[A\n",
            "Iteration:   7% 259/3742 [01:24<18:52,  3.08it/s]\u001b[A\n",
            "Iteration:   7% 260/3742 [01:24<18:50,  3.08it/s]\u001b[A\n",
            "Iteration:   7% 261/3742 [01:24<18:53,  3.07it/s]\u001b[A\n",
            "Iteration:   7% 262/3742 [01:25<18:51,  3.07it/s]\u001b[A\n",
            "Iteration:   7% 263/3742 [01:25<18:50,  3.08it/s]\u001b[A\n",
            "Iteration:   7% 264/3742 [01:25<18:50,  3.08it/s]\u001b[A\n",
            "Iteration:   7% 265/3742 [01:26<18:52,  3.07it/s]\u001b[A\n",
            "Iteration:   7% 266/3742 [01:26<18:51,  3.07it/s]\u001b[A\n",
            "Iteration:   7% 267/3742 [01:26<18:49,  3.08it/s]\u001b[A\n",
            "Iteration:   7% 268/3742 [01:27<18:48,  3.08it/s]\u001b[A\n",
            "Iteration:   7% 269/3742 [01:27<18:50,  3.07it/s]\u001b[A\n",
            "Iteration:   7% 270/3742 [01:27<18:50,  3.07it/s]\u001b[A\n",
            "Iteration:   7% 271/3742 [01:28<18:51,  3.07it/s]\u001b[A\n",
            "Iteration:   7% 272/3742 [01:28<18:51,  3.07it/s]\u001b[A\n",
            "Iteration:   7% 273/3742 [01:28<18:50,  3.07it/s]\u001b[A\n",
            "Iteration:   7% 274/3742 [01:29<18:50,  3.07it/s]\u001b[A\n",
            "Iteration:   7% 275/3742 [01:29<18:49,  3.07it/s]\u001b[A\n",
            "Iteration:   7% 276/3742 [01:29<18:46,  3.08it/s]\u001b[A\n",
            "Iteration:   7% 277/3742 [01:30<18:45,  3.08it/s]\u001b[A\n",
            "Iteration:   7% 278/3742 [01:30<18:44,  3.08it/s]\u001b[A\n",
            "Iteration:   7% 279/3742 [01:30<18:44,  3.08it/s]\u001b[A\n",
            "Iteration:   7% 280/3742 [01:31<18:43,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 281/3742 [01:31<18:43,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 282/3742 [01:31<18:42,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 283/3742 [01:32<18:42,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 284/3742 [01:32<18:41,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 285/3742 [01:32<18:41,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 286/3742 [01:33<18:43,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 287/3742 [01:33<18:43,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 288/3742 [01:33<18:43,  3.07it/s]\u001b[A\n",
            "Iteration:   8% 289/3742 [01:34<18:42,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 290/3742 [01:34<18:42,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 291/3742 [01:34<18:41,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 292/3742 [01:35<18:42,  3.07it/s]\u001b[A\n",
            "Iteration:   8% 293/3742 [01:35<18:40,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 294/3742 [01:35<18:40,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 295/3742 [01:36<18:40,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 296/3742 [01:36<18:38,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 297/3742 [01:36<18:40,  3.07it/s]\u001b[A\n",
            "Iteration:   8% 298/3742 [01:37<18:47,  3.05it/s]\u001b[A\n",
            "Iteration:   8% 299/3742 [01:37<18:51,  3.04it/s]\u001b[A\n",
            "Iteration:   8% 300/3742 [01:37<18:47,  3.05it/s]\u001b[A\n",
            "Iteration:   8% 301/3742 [01:38<18:43,  3.06it/s]\u001b[A\n",
            "Iteration:   8% 302/3742 [01:38<18:41,  3.07it/s]\u001b[A\n",
            "Iteration:   8% 303/3742 [01:38<18:39,  3.07it/s]\u001b[A\n",
            "Iteration:   8% 304/3742 [01:38<18:37,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 305/3742 [01:39<18:35,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 306/3742 [01:39<18:34,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 307/3742 [01:39<18:34,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 308/3742 [01:40<18:33,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 309/3742 [01:40<18:34,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 310/3742 [01:40<18:33,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 311/3742 [01:41<18:35,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 312/3742 [01:41<18:35,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 313/3742 [01:41<18:33,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 314/3742 [01:42<18:32,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 315/3742 [01:42<18:31,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 316/3742 [01:42<18:30,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 317/3742 [01:43<18:30,  3.08it/s]\u001b[A\n",
            "Iteration:   8% 318/3742 [01:43<18:29,  3.09it/s]\u001b[A\n",
            "Iteration:   9% 319/3742 [01:43<18:29,  3.09it/s]\u001b[A\n",
            "Iteration:   9% 320/3742 [01:44<18:28,  3.09it/s]\u001b[A\n",
            "Iteration:   9% 321/3742 [01:44<18:27,  3.09it/s]\u001b[A\n",
            "Iteration:   9% 322/3742 [01:44<18:27,  3.09it/s]\u001b[A\n",
            "Iteration:   9% 323/3742 [01:45<18:28,  3.09it/s]\u001b[A\n",
            "Iteration:   9% 324/3742 [01:45<18:27,  3.09it/s]\u001b[A\n",
            "Iteration:   9% 325/3742 [01:45<18:28,  3.08it/s]\u001b[A\n",
            "Iteration:   9% 326/3742 [01:46<18:27,  3.08it/s]\u001b[A\n",
            "Iteration:   9% 327/3742 [01:46<18:27,  3.08it/s]\u001b[A\n",
            "Iteration:   9% 328/3742 [01:46<18:26,  3.08it/s]\u001b[A\n",
            "Iteration:   9% 329/3742 [01:47<18:26,  3.09it/s]\u001b[A\n",
            "Iteration:   9% 330/3742 [01:47<18:27,  3.08it/s]\u001b[A\n",
            "Iteration:   9% 331/3742 [01:47<18:26,  3.08it/s]\u001b[A\n",
            "Iteration:   9% 332/3742 [01:48<18:26,  3.08it/s]\u001b[A\n",
            "Iteration:   9% 333/3742 [01:48<18:25,  3.08it/s]\u001b[A\n",
            "Iteration:   9% 334/3742 [01:48<18:24,  3.08it/s]\u001b[A\n",
            "Iteration:   9% 335/3742 [01:49<18:23,  3.09it/s]\u001b[A\n",
            "Iteration:   9% 336/3742 [01:49<18:24,  3.08it/s]\u001b[A\n",
            "Iteration:   9% 337/3742 [01:49<18:25,  3.08it/s]\u001b[A\n",
            "Iteration:   9% 338/3742 [01:50<18:26,  3.08it/s]\u001b[A\n",
            "Iteration:   9% 339/3742 [01:50<18:25,  3.08it/s]\u001b[A\n",
            "Iteration:   9% 340/3742 [01:50<18:24,  3.08it/s]\u001b[A\n",
            "Iteration:   9% 341/3742 [01:50<18:24,  3.08it/s]\u001b[A\n",
            "Iteration:   9% 342/3742 [01:51<18:23,  3.08it/s]\u001b[A\n",
            "Iteration:   9% 343/3742 [01:51<18:22,  3.08it/s]\u001b[A\n",
            "Iteration:   9% 344/3742 [01:51<18:22,  3.08it/s]\u001b[A\n",
            "Iteration:   9% 345/3742 [01:52<18:21,  3.08it/s]\u001b[A\n",
            "Iteration:   9% 346/3742 [01:52<18:23,  3.08it/s]\u001b[A\n",
            "Iteration:   9% 347/3742 [01:52<18:23,  3.08it/s]\u001b[A\n",
            "Iteration:   9% 348/3742 [01:53<18:24,  3.07it/s]\u001b[A\n",
            "Iteration:   9% 349/3742 [01:53<18:26,  3.07it/s]\u001b[A\n",
            "Iteration:   9% 350/3742 [01:53<18:24,  3.07it/s]\u001b[A\n",
            "Iteration:   9% 351/3742 [01:54<18:22,  3.08it/s]\u001b[A\n",
            "Iteration:   9% 352/3742 [01:54<18:22,  3.08it/s]\u001b[A\n",
            "Iteration:   9% 353/3742 [01:54<18:22,  3.08it/s]\u001b[A\n",
            "Iteration:   9% 354/3742 [01:55<18:19,  3.08it/s]\u001b[A\n",
            "Iteration:   9% 355/3742 [01:55<18:19,  3.08it/s]\u001b[A\n",
            "Iteration:  10% 356/3742 [01:55<18:20,  3.08it/s]\u001b[A\n",
            "Iteration:  10% 357/3742 [01:56<18:23,  3.07it/s]\u001b[A\n",
            "Iteration:  10% 358/3742 [01:56<18:23,  3.07it/s]\u001b[A\n",
            "Iteration:  10% 359/3742 [01:56<18:22,  3.07it/s]\u001b[A\n",
            "Iteration:  10% 360/3742 [01:57<18:23,  3.07it/s]\u001b[A\n",
            "Iteration:  10% 361/3742 [01:57<18:24,  3.06it/s]\u001b[A\n",
            "Iteration:  10% 362/3742 [01:57<18:22,  3.07it/s]\u001b[A\n",
            "Iteration:  10% 363/3742 [01:58<18:20,  3.07it/s]\u001b[A\n",
            "Iteration:  10% 364/3742 [01:58<18:19,  3.07it/s]\u001b[A\n",
            "Iteration:  10% 365/3742 [01:58<18:18,  3.07it/s]\u001b[A\n",
            "Iteration:  10% 366/3742 [01:59<18:17,  3.08it/s]\u001b[A\n",
            "Iteration:  10% 367/3742 [01:59<18:16,  3.08it/s]\u001b[A\n",
            "Iteration:  10% 368/3742 [01:59<18:15,  3.08it/s]\u001b[A\n",
            "Iteration:  10% 369/3742 [02:00<18:14,  3.08it/s]\u001b[A\n",
            "Iteration:  10% 370/3742 [02:00<18:14,  3.08it/s]\u001b[A\n",
            "Iteration:  10% 371/3742 [02:00<18:14,  3.08it/s]\u001b[A\n",
            "Iteration:  10% 372/3742 [02:01<18:13,  3.08it/s]\u001b[A\n",
            "Iteration:  10% 373/3742 [02:01<18:14,  3.08it/s]\u001b[A\n",
            "Iteration:  10% 374/3742 [02:01<18:16,  3.07it/s]\u001b[A\n",
            "Iteration:  10% 375/3742 [02:02<18:16,  3.07it/s]\u001b[A\n",
            "Iteration:  10% 376/3742 [02:02<18:14,  3.07it/s]\u001b[A\n",
            "Iteration:  10% 377/3742 [02:02<18:15,  3.07it/s]\u001b[A\n",
            "Iteration:  10% 378/3742 [02:03<18:14,  3.07it/s]\u001b[A\n",
            "Iteration:  10% 379/3742 [02:03<18:13,  3.08it/s]\u001b[A\n",
            "Iteration:  10% 380/3742 [02:03<18:12,  3.08it/s]\u001b[A\n",
            "Iteration:  10% 381/3742 [02:03<18:12,  3.08it/s]\u001b[A\n",
            "Iteration:  10% 382/3742 [02:04<18:12,  3.08it/s]\u001b[A\n",
            "Iteration:  10% 383/3742 [02:04<18:12,  3.07it/s]\u001b[A\n",
            "Iteration:  10% 384/3742 [02:04<18:11,  3.08it/s]\u001b[A\n",
            "Iteration:  10% 385/3742 [02:05<18:11,  3.08it/s]\u001b[A\n",
            "Iteration:  10% 386/3742 [02:05<18:10,  3.08it/s]\u001b[A\n",
            "Iteration:  10% 387/3742 [02:05<18:09,  3.08it/s]\u001b[A\n",
            "Iteration:  10% 388/3742 [02:06<18:10,  3.08it/s]\u001b[A\n",
            "Iteration:  10% 389/3742 [02:06<18:10,  3.08it/s]\u001b[A\n",
            "Iteration:  10% 390/3742 [02:06<18:11,  3.07it/s]\u001b[A\n",
            "Iteration:  10% 391/3742 [02:07<18:12,  3.07it/s]\u001b[A\n",
            "Iteration:  10% 392/3742 [02:07<18:10,  3.07it/s]\u001b[A\n",
            "Iteration:  11% 393/3742 [02:07<18:08,  3.08it/s]\u001b[A\n",
            "Iteration:  11% 394/3742 [02:08<18:08,  3.08it/s]\u001b[A\n",
            "Iteration:  11% 395/3742 [02:08<18:07,  3.08it/s]\u001b[A\n",
            "Iteration:  11% 396/3742 [02:08<18:12,  3.06it/s]\u001b[A\n",
            "Iteration:  11% 397/3742 [02:09<18:10,  3.07it/s]\u001b[A\n",
            "Iteration:  11% 398/3742 [02:09<18:09,  3.07it/s]\u001b[A\n",
            "Iteration:  11% 399/3742 [02:09<18:07,  3.07it/s]\u001b[A\n",
            "Iteration:  11% 400/3742 [02:10<18:08,  3.07it/s]\u001b[A\n",
            "Iteration:  11% 401/3742 [02:10<18:07,  3.07it/s]\u001b[A\n",
            "Iteration:  11% 402/3742 [02:10<18:06,  3.07it/s]\u001b[A\n",
            "Iteration:  11% 403/3742 [02:11<18:05,  3.08it/s]\u001b[A\n",
            "Iteration:  11% 404/3742 [02:11<18:04,  3.08it/s]\u001b[A\n",
            "Iteration:  11% 405/3742 [02:11<18:04,  3.08it/s]\u001b[A\n",
            "Iteration:  11% 406/3742 [02:12<18:03,  3.08it/s]\u001b[A\n",
            "Iteration:  11% 407/3742 [02:12<18:04,  3.08it/s]\u001b[A\n",
            "Iteration:  11% 408/3742 [02:12<18:03,  3.08it/s]\u001b[A\n",
            "Iteration:  11% 409/3742 [02:13<18:06,  3.07it/s]\u001b[A\n",
            "Iteration:  11% 410/3742 [02:13<18:07,  3.06it/s]\u001b[A\n",
            "Iteration:  11% 411/3742 [02:13<18:05,  3.07it/s]\u001b[A\n",
            "Iteration:  11% 412/3742 [02:14<18:04,  3.07it/s]\u001b[A\n",
            "Iteration:  11% 413/3742 [02:14<18:01,  3.08it/s]\u001b[A\n",
            "Iteration:  11% 414/3742 [02:14<18:02,  3.07it/s]\u001b[A\n",
            "Iteration:  11% 415/3742 [02:15<18:01,  3.08it/s]\u001b[A\n",
            "Iteration:  11% 416/3742 [02:15<18:02,  3.07it/s]\u001b[A\n",
            "Iteration:  11% 417/3742 [02:15<18:02,  3.07it/s]\u001b[A\n",
            "Iteration:  11% 418/3742 [02:16<18:17,  3.03it/s]\u001b[A\n",
            "Iteration:  11% 419/3742 [02:16<18:12,  3.04it/s]\u001b[A\n",
            "Iteration:  11% 420/3742 [02:16<18:07,  3.05it/s]\u001b[A\n",
            "Iteration:  11% 421/3742 [02:17<18:04,  3.06it/s]\u001b[A\n",
            "Iteration:  11% 422/3742 [02:17<18:04,  3.06it/s]\u001b[A\n",
            "Iteration:  11% 423/3742 [02:17<18:04,  3.06it/s]\u001b[A\n",
            "Iteration:  11% 424/3742 [02:17<18:01,  3.07it/s]\u001b[A\n",
            "Iteration:  11% 425/3742 [02:18<17:59,  3.07it/s]\u001b[A\n",
            "Iteration:  11% 426/3742 [02:18<17:57,  3.08it/s]\u001b[A\n",
            "Iteration:  11% 427/3742 [02:18<17:59,  3.07it/s]\u001b[A\n",
            "Iteration:  11% 428/3742 [02:19<17:59,  3.07it/s]\u001b[A\n",
            "Iteration:  11% 429/3742 [02:19<17:57,  3.07it/s]\u001b[A\n",
            "Iteration:  11% 430/3742 [02:19<17:56,  3.08it/s]\u001b[A\n",
            "Iteration:  12% 431/3742 [02:20<17:55,  3.08it/s]\u001b[A\n",
            "Iteration:  12% 432/3742 [02:20<17:57,  3.07it/s]\u001b[A\n",
            "Iteration:  12% 433/3742 [02:20<17:57,  3.07it/s]\u001b[A\n",
            "Iteration:  12% 434/3742 [02:21<17:55,  3.08it/s]\u001b[A\n",
            "Iteration:  12% 435/3742 [02:21<17:54,  3.08it/s]\u001b[A\n",
            "Iteration:  12% 436/3742 [02:21<17:56,  3.07it/s]\u001b[A\n",
            "Iteration:  12% 437/3742 [02:22<17:56,  3.07it/s]\u001b[A\n",
            "Iteration:  12% 438/3742 [02:22<17:54,  3.08it/s]\u001b[A\n",
            "Iteration:  12% 439/3742 [02:22<17:53,  3.08it/s]\u001b[A\n",
            "Iteration:  12% 440/3742 [02:23<17:55,  3.07it/s]\u001b[A\n",
            "Iteration:  12% 441/3742 [02:23<17:55,  3.07it/s]\u001b[A\n",
            "Iteration:  12% 442/3742 [02:23<17:52,  3.08it/s]\u001b[A\n",
            "Iteration:  12% 443/3742 [02:24<17:51,  3.08it/s]\u001b[A\n",
            "Iteration:  12% 444/3742 [02:24<17:50,  3.08it/s]\u001b[A\n",
            "Iteration:  12% 445/3742 [02:24<17:50,  3.08it/s]\u001b[A\n",
            "Iteration:  12% 446/3742 [02:25<17:50,  3.08it/s]\u001b[A\n",
            "Iteration:  12% 447/3742 [02:25<17:49,  3.08it/s]\u001b[A\n",
            "Iteration:  12% 448/3742 [02:25<17:48,  3.08it/s]\u001b[A\n",
            "Iteration:  12% 449/3742 [02:26<17:48,  3.08it/s]\u001b[A\n",
            "Iteration:  12% 450/3742 [02:26<17:49,  3.08it/s]\u001b[A\n",
            "Iteration:  12% 451/3742 [02:26<17:51,  3.07it/s]\u001b[A\n",
            "Iteration:  12% 452/3742 [02:27<17:50,  3.07it/s]\u001b[A\n",
            "Iteration:  12% 453/3742 [02:27<17:50,  3.07it/s]\u001b[A\n",
            "Iteration:  12% 454/3742 [02:27<17:52,  3.07it/s]\u001b[A\n",
            "Iteration:  12% 455/3742 [02:28<17:51,  3.07it/s]\u001b[A\n",
            "Iteration:  12% 456/3742 [02:28<17:50,  3.07it/s]\u001b[A\n",
            "Iteration:  12% 457/3742 [02:28<17:48,  3.08it/s]\u001b[A\n",
            "Iteration:  12% 458/3742 [02:29<17:47,  3.08it/s]\u001b[A\n",
            "Iteration:  12% 459/3742 [02:29<17:47,  3.07it/s]\u001b[A\n",
            "Iteration:  12% 460/3742 [02:29<17:47,  3.07it/s]\u001b[A\n",
            "Iteration:  12% 461/3742 [02:30<17:46,  3.08it/s]\u001b[A\n",
            "Iteration:  12% 462/3742 [02:30<17:47,  3.07it/s]\u001b[A\n",
            "Iteration:  12% 463/3742 [02:30<17:45,  3.08it/s]\u001b[A\n",
            "Iteration:  12% 464/3742 [02:31<17:44,  3.08it/s]\u001b[A\n",
            "Iteration:  12% 465/3742 [02:31<17:44,  3.08it/s]\u001b[A\n",
            "Iteration:  12% 466/3742 [02:31<17:45,  3.07it/s]\u001b[A\n",
            "Iteration:  12% 467/3742 [02:31<17:46,  3.07it/s]\u001b[A\n",
            "Iteration:  13% 468/3742 [02:32<17:44,  3.08it/s]\u001b[A\n",
            "Iteration:  13% 469/3742 [02:32<17:42,  3.08it/s]\u001b[A\n",
            "Iteration:  13% 470/3742 [02:32<17:41,  3.08it/s]\u001b[A\n",
            "Iteration:  13% 471/3742 [02:33<17:41,  3.08it/s]\u001b[A\n",
            "Iteration:  13% 472/3742 [02:33<17:40,  3.08it/s]\u001b[A\n",
            "Iteration:  13% 473/3742 [02:33<17:39,  3.08it/s]\u001b[A\n",
            "Iteration:  13% 474/3742 [02:34<17:39,  3.09it/s]\u001b[A\n",
            "Iteration:  13% 475/3742 [02:34<17:39,  3.08it/s]\u001b[A\n",
            "Iteration:  13% 476/3742 [02:34<17:38,  3.08it/s]\u001b[A\n",
            "Iteration:  13% 477/3742 [02:35<17:38,  3.08it/s]\u001b[A\n",
            "Iteration:  13% 478/3742 [02:35<17:38,  3.08it/s]\u001b[A\n",
            "Iteration:  13% 479/3742 [02:35<17:37,  3.09it/s]\u001b[A\n",
            "Iteration:  13% 480/3742 [02:36<17:39,  3.08it/s]\u001b[A\n",
            "Iteration:  13% 481/3742 [02:36<17:38,  3.08it/s]\u001b[A\n",
            "Iteration:  13% 482/3742 [02:36<17:42,  3.07it/s]\u001b[A\n",
            "Iteration:  13% 483/3742 [02:37<17:46,  3.06it/s]\u001b[A\n",
            "Iteration:  13% 484/3742 [02:37<17:44,  3.06it/s]\u001b[A\n",
            "Iteration:  13% 485/3742 [02:37<17:41,  3.07it/s]\u001b[A\n",
            "Iteration:  13% 486/3742 [02:38<17:39,  3.07it/s]\u001b[A\n",
            "Iteration:  13% 487/3742 [02:38<17:38,  3.08it/s]\u001b[A\n",
            "Iteration:  13% 488/3742 [02:38<17:37,  3.08it/s]\u001b[A\n",
            "Iteration:  13% 489/3742 [02:39<17:36,  3.08it/s]\u001b[A\n",
            "Iteration:  13% 490/3742 [02:39<17:35,  3.08it/s]\u001b[A\n",
            "Iteration:  13% 491/3742 [02:39<17:34,  3.08it/s]\u001b[A\n",
            "Iteration:  13% 492/3742 [02:40<17:33,  3.08it/s]\u001b[A\n",
            "Iteration:  13% 493/3742 [02:40<17:33,  3.08it/s]\u001b[A\n",
            "Iteration:  13% 494/3742 [02:40<17:33,  3.08it/s]\u001b[A\n",
            "Iteration:  13% 495/3742 [02:41<17:33,  3.08it/s]\u001b[A\n",
            "Iteration:  13% 496/3742 [02:41<17:33,  3.08it/s]\u001b[A\n",
            "Iteration:  13% 497/3742 [02:41<17:32,  3.08it/s]\u001b[A\n",
            "Iteration:  13% 498/3742 [02:42<17:32,  3.08it/s]\u001b[A\n",
            "Iteration:  13% 499/3742 [02:42<17:31,  3.08it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "03/05/2020 06:53:27 - INFO - transformers.configuration_utils -   Configuration saved in output_roberta_US/checkpoint-500/config.json\n",
            "03/05/2020 06:53:28 - INFO - transformers.modeling_utils -   Model weights saved in output_roberta_US/checkpoint-500/pytorch_model.bin\n",
            "03/05/2020 06:53:28 - INFO - __main__ -   Saving model checkpoint to output_roberta_US/checkpoint-500\n",
            "03/05/2020 06:53:32 - INFO - __main__ -   Saving optimizer and scheduler states to output_roberta_US/checkpoint-500\n",
            "\n",
            "Iteration:  13% 500/3742 [02:47<1:31:47,  1.70s/it]\u001b[A\n",
            "Iteration:  13% 501/3742 [02:47<1:09:46,  1.29s/it]\u001b[A\n",
            "Iteration:  13% 502/3742 [02:47<54:10,  1.00s/it]  \u001b[A\n",
            "Iteration:  13% 503/3742 [02:48<43:12,  1.25it/s]\u001b[A\n",
            "Iteration:  13% 504/3742 [02:48<35:31,  1.52it/s]\u001b[A\n",
            "Iteration:  13% 505/3742 [02:48<30:10,  1.79it/s]\u001b[A\n",
            "Iteration:  14% 506/3742 [02:49<26:25,  2.04it/s]\u001b[A\n",
            "Iteration:  14% 507/3742 [02:49<23:44,  2.27it/s]\u001b[A\n",
            "Iteration:  14% 508/3742 [02:49<21:51,  2.47it/s]\u001b[A\n",
            "Iteration:  14% 509/3742 [02:50<20:33,  2.62it/s]\u001b[A\n",
            "Iteration:  14% 510/3742 [02:50<19:37,  2.74it/s]\u001b[A\n",
            "Iteration:  14% 511/3742 [02:50<18:59,  2.84it/s]\u001b[A\n",
            "Iteration:  14% 512/3742 [02:51<18:33,  2.90it/s]\u001b[A\n",
            "Iteration:  14% 513/3742 [02:51<18:13,  2.95it/s]\u001b[A\n",
            "Iteration:  14% 514/3742 [02:51<17:59,  2.99it/s]\u001b[A\n",
            "Iteration:  14% 515/3742 [02:52<17:50,  3.01it/s]\u001b[A\n",
            "Iteration:  14% 516/3742 [02:52<17:43,  3.03it/s]\u001b[A\n",
            "Iteration:  14% 517/3742 [02:52<17:39,  3.05it/s]\u001b[A\n",
            "Iteration:  14% 518/3742 [02:53<17:41,  3.04it/s]\u001b[A\n",
            "Iteration:  14% 519/3742 [02:53<17:42,  3.03it/s]\u001b[A\n",
            "Iteration:  14% 520/3742 [02:53<17:38,  3.05it/s]\u001b[A\n",
            "Iteration:  14% 521/3742 [02:54<17:33,  3.06it/s]\u001b[A\n",
            "Iteration:  14% 522/3742 [02:54<17:31,  3.06it/s]\u001b[A\n",
            "Iteration:  14% 523/3742 [02:54<17:28,  3.07it/s]\u001b[A\n",
            "Iteration:  14% 524/3742 [02:55<17:27,  3.07it/s]\u001b[A\n",
            "Iteration:  14% 525/3742 [02:55<17:25,  3.08it/s]\u001b[A\n",
            "Iteration:  14% 526/3742 [02:55<17:24,  3.08it/s]\u001b[A\n",
            "Iteration:  14% 527/3742 [02:56<17:23,  3.08it/s]\u001b[A\n",
            "Iteration:  14% 528/3742 [02:56<17:23,  3.08it/s]\u001b[A\n",
            "Iteration:  14% 529/3742 [02:56<17:26,  3.07it/s]\u001b[A\n",
            "Iteration:  14% 530/3742 [02:57<17:27,  3.07it/s]\u001b[A\n",
            "Iteration:  14% 531/3742 [02:57<17:26,  3.07it/s]\u001b[A\n",
            "Iteration:  14% 532/3742 [02:57<17:25,  3.07it/s]\u001b[A\n",
            "Iteration:  14% 533/3742 [02:58<17:25,  3.07it/s]\u001b[A\n",
            "Iteration:  14% 534/3742 [02:58<17:23,  3.07it/s]\u001b[A\n",
            "Iteration:  14% 535/3742 [02:58<17:22,  3.08it/s]\u001b[A\n",
            "Iteration:  14% 536/3742 [02:59<17:26,  3.06it/s]\u001b[A\n",
            "Iteration:  14% 537/3742 [02:59<17:24,  3.07it/s]\u001b[A\n",
            "Iteration:  14% 538/3742 [02:59<17:23,  3.07it/s]\u001b[A\n",
            "Iteration:  14% 539/3742 [02:59<17:21,  3.08it/s]\u001b[A\n",
            "Iteration:  14% 540/3742 [03:00<17:19,  3.08it/s]\u001b[A\n",
            "Iteration:  14% 541/3742 [03:00<17:19,  3.08it/s]\u001b[A\n",
            "Iteration:  14% 542/3742 [03:00<17:18,  3.08it/s]\u001b[A\n",
            "Iteration:  15% 543/3742 [03:01<17:20,  3.07it/s]\u001b[A\n",
            "Iteration:  15% 544/3742 [03:01<17:21,  3.07it/s]\u001b[A\n",
            "Iteration:  15% 545/3742 [03:01<17:20,  3.07it/s]\u001b[A\n",
            "Iteration:  15% 546/3742 [03:02<17:20,  3.07it/s]\u001b[A\n",
            "Iteration:  15% 547/3742 [03:02<17:20,  3.07it/s]\u001b[A\n",
            "Iteration:  15% 548/3742 [03:02<17:18,  3.08it/s]\u001b[A\n",
            "Iteration:  15% 549/3742 [03:03<17:19,  3.07it/s]\u001b[A\n",
            "Iteration:  15% 550/3742 [03:03<17:18,  3.07it/s]\u001b[A\n",
            "Iteration:  15% 551/3742 [03:03<17:17,  3.08it/s]\u001b[A\n",
            "Iteration:  15% 552/3742 [03:04<17:16,  3.08it/s]\u001b[A\n",
            "Iteration:  15% 553/3742 [03:04<17:15,  3.08it/s]\u001b[A\n",
            "Iteration:  15% 554/3742 [03:04<17:14,  3.08it/s]\u001b[A\n",
            "Iteration:  15% 555/3742 [03:05<17:13,  3.08it/s]\u001b[A\n",
            "Iteration:  15% 556/3742 [03:05<17:12,  3.09it/s]\u001b[A\n",
            "Iteration:  15% 557/3742 [03:05<17:13,  3.08it/s]\u001b[A\n",
            "Iteration:  15% 558/3742 [03:06<17:13,  3.08it/s]\u001b[A\n",
            "Iteration:  15% 559/3742 [03:06<17:13,  3.08it/s]\u001b[A\n",
            "Iteration:  15% 560/3742 [03:06<17:13,  3.08it/s]\u001b[A\n",
            "Iteration:  15% 561/3742 [03:07<17:12,  3.08it/s]\u001b[A\n",
            "Iteration:  15% 562/3742 [03:07<17:11,  3.08it/s]\u001b[A\n",
            "Iteration:  15% 563/3742 [03:07<17:10,  3.08it/s]\u001b[A\n",
            "Iteration:  15% 564/3742 [03:08<17:09,  3.09it/s]\u001b[A\n",
            "Iteration:  15% 565/3742 [03:08<17:09,  3.09it/s]\u001b[A\n",
            "Iteration:  15% 566/3742 [03:08<17:09,  3.09it/s]\u001b[A\n",
            "Iteration:  15% 567/3742 [03:09<17:09,  3.08it/s]\u001b[A\n",
            "Iteration:  15% 568/3742 [03:09<17:09,  3.08it/s]\u001b[A\n",
            "Iteration:  15% 569/3742 [03:09<17:09,  3.08it/s]\u001b[A\n",
            "Iteration:  15% 570/3742 [03:10<17:09,  3.08it/s]\u001b[A\n",
            "Iteration:  15% 571/3742 [03:10<17:14,  3.07it/s]\u001b[A\n",
            "Iteration:  15% 572/3742 [03:10<17:12,  3.07it/s]\u001b[A\n",
            "Iteration:  15% 573/3742 [03:11<17:11,  3.07it/s]\u001b[A\n",
            "Iteration:  15% 574/3742 [03:11<17:09,  3.08it/s]\u001b[A\n",
            "Iteration:  15% 575/3742 [03:11<17:08,  3.08it/s]\u001b[A\n",
            "Iteration:  15% 576/3742 [03:12<17:08,  3.08it/s]\u001b[A\n",
            "Iteration:  15% 577/3742 [03:12<17:08,  3.08it/s]\u001b[A\n",
            "Iteration:  15% 578/3742 [03:12<17:08,  3.08it/s]\u001b[A\n",
            "Iteration:  15% 579/3742 [03:12<17:12,  3.06it/s]\u001b[A\n",
            "Iteration:  15% 580/3742 [03:13<17:15,  3.05it/s]\u001b[A\n",
            "Iteration:  16% 581/3742 [03:13<17:13,  3.06it/s]\u001b[A\n",
            "Iteration:  16% 582/3742 [03:13<17:12,  3.06it/s]\u001b[A\n",
            "Iteration:  16% 583/3742 [03:14<17:11,  3.06it/s]\u001b[A\n",
            "Iteration:  16% 584/3742 [03:14<17:08,  3.07it/s]\u001b[A\n",
            "Iteration:  16% 585/3742 [03:14<17:07,  3.07it/s]\u001b[A\n",
            "Iteration:  16% 586/3742 [03:15<17:05,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 587/3742 [03:15<17:04,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 588/3742 [03:15<17:03,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 589/3742 [03:16<17:03,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 590/3742 [03:16<17:03,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 591/3742 [03:16<17:01,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 592/3742 [03:17<17:00,  3.09it/s]\u001b[A\n",
            "Iteration:  16% 593/3742 [03:17<17:03,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 594/3742 [03:17<17:02,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 595/3742 [03:18<17:04,  3.07it/s]\u001b[A\n",
            "Iteration:  16% 596/3742 [03:18<17:04,  3.07it/s]\u001b[A\n",
            "Iteration:  16% 597/3742 [03:18<17:02,  3.07it/s]\u001b[A\n",
            "Iteration:  16% 598/3742 [03:19<17:01,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 599/3742 [03:19<17:00,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 600/3742 [03:19<16:59,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 601/3742 [03:20<17:00,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 602/3742 [03:20<16:59,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 603/3742 [03:20<16:58,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 604/3742 [03:21<16:58,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 605/3742 [03:21<16:57,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 606/3742 [03:21<16:56,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 607/3742 [03:22<16:57,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 608/3742 [03:22<16:56,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 609/3742 [03:22<16:58,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 610/3742 [03:23<16:57,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 611/3742 [03:23<16:56,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 612/3742 [03:23<16:55,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 613/3742 [03:24<16:54,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 614/3742 [03:24<16:55,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 615/3742 [03:24<16:55,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 616/3742 [03:25<16:54,  3.08it/s]\u001b[A\n",
            "Iteration:  16% 617/3742 [03:25<16:54,  3.08it/s]\u001b[A\n",
            "Iteration:  17% 618/3742 [03:25<16:54,  3.08it/s]\u001b[A\n",
            "Iteration:  17% 619/3742 [03:26<17:21,  3.00it/s]\u001b[A\n",
            "Iteration:  17% 620/3742 [03:26<17:17,  3.01it/s]\u001b[A\n",
            "Iteration:  17% 621/3742 [03:26<17:13,  3.02it/s]\u001b[A\n",
            "Iteration:  17% 622/3742 [03:27<17:07,  3.04it/s]\u001b[A\n",
            "Iteration:  17% 623/3742 [03:27<17:03,  3.05it/s]\u001b[A\n",
            "Iteration:  17% 624/3742 [03:27<17:00,  3.06it/s]\u001b[A\n",
            "Iteration:  17% 625/3742 [03:27<16:59,  3.06it/s]\u001b[A\n",
            "Iteration:  17% 626/3742 [03:28<16:57,  3.06it/s]\u001b[A\n",
            "Iteration:  17% 627/3742 [03:28<16:55,  3.07it/s]\u001b[A\n",
            "Iteration:  17% 628/3742 [03:28<16:55,  3.07it/s]\u001b[A\n",
            "Iteration:  17% 629/3742 [03:29<16:56,  3.06it/s]\u001b[A\n",
            "Iteration:  17% 630/3742 [03:29<16:54,  3.07it/s]\u001b[A\n",
            "Iteration:  17% 631/3742 [03:29<16:52,  3.07it/s]\u001b[A\n",
            "Iteration:  17% 632/3742 [03:30<16:50,  3.08it/s]\u001b[A\n",
            "Iteration:  17% 633/3742 [03:30<16:49,  3.08it/s]\u001b[A\n",
            "Iteration:  17% 634/3742 [03:30<16:49,  3.08it/s]\u001b[A\n",
            "Iteration:  17% 635/3742 [03:31<16:49,  3.08it/s]\u001b[A\n",
            "Iteration:  17% 636/3742 [03:31<16:48,  3.08it/s]\u001b[A\n",
            "Iteration:  17% 637/3742 [03:31<16:47,  3.08it/s]\u001b[A\n",
            "Iteration:  17% 638/3742 [03:32<16:46,  3.08it/s]\u001b[A\n",
            "Iteration:  17% 639/3742 [03:32<16:47,  3.08it/s]\u001b[A\n",
            "Iteration:  17% 640/3742 [03:32<16:46,  3.08it/s]\u001b[A\n",
            "Iteration:  17% 641/3742 [03:33<16:46,  3.08it/s]\u001b[A\n",
            "Iteration:  17% 642/3742 [03:33<16:46,  3.08it/s]\u001b[A\n",
            "Iteration:  17% 643/3742 [03:33<16:45,  3.08it/s]\u001b[A\n",
            "Iteration:  17% 644/3742 [03:34<16:45,  3.08it/s]\u001b[A\n",
            "Iteration:  17% 645/3742 [03:34<16:44,  3.08it/s]\u001b[A\n",
            "Iteration:  17% 646/3742 [03:34<16:42,  3.09it/s]\u001b[A\n",
            "Iteration:  17% 647/3742 [03:35<16:43,  3.08it/s]\u001b[A\n",
            "Iteration:  17% 648/3742 [03:35<16:45,  3.08it/s]\u001b[A\n",
            "Iteration:  17% 649/3742 [03:35<16:44,  3.08it/s]\u001b[A\n",
            "Iteration:  17% 650/3742 [03:36<16:43,  3.08it/s]\u001b[A\n",
            "Iteration:  17% 651/3742 [03:36<16:43,  3.08it/s]\u001b[A\n",
            "Iteration:  17% 652/3742 [03:36<16:48,  3.06it/s]\u001b[A\n",
            "Iteration:  17% 653/3742 [03:37<16:51,  3.05it/s]\u001b[A\n",
            "Iteration:  17% 654/3742 [03:37<16:54,  3.04it/s]\u001b[A\n",
            "Iteration:  18% 655/3742 [03:37<16:52,  3.05it/s]\u001b[A\n",
            "Iteration:  18% 656/3742 [03:38<16:49,  3.06it/s]\u001b[A\n",
            "Iteration:  18% 657/3742 [03:38<16:45,  3.07it/s]\u001b[A\n",
            "Iteration:  18% 658/3742 [03:38<16:44,  3.07it/s]\u001b[A\n",
            "Iteration:  18% 659/3742 [03:39<16:42,  3.08it/s]\u001b[A\n",
            "Iteration:  18% 660/3742 [03:39<16:43,  3.07it/s]\u001b[A\n",
            "Iteration:  18% 661/3742 [03:39<16:42,  3.07it/s]\u001b[A\n",
            "Iteration:  18% 662/3742 [03:40<16:44,  3.07it/s]\u001b[A\n",
            "Iteration:  18% 663/3742 [03:40<16:43,  3.07it/s]\u001b[A\n",
            "Iteration:  18% 664/3742 [03:40<16:43,  3.07it/s]\u001b[A\n",
            "Iteration:  18% 665/3742 [03:40<16:41,  3.07it/s]\u001b[A\n",
            "Iteration:  18% 666/3742 [03:41<16:40,  3.07it/s]\u001b[A\n",
            "Iteration:  18% 667/3742 [03:41<16:39,  3.08it/s]\u001b[A\n",
            "Iteration:  18% 668/3742 [03:41<16:43,  3.06it/s]\u001b[A\n",
            "Iteration:  18% 669/3742 [03:42<16:41,  3.07it/s]\u001b[A\n",
            "Iteration:  18% 670/3742 [03:42<16:39,  3.07it/s]\u001b[A\n",
            "Iteration:  18% 671/3742 [03:42<16:39,  3.07it/s]\u001b[A\n",
            "Iteration:  18% 672/3742 [03:43<16:42,  3.06it/s]\u001b[A\n",
            "Iteration:  18% 673/3742 [03:43<16:40,  3.07it/s]\u001b[A\n",
            "Iteration:  18% 674/3742 [03:43<16:40,  3.07it/s]\u001b[A\n",
            "Iteration:  18% 675/3742 [03:44<16:38,  3.07it/s]\u001b[A\n",
            "Iteration:  18% 676/3742 [03:44<16:37,  3.07it/s]\u001b[A\n",
            "Iteration:  18% 677/3742 [03:44<16:38,  3.07it/s]\u001b[A\n",
            "Iteration:  18% 678/3742 [03:45<16:37,  3.07it/s]\u001b[A\n",
            "Iteration:  18% 679/3742 [03:45<16:36,  3.07it/s]\u001b[A\n",
            "Iteration:  18% 680/3742 [03:45<16:34,  3.08it/s]\u001b[A\n",
            "Iteration:  18% 681/3742 [03:46<16:34,  3.08it/s]\u001b[A\n",
            "Iteration:  18% 682/3742 [03:46<16:34,  3.08it/s]\u001b[A\n",
            "Iteration:  18% 683/3742 [03:46<16:33,  3.08it/s]\u001b[A\n",
            "Iteration:  18% 684/3742 [03:47<16:35,  3.07it/s]\u001b[A\n",
            "Iteration:  18% 685/3742 [03:47<16:38,  3.06it/s]\u001b[A\n",
            "Iteration:  18% 686/3742 [03:47<16:37,  3.06it/s]\u001b[A\n",
            "Iteration:  18% 687/3742 [03:48<16:42,  3.05it/s]\u001b[A\n",
            "Iteration:  18% 688/3742 [03:48<16:39,  3.05it/s]\u001b[A\n",
            "Iteration:  18% 689/3742 [03:48<16:36,  3.06it/s]\u001b[A\n",
            "Iteration:  18% 690/3742 [03:49<16:34,  3.07it/s]\u001b[A\n",
            "Iteration:  18% 691/3742 [03:49<16:34,  3.07it/s]\u001b[A\n",
            "Iteration:  18% 692/3742 [03:49<16:33,  3.07it/s]\u001b[A\n",
            "Iteration:  19% 693/3742 [03:50<16:32,  3.07it/s]\u001b[A\n",
            "Iteration:  19% 694/3742 [03:50<16:31,  3.07it/s]\u001b[A\n",
            "Iteration:  19% 695/3742 [03:50<16:30,  3.08it/s]\u001b[A\n",
            "Iteration:  19% 696/3742 [03:51<16:29,  3.08it/s]\u001b[A\n",
            "Iteration:  19% 697/3742 [03:51<16:28,  3.08it/s]\u001b[A\n",
            "Iteration:  19% 698/3742 [03:51<16:29,  3.07it/s]\u001b[A\n",
            "Iteration:  19% 699/3742 [03:52<16:28,  3.08it/s]\u001b[A\n",
            "Iteration:  19% 700/3742 [03:52<16:27,  3.08it/s]\u001b[A\n",
            "Iteration:  19% 701/3742 [03:52<16:26,  3.08it/s]\u001b[A\n",
            "Iteration:  19% 702/3742 [03:53<16:30,  3.07it/s]\u001b[A\n",
            "Iteration:  19% 703/3742 [03:53<16:33,  3.06it/s]\u001b[A\n",
            "Iteration:  19% 704/3742 [03:53<16:36,  3.05it/s]\u001b[A\n",
            "Iteration:  19% 705/3742 [03:54<16:33,  3.06it/s]\u001b[A\n",
            "Iteration:  19% 706/3742 [03:54<16:31,  3.06it/s]\u001b[A\n",
            "Iteration:  19% 707/3742 [03:54<16:28,  3.07it/s]\u001b[A\n",
            "Iteration:  19% 708/3742 [03:54<16:26,  3.07it/s]\u001b[A\n",
            "Iteration:  19% 709/3742 [03:55<16:29,  3.07it/s]\u001b[A\n",
            "Iteration:  19% 710/3742 [03:55<16:28,  3.07it/s]\u001b[A\n",
            "Iteration:  19% 711/3742 [03:55<16:26,  3.07it/s]\u001b[A\n",
            "Iteration:  19% 712/3742 [03:56<16:26,  3.07it/s]\u001b[A\n",
            "Iteration:  19% 713/3742 [03:56<16:24,  3.08it/s]\u001b[A\n",
            "Iteration:  19% 714/3742 [03:56<16:23,  3.08it/s]\u001b[A\n",
            "Iteration:  19% 715/3742 [03:57<16:22,  3.08it/s]\u001b[A\n",
            "Iteration:  19% 716/3742 [03:57<16:23,  3.08it/s]\u001b[A\n",
            "Iteration:  19% 717/3742 [03:57<16:23,  3.08it/s]\u001b[A\n",
            "Iteration:  19% 718/3742 [03:58<16:22,  3.08it/s]\u001b[A\n",
            "Iteration:  19% 719/3742 [03:58<16:22,  3.08it/s]\u001b[A\n",
            "Iteration:  19% 720/3742 [03:58<16:21,  3.08it/s]\u001b[A\n",
            "Iteration:  19% 721/3742 [03:59<16:22,  3.07it/s]\u001b[A\n",
            "Iteration:  19% 722/3742 [03:59<16:22,  3.07it/s]\u001b[A\n",
            "Iteration:  19% 723/3742 [03:59<16:21,  3.08it/s]\u001b[A\n",
            "Iteration:  19% 724/3742 [04:00<16:20,  3.08it/s]\u001b[A\n",
            "Iteration:  19% 725/3742 [04:00<16:20,  3.08it/s]\u001b[A\n",
            "Iteration:  19% 726/3742 [04:00<16:19,  3.08it/s]\u001b[A\n",
            "Iteration:  19% 727/3742 [04:01<16:19,  3.08it/s]\u001b[A\n",
            "Iteration:  19% 728/3742 [04:01<16:18,  3.08it/s]\u001b[A\n",
            "Iteration:  19% 729/3742 [04:01<16:20,  3.07it/s]\u001b[A\n",
            "Iteration:  20% 730/3742 [04:02<16:18,  3.08it/s]\u001b[A\n",
            "Iteration:  20% 731/3742 [04:02<16:19,  3.07it/s]\u001b[A\n",
            "Iteration:  20% 732/3742 [04:02<16:18,  3.08it/s]\u001b[A\n",
            "Iteration:  20% 733/3742 [04:03<16:17,  3.08it/s]\u001b[A\n",
            "Iteration:  20% 734/3742 [04:03<16:17,  3.08it/s]\u001b[A\n",
            "Iteration:  20% 735/3742 [04:03<16:17,  3.08it/s]\u001b[A\n",
            "Iteration:  20% 736/3742 [04:04<16:17,  3.08it/s]\u001b[A\n",
            "Iteration:  20% 737/3742 [04:04<16:16,  3.08it/s]\u001b[A\n",
            "Iteration:  20% 738/3742 [04:04<16:15,  3.08it/s]\u001b[A\n",
            "Iteration:  20% 739/3742 [04:05<16:14,  3.08it/s]\u001b[A\n",
            "Iteration:  20% 740/3742 [04:05<16:13,  3.08it/s]\u001b[A\n",
            "Iteration:  20% 741/3742 [04:05<16:13,  3.08it/s]\u001b[A\n",
            "Iteration:  20% 742/3742 [04:06<16:12,  3.09it/s]\u001b[A\n",
            "Iteration:  20% 743/3742 [04:06<16:13,  3.08it/s]\u001b[A\n",
            "Iteration:  20% 744/3742 [04:06<16:12,  3.08it/s]\u001b[A\n",
            "Iteration:  20% 745/3742 [04:07<16:13,  3.08it/s]\u001b[A\n",
            "Iteration:  20% 746/3742 [04:07<16:12,  3.08it/s]\u001b[A\n",
            "Iteration:  20% 747/3742 [04:07<16:11,  3.08it/s]\u001b[A\n",
            "Iteration:  20% 748/3742 [04:07<16:11,  3.08it/s]\u001b[A\n",
            "Iteration:  20% 749/3742 [04:08<16:11,  3.08it/s]\u001b[A\n",
            "Iteration:  20% 750/3742 [04:08<16:11,  3.08it/s]\u001b[A\n",
            "Iteration:  20% 751/3742 [04:08<16:11,  3.08it/s]\u001b[A\n",
            "Iteration:  20% 752/3742 [04:09<16:11,  3.08it/s]\u001b[A\n",
            "Iteration:  20% 753/3742 [04:09<16:13,  3.07it/s]\u001b[A\n",
            "Iteration:  20% 754/3742 [04:09<16:12,  3.07it/s]\u001b[A\n",
            "Iteration:  20% 755/3742 [04:10<16:13,  3.07it/s]\u001b[A\n",
            "Iteration:  20% 756/3742 [04:10<16:12,  3.07it/s]\u001b[A\n",
            "Iteration:  20% 757/3742 [04:10<16:10,  3.08it/s]\u001b[A\n",
            "Iteration:  20% 758/3742 [04:11<16:10,  3.07it/s]\u001b[A\n",
            "Iteration:  20% 759/3742 [04:11<16:09,  3.08it/s]\u001b[A\n",
            "Iteration:  20% 760/3742 [04:11<16:08,  3.08it/s]\u001b[A\n",
            "Iteration:  20% 761/3742 [04:12<16:07,  3.08it/s]\u001b[A\n",
            "Iteration:  20% 762/3742 [04:12<16:08,  3.08it/s]\u001b[A\n",
            "Iteration:  20% 763/3742 [04:12<16:11,  3.07it/s]\u001b[A\n",
            "Iteration:  20% 764/3742 [04:13<16:15,  3.05it/s]\u001b[A\n",
            "Iteration:  20% 765/3742 [04:13<16:17,  3.04it/s]\u001b[A\n",
            "Iteration:  20% 766/3742 [04:13<16:16,  3.05it/s]\u001b[A\n",
            "Iteration:  20% 767/3742 [04:14<16:12,  3.06it/s]\u001b[A\n",
            "Iteration:  21% 768/3742 [04:14<16:09,  3.07it/s]\u001b[A\n",
            "Iteration:  21% 769/3742 [04:14<16:08,  3.07it/s]\u001b[A\n",
            "Iteration:  21% 770/3742 [04:15<16:08,  3.07it/s]\u001b[A\n",
            "Iteration:  21% 771/3742 [04:15<16:06,  3.07it/s]\u001b[A\n",
            "Iteration:  21% 772/3742 [04:15<16:05,  3.08it/s]\u001b[A\n",
            "Iteration:  21% 773/3742 [04:16<16:04,  3.08it/s]\u001b[A\n",
            "Iteration:  21% 774/3742 [04:16<16:03,  3.08it/s]\u001b[A\n",
            "Iteration:  21% 775/3742 [04:16<16:04,  3.07it/s]\u001b[A\n",
            "Iteration:  21% 776/3742 [04:17<16:05,  3.07it/s]\u001b[A\n",
            "Iteration:  21% 777/3742 [04:17<16:04,  3.07it/s]\u001b[A\n",
            "Iteration:  21% 778/3742 [04:17<16:03,  3.08it/s]\u001b[A\n",
            "Iteration:  21% 779/3742 [04:18<16:03,  3.08it/s]\u001b[A\n",
            "Iteration:  21% 780/3742 [04:18<16:02,  3.08it/s]\u001b[A\n",
            "Iteration:  21% 781/3742 [04:18<16:01,  3.08it/s]\u001b[A\n",
            "Iteration:  21% 782/3742 [04:19<16:01,  3.08it/s]\u001b[A\n",
            "Iteration:  21% 783/3742 [04:19<16:03,  3.07it/s]\u001b[A\n",
            "Iteration:  21% 784/3742 [04:19<16:02,  3.07it/s]\u001b[A\n",
            "Iteration:  21% 785/3742 [04:20<16:02,  3.07it/s]\u001b[A\n",
            "Iteration:  21% 786/3742 [04:20<16:00,  3.08it/s]\u001b[A\n",
            "Iteration:  21% 787/3742 [04:20<16:00,  3.08it/s]\u001b[A\n",
            "Iteration:  21% 788/3742 [04:21<16:00,  3.07it/s]\u001b[A\n",
            "Iteration:  21% 789/3742 [04:21<16:00,  3.07it/s]\u001b[A\n",
            "Iteration:  21% 790/3742 [04:21<15:59,  3.08it/s]\u001b[A\n",
            "Iteration:  21% 791/3742 [04:21<15:59,  3.08it/s]\u001b[A\n",
            "Iteration:  21% 792/3742 [04:22<16:00,  3.07it/s]\u001b[A\n",
            "Iteration:  21% 793/3742 [04:22<16:00,  3.07it/s]\u001b[A\n",
            "Iteration:  21% 794/3742 [04:22<16:02,  3.06it/s]\u001b[A\n",
            "Iteration:  21% 795/3742 [04:23<15:59,  3.07it/s]\u001b[A\n",
            "Iteration:  21% 796/3742 [04:23<15:59,  3.07it/s]\u001b[A\n",
            "Iteration:  21% 797/3742 [04:23<16:00,  3.07it/s]\u001b[A\n",
            "Iteration:  21% 798/3742 [04:24<15:59,  3.07it/s]\u001b[A\n",
            "Iteration:  21% 799/3742 [04:24<15:57,  3.07it/s]\u001b[A\n",
            "Iteration:  21% 800/3742 [04:24<15:57,  3.07it/s]\u001b[A\n",
            "Iteration:  21% 801/3742 [04:25<15:56,  3.08it/s]\u001b[A\n",
            "Iteration:  21% 802/3742 [04:25<15:55,  3.08it/s]\u001b[A\n",
            "Iteration:  21% 803/3742 [04:25<15:54,  3.08it/s]\u001b[A\n",
            "Iteration:  21% 804/3742 [04:26<15:55,  3.07it/s]\u001b[A\n",
            "Iteration:  22% 805/3742 [04:26<15:54,  3.08it/s]\u001b[A\n",
            "Iteration:  22% 806/3742 [04:26<15:53,  3.08it/s]\u001b[A\n",
            "Iteration:  22% 807/3742 [04:27<15:54,  3.08it/s]\u001b[A\n",
            "Iteration:  22% 808/3742 [04:27<15:53,  3.08it/s]\u001b[A\n",
            "Iteration:  22% 809/3742 [04:27<15:54,  3.07it/s]\u001b[A\n",
            "Iteration:  22% 810/3742 [04:28<15:54,  3.07it/s]\u001b[A\n",
            "Iteration:  22% 811/3742 [04:28<15:53,  3.07it/s]\u001b[A\n",
            "Iteration:  22% 812/3742 [04:28<15:52,  3.08it/s]\u001b[A\n",
            "Iteration:  22% 813/3742 [04:29<15:52,  3.07it/s]\u001b[A\n",
            "Iteration:  22% 814/3742 [04:29<15:51,  3.08it/s]\u001b[A\n",
            "Iteration:  22% 815/3742 [04:29<15:50,  3.08it/s]\u001b[A\n",
            "Iteration:  22% 816/3742 [04:30<15:48,  3.08it/s]\u001b[A\n",
            "Iteration:  22% 817/3742 [04:30<15:49,  3.08it/s]\u001b[A\n",
            "Iteration:  22% 818/3742 [04:30<15:48,  3.08it/s]\u001b[A\n",
            "Iteration:  22% 819/3742 [04:31<15:50,  3.08it/s]\u001b[A\n",
            "Iteration:  22% 820/3742 [04:31<15:53,  3.06it/s]\u001b[A\n",
            "Iteration:  22% 821/3742 [04:31<15:52,  3.07it/s]\u001b[A\n",
            "Iteration:  22% 822/3742 [04:32<15:52,  3.07it/s]\u001b[A\n",
            "Iteration:  22% 823/3742 [04:32<15:50,  3.07it/s]\u001b[A\n",
            "Iteration:  22% 824/3742 [04:32<15:48,  3.08it/s]\u001b[A\n",
            "Iteration:  22% 825/3742 [04:33<15:47,  3.08it/s]\u001b[A\n",
            "Iteration:  22% 826/3742 [04:33<15:48,  3.08it/s]\u001b[A\n",
            "Iteration:  22% 827/3742 [04:33<15:47,  3.08it/s]\u001b[A\n",
            "Iteration:  22% 828/3742 [04:34<15:47,  3.08it/s]\u001b[A\n",
            "Iteration:  22% 829/3742 [04:34<15:48,  3.07it/s]\u001b[A\n",
            "Iteration:  22% 830/3742 [04:34<15:46,  3.08it/s]\u001b[A\n",
            "Iteration:  22% 831/3742 [04:35<15:46,  3.08it/s]\u001b[A\n",
            "Iteration:  22% 832/3742 [04:35<15:46,  3.08it/s]\u001b[A\n",
            "Iteration:  22% 833/3742 [04:35<15:44,  3.08it/s]\u001b[A\n",
            "Iteration:  22% 834/3742 [04:35<15:44,  3.08it/s]\u001b[A\n",
            "Iteration:  22% 835/3742 [04:36<15:43,  3.08it/s]\u001b[A\n",
            "Iteration:  22% 836/3742 [04:36<15:42,  3.08it/s]\u001b[A\n",
            "Iteration:  22% 837/3742 [04:36<15:46,  3.07it/s]\u001b[A\n",
            "Iteration:  22% 838/3742 [04:37<15:47,  3.06it/s]\u001b[A\n",
            "Iteration:  22% 839/3742 [04:37<15:47,  3.07it/s]\u001b[A\n",
            "Iteration:  22% 840/3742 [04:37<15:45,  3.07it/s]\u001b[A\n",
            "Iteration:  22% 841/3742 [04:38<15:44,  3.07it/s]\u001b[A\n",
            "Iteration:  23% 842/3742 [04:38<15:44,  3.07it/s]\u001b[A\n",
            "Iteration:  23% 843/3742 [04:38<15:44,  3.07it/s]\u001b[A\n",
            "Iteration:  23% 844/3742 [04:39<15:45,  3.07it/s]\u001b[A\n",
            "Iteration:  23% 845/3742 [04:39<15:44,  3.07it/s]\u001b[A\n",
            "Iteration:  23% 846/3742 [04:39<15:42,  3.07it/s]\u001b[A\n",
            "Iteration:  23% 847/3742 [04:40<15:40,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 848/3742 [04:40<15:40,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 849/3742 [04:40<15:39,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 850/3742 [04:41<15:38,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 851/3742 [04:41<15:37,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 852/3742 [04:41<15:37,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 853/3742 [04:42<15:37,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 854/3742 [04:42<15:36,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 855/3742 [04:42<15:37,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 856/3742 [04:43<15:36,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 857/3742 [04:43<15:36,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 858/3742 [04:43<15:35,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 859/3742 [04:44<15:35,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 860/3742 [04:44<15:34,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 861/3742 [04:44<15:36,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 862/3742 [04:45<15:35,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 863/3742 [04:45<15:34,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 864/3742 [04:45<15:33,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 865/3742 [04:46<15:33,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 866/3742 [04:46<15:34,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 867/3742 [04:46<15:35,  3.07it/s]\u001b[A\n",
            "Iteration:  23% 868/3742 [04:47<15:34,  3.07it/s]\u001b[A\n",
            "Iteration:  23% 869/3742 [04:47<15:34,  3.07it/s]\u001b[A\n",
            "Iteration:  23% 870/3742 [04:47<15:34,  3.07it/s]\u001b[A\n",
            "Iteration:  23% 871/3742 [04:48<15:32,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 872/3742 [04:48<15:31,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 873/3742 [04:48<15:30,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 874/3742 [04:48<15:30,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 875/3742 [04:49<15:29,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 876/3742 [04:49<15:30,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 877/3742 [04:49<15:29,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 878/3742 [04:50<15:29,  3.08it/s]\u001b[A\n",
            "Iteration:  23% 879/3742 [04:50<15:29,  3.08it/s]\u001b[A\n",
            "Iteration:  24% 880/3742 [04:50<15:28,  3.08it/s]\u001b[A\n",
            "Iteration:  24% 881/3742 [04:51<15:28,  3.08it/s]\u001b[A\n",
            "Iteration:  24% 882/3742 [04:51<15:29,  3.08it/s]\u001b[A\n",
            "Iteration:  24% 883/3742 [04:51<15:27,  3.08it/s]\u001b[A\n",
            "Iteration:  24% 884/3742 [04:52<15:27,  3.08it/s]\u001b[A\n",
            "Iteration:  24% 885/3742 [04:52<15:27,  3.08it/s]\u001b[A\n",
            "Iteration:  24% 886/3742 [04:52<15:26,  3.08it/s]\u001b[A\n",
            "Iteration:  24% 887/3742 [04:53<15:30,  3.07it/s]\u001b[A\n",
            "Iteration:  24% 888/3742 [04:53<15:37,  3.04it/s]\u001b[A\n",
            "Iteration:  24% 889/3742 [04:53<15:34,  3.05it/s]\u001b[A\n",
            "Iteration:  24% 890/3742 [04:54<15:31,  3.06it/s]\u001b[A\n",
            "Iteration:  24% 891/3742 [04:54<15:28,  3.07it/s]\u001b[A\n",
            "Iteration:  24% 892/3742 [04:54<15:27,  3.07it/s]\u001b[A\n",
            "Iteration:  24% 893/3742 [04:55<15:26,  3.08it/s]\u001b[A\n",
            "Iteration:  24% 894/3742 [04:55<15:25,  3.08it/s]\u001b[A\n",
            "Iteration:  24% 895/3742 [04:55<15:25,  3.08it/s]\u001b[A\n",
            "Iteration:  24% 896/3742 [04:56<15:25,  3.08it/s]\u001b[A\n",
            "Iteration:  24% 897/3742 [04:56<15:23,  3.08it/s]\u001b[A\n",
            "Iteration:  24% 898/3742 [04:56<15:25,  3.07it/s]\u001b[A\n",
            "Iteration:  24% 899/3742 [04:57<15:24,  3.08it/s]\u001b[A\n",
            "Iteration:  24% 900/3742 [04:57<15:24,  3.07it/s]\u001b[A\n",
            "Iteration:  24% 901/3742 [04:57<15:24,  3.07it/s]\u001b[A\n",
            "Iteration:  24% 902/3742 [04:58<15:23,  3.08it/s]\u001b[A\n",
            "Iteration:  24% 903/3742 [04:58<15:21,  3.08it/s]\u001b[A\n",
            "Iteration:  24% 904/3742 [04:58<15:21,  3.08it/s]\u001b[A\n",
            "Iteration:  24% 905/3742 [04:59<15:21,  3.08it/s]\u001b[A\n",
            "Iteration:  24% 906/3742 [04:59<15:21,  3.08it/s]\u001b[A\n",
            "Iteration:  24% 907/3742 [04:59<15:22,  3.07it/s]\u001b[A\n",
            "Iteration:  24% 908/3742 [05:00<15:22,  3.07it/s]\u001b[A\n",
            "Iteration:  24% 909/3742 [05:00<15:22,  3.07it/s]\u001b[A\n",
            "Iteration:  24% 910/3742 [05:00<15:22,  3.07it/s]\u001b[A\n",
            "Iteration:  24% 911/3742 [05:01<15:20,  3.07it/s]\u001b[A\n",
            "Iteration:  24% 912/3742 [05:01<15:20,  3.08it/s]\u001b[A\n",
            "Iteration:  24% 913/3742 [05:01<15:18,  3.08it/s]\u001b[A\n",
            "Iteration:  24% 914/3742 [05:01<15:18,  3.08it/s]\u001b[A\n",
            "Iteration:  24% 915/3742 [05:02<15:17,  3.08it/s]\u001b[A\n",
            "Iteration:  24% 916/3742 [05:02<15:16,  3.08it/s]\u001b[A\n",
            "Iteration:  25% 917/3742 [05:02<15:16,  3.08it/s]\u001b[A\n",
            "Iteration:  25% 918/3742 [05:03<15:15,  3.08it/s]\u001b[A\n",
            "Iteration:  25% 919/3742 [05:03<15:15,  3.08it/s]\u001b[A\n",
            "Iteration:  25% 920/3742 [05:03<15:16,  3.08it/s]\u001b[A\n",
            "Iteration:  25% 921/3742 [05:04<15:19,  3.07it/s]\u001b[A\n",
            "Iteration:  25% 922/3742 [05:04<15:18,  3.07it/s]\u001b[A\n",
            "Iteration:  25% 923/3742 [05:04<15:18,  3.07it/s]\u001b[A\n",
            "Iteration:  25% 924/3742 [05:05<15:16,  3.07it/s]\u001b[A\n",
            "Iteration:  25% 925/3742 [05:05<15:16,  3.07it/s]\u001b[A\n",
            "Iteration:  25% 926/3742 [05:05<15:15,  3.08it/s]\u001b[A\n",
            "Iteration:  25% 927/3742 [05:06<15:14,  3.08it/s]\u001b[A\n",
            "Iteration:  25% 928/3742 [05:06<15:13,  3.08it/s]\u001b[A\n",
            "Iteration:  25% 929/3742 [05:06<15:13,  3.08it/s]\u001b[A\n",
            "Iteration:  25% 930/3742 [05:07<15:13,  3.08it/s]\u001b[A\n",
            "Iteration:  25% 931/3742 [05:07<15:12,  3.08it/s]\u001b[A\n",
            "Iteration:  25% 932/3742 [05:07<15:14,  3.07it/s]\u001b[A\n",
            "Iteration:  25% 933/3742 [05:08<15:13,  3.08it/s]\u001b[A\n",
            "Iteration:  25% 934/3742 [05:08<15:13,  3.07it/s]\u001b[A\n",
            "Iteration:  25% 935/3742 [05:08<15:12,  3.08it/s]\u001b[A\n",
            "Iteration:  25% 936/3742 [05:09<15:11,  3.08it/s]\u001b[A\n",
            "Iteration:  25% 937/3742 [05:09<15:12,  3.08it/s]\u001b[A\n",
            "Iteration:  25% 938/3742 [05:09<15:12,  3.07it/s]\u001b[A\n",
            "Iteration:  25% 939/3742 [05:10<15:11,  3.07it/s]\u001b[A\n",
            "Iteration:  25% 940/3742 [05:10<15:10,  3.08it/s]\u001b[A\n",
            "Iteration:  25% 941/3742 [05:10<15:10,  3.08it/s]\u001b[A\n",
            "Iteration:  25% 942/3742 [05:11<15:09,  3.08it/s]\u001b[A\n",
            "Iteration:  25% 943/3742 [05:11<15:09,  3.08it/s]\u001b[A\n",
            "Iteration:  25% 944/3742 [05:11<15:08,  3.08it/s]\u001b[A\n",
            "Iteration:  25% 945/3742 [05:12<15:08,  3.08it/s]\u001b[A\n",
            "Iteration:  25% 946/3742 [05:12<15:07,  3.08it/s]\u001b[A\n",
            "Iteration:  25% 947/3742 [05:12<15:08,  3.08it/s]\u001b[A\n",
            "Iteration:  25% 948/3742 [05:13<15:11,  3.07it/s]\u001b[A\n",
            "Iteration:  25% 949/3742 [05:13<15:16,  3.05it/s]\u001b[A\n",
            "Iteration:  25% 950/3742 [05:13<15:17,  3.04it/s]\u001b[A\n",
            "Iteration:  25% 951/3742 [05:14<15:15,  3.05it/s]\u001b[A\n",
            "Iteration:  25% 952/3742 [05:14<15:11,  3.06it/s]\u001b[A\n",
            "Iteration:  25% 953/3742 [05:14<15:09,  3.07it/s]\u001b[A\n",
            "Iteration:  25% 954/3742 [05:14<15:08,  3.07it/s]\u001b[A\n",
            "Iteration:  26% 955/3742 [05:15<15:10,  3.06it/s]\u001b[A\n",
            "Iteration:  26% 956/3742 [05:15<15:12,  3.05it/s]\u001b[A\n",
            "Iteration:  26% 957/3742 [05:15<15:12,  3.05it/s]\u001b[A\n",
            "Iteration:  26% 958/3742 [05:16<15:10,  3.06it/s]\u001b[A\n",
            "Iteration:  26% 959/3742 [05:16<15:09,  3.06it/s]\u001b[A\n",
            "Iteration:  26% 960/3742 [05:16<15:06,  3.07it/s]\u001b[A\n",
            "Iteration:  26% 961/3742 [05:17<15:05,  3.07it/s]\u001b[A\n",
            "Iteration:  26% 962/3742 [05:17<15:03,  3.08it/s]\u001b[A\n",
            "Iteration:  26% 963/3742 [05:17<15:03,  3.08it/s]\u001b[A\n",
            "Iteration:  26% 964/3742 [05:18<15:03,  3.07it/s]\u001b[A\n",
            "Iteration:  26% 965/3742 [05:18<15:02,  3.08it/s]\u001b[A\n",
            "Iteration:  26% 966/3742 [05:18<15:01,  3.08it/s]\u001b[A\n",
            "Iteration:  26% 967/3742 [05:19<15:01,  3.08it/s]\u001b[A\n",
            "Iteration:  26% 968/3742 [05:19<15:01,  3.08it/s]\u001b[A\n",
            "Iteration:  26% 969/3742 [05:19<14:59,  3.08it/s]\u001b[A\n",
            "Iteration:  26% 970/3742 [05:20<15:00,  3.08it/s]\u001b[A\n",
            "Iteration:  26% 971/3742 [05:20<14:59,  3.08it/s]\u001b[A\n",
            "Iteration:  26% 972/3742 [05:20<15:00,  3.08it/s]\u001b[A\n",
            "Iteration:  26% 973/3742 [05:21<14:59,  3.08it/s]\u001b[A\n",
            "Iteration:  26% 974/3742 [05:21<14:58,  3.08it/s]\u001b[A\n",
            "Iteration:  26% 975/3742 [05:21<14:58,  3.08it/s]\u001b[A\n",
            "Iteration:  26% 976/3742 [05:22<14:57,  3.08it/s]\u001b[A\n",
            "Iteration:  26% 977/3742 [05:22<14:58,  3.08it/s]\u001b[A\n",
            "Iteration:  26% 978/3742 [05:22<14:57,  3.08it/s]\u001b[A\n",
            "Iteration:  26% 979/3742 [05:23<15:00,  3.07it/s]\u001b[A\n",
            "Iteration:  26% 980/3742 [05:23<15:00,  3.07it/s]\u001b[A\n",
            "Iteration:  26% 981/3742 [05:23<14:59,  3.07it/s]\u001b[A\n",
            "Iteration:  26% 982/3742 [05:24<14:57,  3.08it/s]\u001b[A\n",
            "Iteration:  26% 983/3742 [05:24<14:56,  3.08it/s]\u001b[A\n",
            "Iteration:  26% 984/3742 [05:24<14:55,  3.08it/s]\u001b[A\n",
            "Iteration:  26% 985/3742 [05:25<14:55,  3.08it/s]\u001b[A\n",
            "Iteration:  26% 986/3742 [05:25<14:54,  3.08it/s]\u001b[A\n",
            "Iteration:  26% 987/3742 [05:25<14:54,  3.08it/s]\u001b[A\n",
            "Iteration:  26% 988/3742 [05:26<14:53,  3.08it/s]\u001b[A\n",
            "Iteration:  26% 989/3742 [05:26<14:53,  3.08it/s]\u001b[A\n",
            "Iteration:  26% 990/3742 [05:26<14:53,  3.08it/s]\u001b[A\n",
            "Iteration:  26% 991/3742 [05:27<14:54,  3.08it/s]\u001b[A\n",
            "Iteration:  27% 992/3742 [05:27<14:52,  3.08it/s]\u001b[A\n",
            "Iteration:  27% 993/3742 [05:27<14:53,  3.08it/s]\u001b[A\n",
            "Iteration:  27% 994/3742 [05:28<14:53,  3.07it/s]\u001b[A\n",
            "Iteration:  27% 995/3742 [05:28<14:53,  3.07it/s]\u001b[A\n",
            "Iteration:  27% 996/3742 [05:28<14:52,  3.08it/s]\u001b[A\n",
            "Iteration:  27% 997/3742 [05:28<14:55,  3.07it/s]\u001b[A\n",
            "Iteration:  27% 998/3742 [05:29<14:56,  3.06it/s]\u001b[A\n",
            "Iteration:  27% 999/3742 [05:29<14:53,  3.07it/s]\u001b[A03/05/2020 06:56:14 - INFO - transformers.configuration_utils -   Configuration saved in output_roberta_US/checkpoint-1000/config.json\n",
            "03/05/2020 06:56:15 - INFO - transformers.modeling_utils -   Model weights saved in output_roberta_US/checkpoint-1000/pytorch_model.bin\n",
            "03/05/2020 06:56:15 - INFO - __main__ -   Saving model checkpoint to output_roberta_US/checkpoint-1000\n",
            "03/05/2020 06:56:19 - INFO - __main__ -   Saving optimizer and scheduler states to output_roberta_US/checkpoint-1000\n",
            "\n",
            "Iteration:  27% 1000/3742 [05:34<1:18:27,  1.72s/it]\u001b[A\n",
            "Iteration:  27% 1001/3742 [05:34<59:32,  1.30s/it]  \u001b[A\n",
            "Iteration:  27% 1002/3742 [05:35<46:08,  1.01s/it]\u001b[A\n",
            "Iteration:  27% 1003/3742 [05:35<36:44,  1.24it/s]\u001b[A\n",
            "Iteration:  27% 1004/3742 [05:35<30:13,  1.51it/s]\u001b[A\n",
            "Iteration:  27% 1005/3742 [05:36<25:36,  1.78it/s]\u001b[A\n",
            "Iteration:  27% 1006/3742 [05:36<22:22,  2.04it/s]\u001b[A\n",
            "Iteration:  27% 1007/3742 [05:36<20:08,  2.26it/s]\u001b[A\n",
            "Iteration:  27% 1008/3742 [05:37<18:36,  2.45it/s]\u001b[A\n",
            "Iteration:  27% 1009/3742 [05:37<17:28,  2.61it/s]\u001b[A\n",
            "Iteration:  27% 1010/3742 [05:37<16:39,  2.73it/s]\u001b[A\n",
            "Iteration:  27% 1011/3742 [05:38<16:08,  2.82it/s]\u001b[A\n",
            "Iteration:  27% 1012/3742 [05:38<15:43,  2.89it/s]\u001b[A\n",
            "Iteration:  27% 1013/3742 [05:38<15:26,  2.94it/s]\u001b[A\n",
            "Iteration:  27% 1014/3742 [05:39<15:15,  2.98it/s]\u001b[A\n",
            "Iteration:  27% 1015/3742 [05:39<15:06,  3.01it/s]\u001b[A\n",
            "Iteration:  27% 1016/3742 [05:39<15:00,  3.03it/s]\u001b[A\n",
            "Iteration:  27% 1017/3742 [05:40<14:59,  3.03it/s]\u001b[A\n",
            "Iteration:  27% 1018/3742 [05:40<14:56,  3.04it/s]\u001b[A\n",
            "Iteration:  27% 1019/3742 [05:40<14:52,  3.05it/s]\u001b[A\n",
            "Iteration:  27% 1020/3742 [05:41<14:50,  3.06it/s]\u001b[A\n",
            "Iteration:  27% 1021/3742 [05:41<14:48,  3.06it/s]\u001b[A\n",
            "Iteration:  27% 1022/3742 [05:41<14:46,  3.07it/s]\u001b[A\n",
            "Iteration:  27% 1023/3742 [05:42<14:45,  3.07it/s]\u001b[A\n",
            "Iteration:  27% 1024/3742 [05:42<14:45,  3.07it/s]\u001b[A\n",
            "Iteration:  27% 1025/3742 [05:42<14:43,  3.07it/s]\u001b[A\n",
            "Iteration:  27% 1026/3742 [05:43<14:43,  3.08it/s]\u001b[A\n",
            "Iteration:  27% 1027/3742 [05:43<14:42,  3.08it/s]\u001b[A\n",
            "Iteration:  27% 1028/3742 [05:43<14:40,  3.08it/s]\u001b[A\n",
            "Iteration:  27% 1029/3742 [05:44<14:39,  3.08it/s]\u001b[A\n",
            "Iteration:  28% 1030/3742 [05:44<14:40,  3.08it/s]\u001b[A\n",
            "Iteration:  28% 1031/3742 [05:44<14:40,  3.08it/s]\u001b[A\n",
            "Iteration:  28% 1032/3742 [05:45<14:39,  3.08it/s]\u001b[A\n",
            "Iteration:  28% 1033/3742 [05:45<14:38,  3.08it/s]\u001b[A\n",
            "Iteration:  28% 1034/3742 [05:45<14:40,  3.08it/s]\u001b[A\n",
            "Iteration:  28% 1035/3742 [05:46<14:39,  3.08it/s]\u001b[A\n",
            "Iteration:  28% 1036/3742 [05:46<14:38,  3.08it/s]\u001b[A\n",
            "Iteration:  28% 1037/3742 [05:46<14:38,  3.08it/s]\u001b[A\n",
            "Iteration:  28% 1038/3742 [05:46<14:36,  3.08it/s]\u001b[A\n",
            "Iteration:  28% 1039/3742 [05:47<14:37,  3.08it/s]\u001b[A\n",
            "Iteration:  28% 1040/3742 [05:47<14:36,  3.08it/s]\u001b[A\n",
            "Iteration:  28% 1041/3742 [05:47<14:36,  3.08it/s]\u001b[A\n",
            "Iteration:  28% 1042/3742 [05:48<14:36,  3.08it/s]\u001b[A\n",
            "Iteration:  28% 1043/3742 [05:48<14:36,  3.08it/s]\u001b[A\n",
            "Iteration:  28% 1044/3742 [05:48<14:36,  3.08it/s]\u001b[A\n",
            "Iteration:  28% 1045/3742 [05:49<14:36,  3.08it/s]\u001b[A\n",
            "Iteration:  28% 1046/3742 [05:49<14:35,  3.08it/s]\u001b[A\n",
            "Iteration:  28% 1047/3742 [05:49<14:36,  3.08it/s]\u001b[A\n",
            "Iteration:  28% 1048/3742 [05:50<14:39,  3.06it/s]\u001b[A\n",
            "Iteration:  28% 1049/3742 [05:50<14:38,  3.07it/s]\u001b[A\n",
            "Iteration:  28% 1050/3742 [05:50<14:36,  3.07it/s]\u001b[A\n",
            "Iteration:  28% 1051/3742 [05:51<14:35,  3.08it/s]\u001b[A\n",
            "Iteration:  28% 1052/3742 [05:51<14:35,  3.07it/s]\u001b[A\n",
            "Iteration:  28% 1053/3742 [05:51<14:36,  3.07it/s]\u001b[A\n",
            "Iteration:  28% 1054/3742 [05:52<14:35,  3.07it/s]\u001b[A\n",
            "Iteration:  28% 1055/3742 [05:52<14:34,  3.07it/s]\u001b[A\n",
            "Iteration:  28% 1056/3742 [05:52<14:34,  3.07it/s]\u001b[A\n",
            "Iteration:  28% 1057/3742 [05:53<14:37,  3.06it/s]\u001b[A\n",
            "Iteration:  28% 1058/3742 [05:53<14:39,  3.05it/s]\u001b[A\n",
            "Iteration:  28% 1059/3742 [05:53<14:40,  3.05it/s]\u001b[A\n",
            "Iteration:  28% 1060/3742 [05:54<14:39,  3.05it/s]\u001b[A\n",
            "Iteration:  28% 1061/3742 [05:54<14:37,  3.06it/s]\u001b[A\n",
            "Iteration:  28% 1062/3742 [05:54<14:35,  3.06it/s]\u001b[A\n",
            "Iteration:  28% 1063/3742 [05:55<14:33,  3.07it/s]\u001b[A\n",
            "Iteration:  28% 1064/3742 [05:55<14:32,  3.07it/s]\u001b[A\n",
            "Iteration:  28% 1065/3742 [05:55<14:32,  3.07it/s]\u001b[A\n",
            "Iteration:  28% 1066/3742 [05:56<14:31,  3.07it/s]\u001b[A\n",
            "Iteration:  29% 1067/3742 [05:56<14:30,  3.07it/s]\u001b[A\n",
            "Iteration:  29% 1068/3742 [05:56<14:32,  3.06it/s]\u001b[A\n",
            "Iteration:  29% 1069/3742 [05:57<14:33,  3.06it/s]\u001b[A\n",
            "Iteration:  29% 1070/3742 [05:57<14:32,  3.06it/s]\u001b[A\n",
            "Iteration:  29% 1071/3742 [05:57<14:32,  3.06it/s]\u001b[A\n",
            "Iteration:  29% 1072/3742 [05:58<14:30,  3.07it/s]\u001b[A\n",
            "Iteration:  29% 1073/3742 [05:58<14:29,  3.07it/s]\u001b[A\n",
            "Iteration:  29% 1074/3742 [05:58<14:29,  3.07it/s]\u001b[A\n",
            "Iteration:  29% 1075/3742 [05:59<14:29,  3.07it/s]\u001b[A\n",
            "Iteration:  29% 1076/3742 [05:59<14:28,  3.07it/s]\u001b[A\n",
            "Iteration:  29% 1077/3742 [05:59<14:28,  3.07it/s]\u001b[A\n",
            "Iteration:  29% 1078/3742 [06:00<14:28,  3.07it/s]\u001b[A\n",
            "Iteration:  29% 1079/3742 [06:00<14:28,  3.07it/s]\u001b[A\n",
            "Iteration:  29% 1080/3742 [06:00<14:26,  3.07it/s]\u001b[A\n",
            "Iteration:  29% 1081/3742 [06:00<14:25,  3.08it/s]\u001b[A\n",
            "Iteration:  29% 1082/3742 [06:01<14:26,  3.07it/s]\u001b[A\n",
            "Iteration:  29% 1083/3742 [06:01<14:25,  3.07it/s]\u001b[A\n",
            "Iteration:  29% 1084/3742 [06:01<14:23,  3.08it/s]\u001b[A\n",
            "Iteration:  29% 1085/3742 [06:02<14:23,  3.08it/s]\u001b[A\n",
            "Iteration:  29% 1086/3742 [06:02<14:22,  3.08it/s]\u001b[A\n",
            "Iteration:  29% 1087/3742 [06:02<14:21,  3.08it/s]\u001b[A\n",
            "Iteration:  29% 1088/3742 [06:03<14:20,  3.08it/s]\u001b[A\n",
            "Iteration:  29% 1089/3742 [06:03<14:20,  3.08it/s]\u001b[A\n",
            "Iteration:  29% 1090/3742 [06:03<14:20,  3.08it/s]\u001b[A\n",
            "Iteration:  29% 1091/3742 [06:04<14:20,  3.08it/s]\u001b[A\n",
            "Iteration:  29% 1092/3742 [06:04<14:19,  3.08it/s]\u001b[A\n",
            "Iteration:  29% 1093/3742 [06:04<14:19,  3.08it/s]\u001b[A\n",
            "Iteration:  29% 1094/3742 [06:05<14:18,  3.08it/s]\u001b[A\n",
            "Iteration:  29% 1095/3742 [06:05<14:20,  3.08it/s]\u001b[A\n",
            "Iteration:  29% 1096/3742 [06:05<14:19,  3.08it/s]\u001b[A\n",
            "Iteration:  29% 1097/3742 [06:06<14:18,  3.08it/s]\u001b[A\n",
            "Iteration:  29% 1098/3742 [06:06<14:17,  3.08it/s]\u001b[A\n",
            "Iteration:  29% 1099/3742 [06:06<14:17,  3.08it/s]\u001b[A\n",
            "Iteration:  29% 1100/3742 [06:07<14:16,  3.08it/s]\u001b[A\n",
            "Iteration:  29% 1101/3742 [06:07<14:16,  3.08it/s]\u001b[A\n",
            "Iteration:  29% 1102/3742 [06:07<14:15,  3.09it/s]\u001b[A\n",
            "Iteration:  29% 1103/3742 [06:08<14:15,  3.09it/s]\u001b[A\n",
            "Iteration:  30% 1104/3742 [06:08<14:14,  3.09it/s]\u001b[A\n",
            "Iteration:  30% 1105/3742 [06:08<14:14,  3.08it/s]\u001b[A\n",
            "Iteration:  30% 1106/3742 [06:09<14:13,  3.09it/s]\u001b[A\n",
            "Iteration:  30% 1107/3742 [06:09<14:14,  3.09it/s]\u001b[A\n",
            "Iteration:  30% 1108/3742 [06:09<14:15,  3.08it/s]\u001b[A\n",
            "Iteration:  30% 1109/3742 [06:10<14:15,  3.08it/s]\u001b[A\n",
            "Iteration:  30% 1110/3742 [06:10<14:14,  3.08it/s]\u001b[A\n",
            "Iteration:  30% 1111/3742 [06:10<14:13,  3.08it/s]\u001b[A\n",
            "Iteration:  30% 1112/3742 [06:11<14:13,  3.08it/s]\u001b[A\n",
            "Iteration:  30% 1113/3742 [06:11<14:13,  3.08it/s]\u001b[A\n",
            "Iteration:  30% 1114/3742 [06:11<14:13,  3.08it/s]\u001b[A\n",
            "Iteration:  30% 1115/3742 [06:12<14:12,  3.08it/s]\u001b[A\n",
            "Iteration:  30% 1116/3742 [06:12<14:11,  3.08it/s]\u001b[A\n",
            "Iteration:  30% 1117/3742 [06:12<14:12,  3.08it/s]\u001b[A\n",
            "Iteration:  30% 1118/3742 [06:13<14:17,  3.06it/s]\u001b[A\n",
            "Iteration:  30% 1119/3742 [06:13<14:19,  3.05it/s]\u001b[A\n",
            "Iteration:  30% 1120/3742 [06:13<14:18,  3.05it/s]\u001b[A\n",
            "Iteration:  30% 1121/3742 [06:13<14:15,  3.06it/s]\u001b[A\n",
            "Iteration:  30% 1122/3742 [06:14<14:13,  3.07it/s]\u001b[A\n",
            "Iteration:  30% 1123/3742 [06:14<14:12,  3.07it/s]\u001b[A\n",
            "Iteration:  30% 1124/3742 [06:14<14:11,  3.08it/s]\u001b[A\n",
            "Iteration:  30% 1125/3742 [06:15<14:10,  3.08it/s]\u001b[A\n",
            "Iteration:  30% 1126/3742 [06:15<14:10,  3.08it/s]\u001b[A\n",
            "Iteration:  30% 1127/3742 [06:15<14:10,  3.07it/s]\u001b[A\n",
            "Iteration:  30% 1128/3742 [06:16<14:09,  3.08it/s]\u001b[A\n",
            "Iteration:  30% 1129/3742 [06:16<14:08,  3.08it/s]\u001b[A\n",
            "Iteration:  30% 1130/3742 [06:16<14:08,  3.08it/s]\u001b[A\n",
            "Iteration:  30% 1131/3742 [06:17<14:08,  3.08it/s]\u001b[A\n",
            "Iteration:  30% 1132/3742 [06:17<14:06,  3.08it/s]\u001b[A\n",
            "Iteration:  30% 1133/3742 [06:17<14:06,  3.08it/s]\u001b[A\n",
            "Iteration:  30% 1134/3742 [06:18<14:10,  3.07it/s]\u001b[A\n",
            "Iteration:  30% 1135/3742 [06:18<14:09,  3.07it/s]\u001b[A\n",
            "Iteration:  30% 1136/3742 [06:18<14:07,  3.07it/s]\u001b[A\n",
            "Iteration:  30% 1137/3742 [06:19<14:06,  3.08it/s]\u001b[A\n",
            "Iteration:  30% 1138/3742 [06:19<14:08,  3.07it/s]\u001b[A\n",
            "Iteration:  30% 1139/3742 [06:19<14:08,  3.07it/s]\u001b[A\n",
            "Iteration:  30% 1140/3742 [06:20<14:07,  3.07it/s]\u001b[A\n",
            "Iteration:  30% 1141/3742 [06:20<14:06,  3.07it/s]\u001b[A\n",
            "Iteration:  31% 1142/3742 [06:20<14:05,  3.08it/s]\u001b[A\n",
            "Iteration:  31% 1143/3742 [06:21<14:05,  3.07it/s]\u001b[A\n",
            "Iteration:  31% 1144/3742 [06:21<14:04,  3.08it/s]\u001b[A\n",
            "Iteration:  31% 1145/3742 [06:21<14:03,  3.08it/s]\u001b[A\n",
            "Iteration:  31% 1146/3742 [06:22<14:02,  3.08it/s]\u001b[A\n",
            "Iteration:  31% 1147/3742 [06:22<14:03,  3.08it/s]\u001b[A\n",
            "Iteration:  31% 1148/3742 [06:22<14:03,  3.08it/s]\u001b[A\n",
            "Iteration:  31% 1149/3742 [06:23<14:02,  3.08it/s]\u001b[A\n",
            "Iteration:  31% 1150/3742 [06:23<14:01,  3.08it/s]\u001b[A\n",
            "Iteration:  31% 1151/3742 [06:23<14:02,  3.08it/s]\u001b[A\n",
            "Iteration:  31% 1152/3742 [06:24<14:01,  3.08it/s]\u001b[A\n",
            "Iteration:  31% 1153/3742 [06:24<14:00,  3.08it/s]\u001b[A\n",
            "Iteration:  31% 1154/3742 [06:24<13:59,  3.08it/s]\u001b[A\n",
            "Iteration:  31% 1155/3742 [06:25<13:59,  3.08it/s]\u001b[A\n",
            "Iteration:  31% 1156/3742 [06:25<13:58,  3.08it/s]\u001b[A\n",
            "Iteration:  31% 1157/3742 [06:25<13:58,  3.08it/s]\u001b[A\n",
            "Iteration:  31% 1158/3742 [06:26<13:58,  3.08it/s]\u001b[A\n",
            "Iteration:  31% 1159/3742 [06:26<13:58,  3.08it/s]\u001b[A\n",
            "Iteration:  31% 1160/3742 [06:26<13:58,  3.08it/s]\u001b[A\n",
            "Iteration:  31% 1161/3742 [06:26<14:01,  3.07it/s]\u001b[A\n",
            "Iteration:  31% 1162/3742 [06:27<14:01,  3.07it/s]\u001b[A\n",
            "Iteration:  31% 1163/3742 [06:27<13:59,  3.07it/s]\u001b[A\n",
            "Iteration:  31% 1164/3742 [06:27<14:03,  3.06it/s]\u001b[A\n",
            "Iteration:  31% 1165/3742 [06:28<14:03,  3.06it/s]\u001b[A\n",
            "Iteration:  31% 1166/3742 [06:28<14:00,  3.06it/s]\u001b[A\n",
            "Iteration:  31% 1167/3742 [06:28<14:01,  3.06it/s]\u001b[A\n",
            "Iteration:  31% 1168/3742 [06:29<14:00,  3.06it/s]\u001b[A\n",
            "Iteration:  31% 1169/3742 [06:29<13:58,  3.07it/s]\u001b[A\n",
            "Iteration:  31% 1170/3742 [06:29<13:57,  3.07it/s]\u001b[A\n",
            "Iteration:  31% 1171/3742 [06:30<13:55,  3.08it/s]\u001b[A\n",
            "Iteration:  31% 1172/3742 [06:30<13:56,  3.07it/s]\u001b[A\n",
            "Iteration:  31% 1173/3742 [06:30<13:56,  3.07it/s]\u001b[A\n",
            "Iteration:  31% 1174/3742 [06:31<13:55,  3.07it/s]\u001b[A\n",
            "Iteration:  31% 1175/3742 [06:31<13:55,  3.07it/s]\u001b[A\n",
            "Iteration:  31% 1176/3742 [06:31<13:53,  3.08it/s]\u001b[A\n",
            "Iteration:  31% 1177/3742 [06:32<13:53,  3.08it/s]\u001b[A\n",
            "Iteration:  31% 1178/3742 [06:32<13:52,  3.08it/s]\u001b[A\n",
            "Iteration:  32% 1179/3742 [06:32<13:51,  3.08it/s]\u001b[A\n",
            "Iteration:  32% 1180/3742 [06:33<13:51,  3.08it/s]\u001b[A\n",
            "Iteration:  32% 1181/3742 [06:33<13:51,  3.08it/s]\u001b[A\n",
            "Iteration:  32% 1182/3742 [06:33<13:50,  3.08it/s]\u001b[A\n",
            "Iteration:  32% 1183/3742 [06:34<13:51,  3.08it/s]\u001b[A\n",
            "Iteration:  32% 1184/3742 [06:34<13:50,  3.08it/s]\u001b[A\n",
            "Iteration:  32% 1185/3742 [06:34<13:49,  3.08it/s]\u001b[A\n",
            "Iteration:  32% 1186/3742 [06:35<13:51,  3.08it/s]\u001b[A\n",
            "Iteration:  32% 1187/3742 [06:35<13:51,  3.07it/s]\u001b[A\n",
            "Iteration:  32% 1188/3742 [06:35<13:49,  3.08it/s]\u001b[A\n",
            "Iteration:  32% 1189/3742 [06:36<13:49,  3.08it/s]\u001b[A\n",
            "Iteration:  32% 1190/3742 [06:36<13:48,  3.08it/s]\u001b[A\n",
            "Iteration:  32% 1191/3742 [06:36<13:50,  3.07it/s]\u001b[A\n",
            "Iteration:  32% 1192/3742 [06:37<13:54,  3.06it/s]\u001b[A\n",
            "Iteration:  32% 1193/3742 [06:37<13:58,  3.04it/s]\u001b[A\n",
            "Iteration:  32% 1194/3742 [06:37<13:55,  3.05it/s]\u001b[A\n",
            "Iteration:  32% 1195/3742 [06:38<13:52,  3.06it/s]\u001b[A\n",
            "Iteration:  32% 1196/3742 [06:38<13:51,  3.06it/s]\u001b[A\n",
            "Iteration:  32% 1197/3742 [06:38<13:51,  3.06it/s]\u001b[A\n",
            "Iteration:  32% 1198/3742 [06:39<13:49,  3.07it/s]\u001b[A\n",
            "Iteration:  32% 1199/3742 [06:39<13:49,  3.07it/s]\u001b[A\n",
            "Iteration:  32% 1200/3742 [06:39<13:47,  3.07it/s]\u001b[A\n",
            "Iteration:  32% 1201/3742 [06:40<13:46,  3.07it/s]\u001b[A\n",
            "Iteration:  32% 1202/3742 [06:40<13:45,  3.08it/s]\u001b[A\n",
            "Iteration:  32% 1203/3742 [06:40<13:44,  3.08it/s]\u001b[A\n",
            "Iteration:  32% 1204/3742 [06:40<13:44,  3.08it/s]\u001b[A\n",
            "Iteration:  32% 1205/3742 [06:41<13:44,  3.08it/s]\u001b[A\n",
            "Iteration:  32% 1206/3742 [06:41<13:45,  3.07it/s]\u001b[A\n",
            "Iteration:  32% 1207/3742 [06:41<13:44,  3.08it/s]\u001b[A\n",
            "Iteration:  32% 1208/3742 [06:42<13:43,  3.08it/s]\u001b[A\n",
            "Iteration:  32% 1209/3742 [06:42<13:44,  3.07it/s]\u001b[A\n",
            "Iteration:  32% 1210/3742 [06:42<13:43,  3.08it/s]\u001b[A\n",
            "Iteration:  32% 1211/3742 [06:43<13:41,  3.08it/s]\u001b[A\n",
            "Iteration:  32% 1212/3742 [06:43<13:41,  3.08it/s]\u001b[A\n",
            "Iteration:  32% 1213/3742 [06:43<13:41,  3.08it/s]\u001b[A\n",
            "Iteration:  32% 1214/3742 [06:44<13:40,  3.08it/s]\u001b[A\n",
            "Iteration:  32% 1215/3742 [06:44<13:40,  3.08it/s]\u001b[A\n",
            "Iteration:  32% 1216/3742 [06:44<13:40,  3.08it/s]\u001b[A\n",
            "Iteration:  33% 1217/3742 [06:45<13:39,  3.08it/s]\u001b[A\n",
            "Iteration:  33% 1218/3742 [06:45<13:40,  3.08it/s]\u001b[A\n",
            "Iteration:  33% 1219/3742 [06:45<13:40,  3.07it/s]\u001b[A\n",
            "Iteration:  33% 1220/3742 [06:46<13:39,  3.08it/s]\u001b[A\n",
            "Iteration:  33% 1221/3742 [06:46<13:39,  3.08it/s]\u001b[A\n",
            "Iteration:  33% 1222/3742 [06:46<13:38,  3.08it/s]\u001b[A\n",
            "Iteration:  33% 1223/3742 [06:47<13:38,  3.08it/s]\u001b[A\n",
            "Iteration:  33% 1224/3742 [06:47<13:37,  3.08it/s]\u001b[A\n",
            "Iteration:  33% 1225/3742 [06:47<13:37,  3.08it/s]\u001b[A\n",
            "Iteration:  33% 1226/3742 [06:48<13:41,  3.06it/s]\u001b[A\n",
            "Iteration:  33% 1227/3742 [06:48<13:42,  3.06it/s]\u001b[A\n",
            "Iteration:  33% 1228/3742 [06:48<13:40,  3.06it/s]\u001b[A\n",
            "Iteration:  33% 1229/3742 [06:49<13:38,  3.07it/s]\u001b[A\n",
            "Iteration:  33% 1230/3742 [06:49<13:41,  3.06it/s]\u001b[A\n",
            "Iteration:  33% 1231/3742 [06:49<13:40,  3.06it/s]\u001b[A\n",
            "Iteration:  33% 1232/3742 [06:50<13:38,  3.07it/s]\u001b[A\n",
            "Iteration:  33% 1233/3742 [06:50<13:37,  3.07it/s]\u001b[A\n",
            "Iteration:  33% 1234/3742 [06:50<13:36,  3.07it/s]\u001b[A\n",
            "Iteration:  33% 1235/3742 [06:51<13:36,  3.07it/s]\u001b[A\n",
            "Iteration:  33% 1236/3742 [06:51<13:36,  3.07it/s]\u001b[A\n",
            "Iteration:  33% 1237/3742 [06:51<13:37,  3.06it/s]\u001b[A\n",
            "Iteration:  33% 1238/3742 [06:52<13:37,  3.06it/s]\u001b[A\n",
            "Iteration:  33% 1239/3742 [06:52<13:35,  3.07it/s]\u001b[A\n",
            "Iteration:  33% 1240/3742 [06:52<13:34,  3.07it/s]\u001b[A\n",
            "Iteration:  33% 1241/3742 [06:53<13:37,  3.06it/s]\u001b[A\n",
            "Iteration:  33% 1242/3742 [06:53<13:40,  3.05it/s]\u001b[A\n",
            "Iteration:  33% 1243/3742 [06:53<13:40,  3.04it/s]\u001b[A\n",
            "Iteration:  33% 1244/3742 [06:54<13:38,  3.05it/s]\u001b[A\n",
            "Iteration:  33% 1245/3742 [06:54<13:36,  3.06it/s]\u001b[A\n",
            "Iteration:  33% 1246/3742 [06:54<13:33,  3.07it/s]\u001b[A\n",
            "Iteration:  33% 1247/3742 [06:55<13:32,  3.07it/s]\u001b[A\n",
            "Iteration:  33% 1248/3742 [06:55<13:33,  3.07it/s]\u001b[A\n",
            "Iteration:  33% 1249/3742 [06:55<13:32,  3.07it/s]\u001b[A\n",
            "Iteration:  33% 1250/3742 [06:55<13:30,  3.07it/s]\u001b[A\n",
            "Iteration:  33% 1251/3742 [06:56<13:30,  3.07it/s]\u001b[A\n",
            "Iteration:  33% 1252/3742 [06:56<13:30,  3.07it/s]\u001b[A\n",
            "Iteration:  33% 1253/3742 [06:56<13:29,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1254/3742 [06:57<13:28,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1255/3742 [06:57<13:27,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1256/3742 [06:57<13:27,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1257/3742 [06:58<13:27,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1258/3742 [06:58<13:26,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1259/3742 [06:58<13:25,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1260/3742 [06:59<13:25,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1261/3742 [06:59<13:25,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1262/3742 [06:59<13:25,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1263/3742 [07:00<13:25,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1264/3742 [07:00<13:24,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1265/3742 [07:00<13:24,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1266/3742 [07:01<13:23,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1267/3742 [07:01<13:22,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1268/3742 [07:01<13:25,  3.07it/s]\u001b[A\n",
            "Iteration:  34% 1269/3742 [07:02<13:24,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1270/3742 [07:02<13:25,  3.07it/s]\u001b[A\n",
            "Iteration:  34% 1271/3742 [07:02<13:24,  3.07it/s]\u001b[A\n",
            "Iteration:  34% 1272/3742 [07:03<13:23,  3.07it/s]\u001b[A\n",
            "Iteration:  34% 1273/3742 [07:03<13:22,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1274/3742 [07:03<13:21,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1275/3742 [07:04<13:21,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1276/3742 [07:04<13:20,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1277/3742 [07:04<13:21,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1278/3742 [07:05<13:22,  3.07it/s]\u001b[A\n",
            "Iteration:  34% 1279/3742 [07:05<13:21,  3.07it/s]\u001b[A\n",
            "Iteration:  34% 1280/3742 [07:05<13:20,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1281/3742 [07:06<13:19,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1282/3742 [07:06<13:18,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1283/3742 [07:06<13:18,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1284/3742 [07:07<13:17,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1285/3742 [07:07<13:17,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1286/3742 [07:07<13:17,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1287/3742 [07:08<13:17,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1288/3742 [07:08<13:16,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1289/3742 [07:08<13:15,  3.08it/s]\u001b[A\n",
            "Iteration:  34% 1290/3742 [07:08<13:16,  3.08it/s]\u001b[A\n",
            "Iteration:  35% 1291/3742 [07:09<13:15,  3.08it/s]\u001b[A\n",
            "Iteration:  35% 1292/3742 [07:09<13:14,  3.08it/s]\u001b[A\n",
            "Iteration:  35% 1293/3742 [07:09<13:14,  3.08it/s]\u001b[A\n",
            "Iteration:  35% 1294/3742 [07:10<13:13,  3.08it/s]\u001b[A\n",
            "Iteration:  35% 1295/3742 [07:10<13:13,  3.09it/s]\u001b[A\n",
            "Iteration:  35% 1296/3742 [07:10<13:12,  3.08it/s]\u001b[A\n",
            "Iteration:  35% 1297/3742 [07:11<13:12,  3.09it/s]\u001b[A\n",
            "Iteration:  35% 1298/3742 [07:11<13:12,  3.08it/s]\u001b[A\n",
            "Iteration:  35% 1299/3742 [07:11<13:13,  3.08it/s]\u001b[A\n",
            "Iteration:  35% 1300/3742 [07:12<13:13,  3.08it/s]\u001b[A\n",
            "Iteration:  35% 1301/3742 [07:12<13:13,  3.08it/s]\u001b[A\n",
            "Iteration:  35% 1302/3742 [07:12<13:15,  3.07it/s]\u001b[A\n",
            "Iteration:  35% 1303/3742 [07:13<13:19,  3.05it/s]\u001b[A\n",
            "Iteration:  35% 1304/3742 [07:13<13:20,  3.04it/s]\u001b[A\n",
            "Iteration:  35% 1305/3742 [07:13<13:18,  3.05it/s]\u001b[A\n",
            "Iteration:  35% 1306/3742 [07:14<13:15,  3.06it/s]\u001b[A\n",
            "Iteration:  35% 1307/3742 [07:14<13:13,  3.07it/s]\u001b[A\n",
            "Iteration:  35% 1308/3742 [07:14<13:13,  3.07it/s]\u001b[A\n",
            "Iteration:  35% 1309/3742 [07:15<13:11,  3.07it/s]\u001b[A\n",
            "Iteration:  35% 1310/3742 [07:15<13:10,  3.08it/s]\u001b[A\n",
            "Iteration:  35% 1311/3742 [07:15<13:09,  3.08it/s]\u001b[A\n",
            "Iteration:  35% 1312/3742 [07:16<13:10,  3.08it/s]\u001b[A\n",
            "Iteration:  35% 1313/3742 [07:16<13:09,  3.08it/s]\u001b[A\n",
            "Iteration:  35% 1314/3742 [07:16<13:08,  3.08it/s]\u001b[A\n",
            "Iteration:  35% 1315/3742 [07:17<13:08,  3.08it/s]\u001b[A\n",
            "Iteration:  35% 1316/3742 [07:17<13:09,  3.07it/s]\u001b[A\n",
            "Iteration:  35% 1317/3742 [07:17<13:08,  3.08it/s]\u001b[A\n",
            "Iteration:  35% 1318/3742 [07:18<13:06,  3.08it/s]\u001b[A\n",
            "Iteration:  35% 1319/3742 [07:18<13:06,  3.08it/s]\u001b[A\n",
            "Iteration:  35% 1320/3742 [07:18<13:06,  3.08it/s]\u001b[A\n",
            "Iteration:  35% 1321/3742 [07:19<13:09,  3.07it/s]\u001b[A\n",
            "Iteration:  35% 1322/3742 [07:19<13:09,  3.06it/s]\u001b[A\n",
            "Iteration:  35% 1323/3742 [07:19<13:08,  3.07it/s]\u001b[A\n",
            "Iteration:  35% 1324/3742 [07:20<13:06,  3.08it/s]\u001b[A\n",
            "Iteration:  35% 1325/3742 [07:20<13:06,  3.07it/s]\u001b[A\n",
            "Iteration:  35% 1326/3742 [07:20<13:05,  3.08it/s]\u001b[A\n",
            "Iteration:  35% 1327/3742 [07:21<13:04,  3.08it/s]\u001b[A\n",
            "Iteration:  35% 1328/3742 [07:21<13:05,  3.08it/s]\u001b[A\n",
            "Iteration:  36% 1329/3742 [07:21<13:05,  3.07it/s]\u001b[A\n",
            "Iteration:  36% 1330/3742 [07:21<13:06,  3.07it/s]\u001b[A\n",
            "Iteration:  36% 1331/3742 [07:22<13:05,  3.07it/s]\u001b[A\n",
            "Iteration:  36% 1332/3742 [07:22<13:04,  3.07it/s]\u001b[A\n",
            "Iteration:  36% 1333/3742 [07:22<13:03,  3.07it/s]\u001b[A\n",
            "Iteration:  36% 1334/3742 [07:23<13:05,  3.06it/s]\u001b[A\n",
            "Iteration:  36% 1335/3742 [07:23<13:04,  3.07it/s]\u001b[A\n",
            "Iteration:  36% 1336/3742 [07:23<13:02,  3.07it/s]\u001b[A\n",
            "Iteration:  36% 1337/3742 [07:24<13:01,  3.08it/s]\u001b[A\n",
            "Iteration:  36% 1338/3742 [07:24<13:02,  3.07it/s]\u001b[A\n",
            "Iteration:  36% 1339/3742 [07:24<13:02,  3.07it/s]\u001b[A\n",
            "Iteration:  36% 1340/3742 [07:25<13:00,  3.08it/s]\u001b[A\n",
            "Iteration:  36% 1341/3742 [07:25<12:59,  3.08it/s]\u001b[A\n",
            "Iteration:  36% 1342/3742 [07:25<13:02,  3.07it/s]\u001b[A\n",
            "Iteration:  36% 1343/3742 [07:26<13:04,  3.06it/s]\u001b[A\n",
            "Iteration:  36% 1344/3742 [07:26<13:01,  3.07it/s]\u001b[A\n",
            "Iteration:  36% 1345/3742 [07:26<13:00,  3.07it/s]\u001b[A\n",
            "Iteration:  36% 1346/3742 [07:27<12:59,  3.07it/s]\u001b[A\n",
            "Iteration:  36% 1347/3742 [07:27<12:59,  3.07it/s]\u001b[A\n",
            "Iteration:  36% 1348/3742 [07:27<12:59,  3.07it/s]\u001b[A\n",
            "Iteration:  36% 1349/3742 [07:28<12:58,  3.07it/s]\u001b[A\n",
            "Iteration:  36% 1350/3742 [07:28<12:58,  3.07it/s]\u001b[A\n",
            "Iteration:  36% 1351/3742 [07:28<12:58,  3.07it/s]\u001b[A\n",
            "Iteration:  36% 1352/3742 [07:29<12:57,  3.07it/s]\u001b[A\n",
            "Iteration:  36% 1353/3742 [07:29<12:56,  3.08it/s]\u001b[A\n",
            "Iteration:  36% 1354/3742 [07:29<12:55,  3.08it/s]\u001b[A\n",
            "Iteration:  36% 1355/3742 [07:30<12:55,  3.08it/s]\u001b[A\n",
            "Iteration:  36% 1356/3742 [07:30<12:55,  3.08it/s]\u001b[A\n",
            "Iteration:  36% 1357/3742 [07:30<12:54,  3.08it/s]\u001b[A\n",
            "Iteration:  36% 1358/3742 [07:31<12:54,  3.08it/s]\u001b[A\n",
            "Iteration:  36% 1359/3742 [07:31<12:57,  3.07it/s]\u001b[A\n",
            "Iteration:  36% 1360/3742 [07:31<12:56,  3.07it/s]\u001b[A\n",
            "Iteration:  36% 1361/3742 [07:32<12:54,  3.07it/s]\u001b[A\n",
            "Iteration:  36% 1362/3742 [07:32<12:53,  3.08it/s]\u001b[A\n",
            "Iteration:  36% 1363/3742 [07:32<12:52,  3.08it/s]\u001b[A\n",
            "Iteration:  36% 1364/3742 [07:33<12:52,  3.08it/s]\u001b[A\n",
            "Iteration:  36% 1365/3742 [07:33<12:52,  3.08it/s]\u001b[A\n",
            "Iteration:  37% 1366/3742 [07:33<12:52,  3.08it/s]\u001b[A\n",
            "Iteration:  37% 1367/3742 [07:34<12:51,  3.08it/s]\u001b[A\n",
            "Iteration:  37% 1368/3742 [07:34<12:52,  3.07it/s]\u001b[A\n",
            "Iteration:  37% 1369/3742 [07:34<12:52,  3.07it/s]\u001b[A\n",
            "Iteration:  37% 1370/3742 [07:35<12:50,  3.08it/s]\u001b[A\n",
            "Iteration:  37% 1371/3742 [07:35<12:49,  3.08it/s]\u001b[A\n",
            "Iteration:  37% 1372/3742 [07:35<12:49,  3.08it/s]\u001b[A\n",
            "Iteration:  37% 1373/3742 [07:35<12:48,  3.08it/s]\u001b[A\n",
            "Iteration:  37% 1374/3742 [07:36<12:48,  3.08it/s]\u001b[A\n",
            "Iteration:  37% 1375/3742 [07:36<12:47,  3.08it/s]\u001b[A\n",
            "Iteration:  37% 1376/3742 [07:36<12:50,  3.07it/s]\u001b[A\n",
            "Iteration:  37% 1377/3742 [07:37<12:56,  3.05it/s]\u001b[A\n",
            "Iteration:  37% 1378/3742 [07:37<12:53,  3.05it/s]\u001b[A\n",
            "Iteration:  37% 1379/3742 [07:37<12:51,  3.06it/s]\u001b[A\n",
            "Iteration:  37% 1380/3742 [07:38<12:51,  3.06it/s]\u001b[A\n",
            "Iteration:  37% 1381/3742 [07:38<12:50,  3.06it/s]\u001b[A\n",
            "Iteration:  37% 1382/3742 [07:38<12:49,  3.07it/s]\u001b[A\n",
            "Iteration:  37% 1383/3742 [07:39<12:47,  3.07it/s]\u001b[A\n",
            "Iteration:  37% 1384/3742 [07:39<12:46,  3.08it/s]\u001b[A\n",
            "Iteration:  37% 1385/3742 [07:39<12:45,  3.08it/s]\u001b[A\n",
            "Iteration:  37% 1386/3742 [07:40<12:45,  3.08it/s]\u001b[A\n",
            "Iteration:  37% 1387/3742 [07:40<12:45,  3.08it/s]\u001b[A\n",
            "Iteration:  37% 1388/3742 [07:40<12:44,  3.08it/s]\u001b[A\n",
            "Iteration:  37% 1389/3742 [07:41<12:43,  3.08it/s]\u001b[A\n",
            "Iteration:  37% 1390/3742 [07:41<12:44,  3.08it/s]\u001b[A\n",
            "Iteration:  37% 1391/3742 [07:41<12:44,  3.08it/s]\u001b[A\n",
            "Iteration:  37% 1392/3742 [07:42<12:43,  3.08it/s]\u001b[A\n",
            "Iteration:  37% 1393/3742 [07:42<12:43,  3.08it/s]\u001b[A\n",
            "Iteration:  37% 1394/3742 [07:42<12:43,  3.07it/s]\u001b[A\n",
            "Iteration:  37% 1395/3742 [07:43<12:43,  3.07it/s]\u001b[A\n",
            "Iteration:  37% 1396/3742 [07:43<12:42,  3.08it/s]\u001b[A\n",
            "Iteration:  37% 1397/3742 [07:43<12:41,  3.08it/s]\u001b[A\n",
            "Iteration:  37% 1398/3742 [07:44<12:41,  3.08it/s]\u001b[A\n",
            "Iteration:  37% 1399/3742 [07:44<12:41,  3.08it/s]\u001b[A\n",
            "Iteration:  37% 1400/3742 [07:44<12:40,  3.08it/s]\u001b[A\n",
            "Iteration:  37% 1401/3742 [07:45<12:39,  3.08it/s]\u001b[A\n",
            "Iteration:  37% 1402/3742 [07:45<12:39,  3.08it/s]\u001b[A\n",
            "Iteration:  37% 1403/3742 [07:45<12:39,  3.08it/s]\u001b[A\n",
            "Iteration:  38% 1404/3742 [07:46<12:38,  3.08it/s]\u001b[A\n",
            "Iteration:  38% 1405/3742 [07:46<12:38,  3.08it/s]\u001b[A\n",
            "Iteration:  38% 1406/3742 [07:46<12:38,  3.08it/s]\u001b[A\n",
            "Iteration:  38% 1407/3742 [07:47<12:38,  3.08it/s]\u001b[A\n",
            "Iteration:  38% 1408/3742 [07:47<12:38,  3.08it/s]\u001b[A\n",
            "Iteration:  38% 1409/3742 [07:47<12:39,  3.07it/s]\u001b[A\n",
            "Iteration:  38% 1410/3742 [07:48<12:38,  3.08it/s]\u001b[A\n",
            "Iteration:  38% 1411/3742 [07:48<12:38,  3.07it/s]\u001b[A\n",
            "Iteration:  38% 1412/3742 [07:48<12:37,  3.07it/s]\u001b[A\n",
            "Iteration:  38% 1413/3742 [07:48<12:38,  3.07it/s]\u001b[A\n",
            "Iteration:  38% 1414/3742 [07:49<12:37,  3.07it/s]\u001b[A\n",
            "Iteration:  38% 1415/3742 [07:49<12:37,  3.07it/s]\u001b[A\n",
            "Iteration:  38% 1416/3742 [07:49<12:37,  3.07it/s]\u001b[A\n",
            "Iteration:  38% 1417/3742 [07:50<12:36,  3.07it/s]\u001b[A\n",
            "Iteration:  38% 1418/3742 [07:50<12:35,  3.07it/s]\u001b[A\n",
            "Iteration:  38% 1419/3742 [07:50<12:36,  3.07it/s]\u001b[A\n",
            "Iteration:  38% 1420/3742 [07:51<12:36,  3.07it/s]\u001b[A\n",
            "Iteration:  38% 1421/3742 [07:51<12:35,  3.07it/s]\u001b[A\n",
            "Iteration:  38% 1422/3742 [07:51<12:33,  3.08it/s]\u001b[A\n",
            "Iteration:  38% 1423/3742 [07:52<12:33,  3.08it/s]\u001b[A\n",
            "Iteration:  38% 1424/3742 [07:52<12:33,  3.08it/s]\u001b[A\n",
            "Iteration:  38% 1425/3742 [07:52<12:32,  3.08it/s]\u001b[A\n",
            "Iteration:  38% 1426/3742 [07:53<12:35,  3.07it/s]\u001b[A\n",
            "Iteration:  38% 1427/3742 [07:53<12:37,  3.06it/s]\u001b[A\n",
            "Iteration:  38% 1428/3742 [07:53<12:35,  3.06it/s]\u001b[A\n",
            "Iteration:  38% 1429/3742 [07:54<12:34,  3.07it/s]\u001b[A\n",
            "Iteration:  38% 1430/3742 [07:54<12:33,  3.07it/s]\u001b[A\n",
            "Iteration:  38% 1431/3742 [07:54<12:31,  3.07it/s]\u001b[A\n",
            "Iteration:  38% 1432/3742 [07:55<12:31,  3.07it/s]\u001b[A\n",
            "Iteration:  38% 1433/3742 [07:55<12:31,  3.07it/s]\u001b[A\n",
            "Iteration:  38% 1434/3742 [07:55<12:30,  3.08it/s]\u001b[A\n",
            "Iteration:  38% 1435/3742 [07:56<12:29,  3.08it/s]\u001b[A\n",
            "Iteration:  38% 1436/3742 [07:56<12:30,  3.07it/s]\u001b[A\n",
            "Iteration:  38% 1437/3742 [07:56<12:29,  3.07it/s]\u001b[A\n",
            "Iteration:  38% 1438/3742 [07:57<12:28,  3.08it/s]\u001b[A\n",
            "Iteration:  38% 1439/3742 [07:57<12:28,  3.08it/s]\u001b[A\n",
            "Iteration:  38% 1440/3742 [07:57<12:27,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1441/3742 [07:58<12:26,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1442/3742 [07:58<12:26,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1443/3742 [07:58<12:26,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1444/3742 [07:59<12:25,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1445/3742 [07:59<12:26,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1446/3742 [07:59<12:26,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1447/3742 [08:00<12:25,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1448/3742 [08:00<12:26,  3.07it/s]\u001b[A\n",
            "Iteration:  39% 1449/3742 [08:00<12:25,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1450/3742 [08:01<12:24,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1451/3742 [08:01<12:24,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1452/3742 [08:01<12:24,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1453/3742 [08:01<12:23,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1454/3742 [08:02<12:22,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1455/3742 [08:02<12:23,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1456/3742 [08:02<12:24,  3.07it/s]\u001b[A\n",
            "Iteration:  39% 1457/3742 [08:03<12:23,  3.07it/s]\u001b[A\n",
            "Iteration:  39% 1458/3742 [08:03<12:22,  3.07it/s]\u001b[A\n",
            "Iteration:  39% 1459/3742 [08:03<12:22,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1460/3742 [08:04<12:21,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1461/3742 [08:04<12:20,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1462/3742 [08:04<12:21,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1463/3742 [08:05<12:20,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1464/3742 [08:05<12:20,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1465/3742 [08:05<12:18,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1466/3742 [08:06<12:18,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1467/3742 [08:06<12:18,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1468/3742 [08:06<12:17,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1469/3742 [08:07<12:18,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1470/3742 [08:07<12:18,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1471/3742 [08:07<12:18,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1472/3742 [08:08<12:18,  3.07it/s]\u001b[A\n",
            "Iteration:  39% 1473/3742 [08:08<12:17,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1474/3742 [08:08<12:16,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1475/3742 [08:09<12:15,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1476/3742 [08:09<12:15,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1477/3742 [08:09<12:14,  3.08it/s]\u001b[A\n",
            "Iteration:  39% 1478/3742 [08:10<12:15,  3.08it/s]\u001b[A\n",
            "Iteration:  40% 1479/3742 [08:10<12:14,  3.08it/s]\u001b[A\n",
            "Iteration:  40% 1480/3742 [08:10<12:13,  3.08it/s]\u001b[A\n",
            "Iteration:  40% 1481/3742 [08:11<12:13,  3.08it/s]\u001b[A\n",
            "Iteration:  40% 1482/3742 [08:11<12:12,  3.09it/s]\u001b[A\n",
            "Iteration:  40% 1483/3742 [08:11<12:12,  3.09it/s]\u001b[A\n",
            "Iteration:  40% 1484/3742 [08:12<12:13,  3.08it/s]\u001b[A\n",
            "Iteration:  40% 1485/3742 [08:12<12:13,  3.08it/s]\u001b[A\n",
            "Iteration:  40% 1486/3742 [08:12<12:16,  3.06it/s]\u001b[A\n",
            "Iteration:  40% 1487/3742 [08:13<12:20,  3.05it/s]\u001b[A\n",
            "Iteration:  40% 1488/3742 [08:13<12:21,  3.04it/s]\u001b[A\n",
            "Iteration:  40% 1489/3742 [08:13<12:22,  3.03it/s]\u001b[A\n",
            "Iteration:  40% 1490/3742 [08:14<12:18,  3.05it/s]\u001b[A\n",
            "Iteration:  40% 1491/3742 [08:14<12:15,  3.06it/s]\u001b[A\n",
            "Iteration:  40% 1492/3742 [08:14<12:15,  3.06it/s]\u001b[A\n",
            "Iteration:  40% 1493/3742 [08:15<12:13,  3.06it/s]\u001b[A\n",
            "Iteration:  40% 1494/3742 [08:15<12:12,  3.07it/s]\u001b[A\n",
            "Iteration:  40% 1495/3742 [08:15<12:15,  3.06it/s]\u001b[A\n",
            "Iteration:  40% 1496/3742 [08:15<12:14,  3.06it/s]\u001b[A\n",
            "Iteration:  40% 1497/3742 [08:16<12:12,  3.07it/s]\u001b[A\n",
            "Iteration:  40% 1498/3742 [08:16<12:11,  3.07it/s]\u001b[A\n",
            "Iteration:  40% 1499/3742 [08:16<12:10,  3.07it/s]\u001b[A03/05/2020 06:59:02 - INFO - transformers.configuration_utils -   Configuration saved in output_roberta_US/checkpoint-1500/config.json\n",
            "03/05/2020 06:59:02 - INFO - transformers.modeling_utils -   Model weights saved in output_roberta_US/checkpoint-1500/pytorch_model.bin\n",
            "03/05/2020 06:59:03 - INFO - __main__ -   Saving model checkpoint to output_roberta_US/checkpoint-1500\n",
            "03/05/2020 06:59:06 - INFO - __main__ -   Saving optimizer and scheduler states to output_roberta_US/checkpoint-1500\n",
            "\n",
            "Iteration:  40% 1500/3742 [08:21<1:04:14,  1.72s/it]\u001b[A\n",
            "Iteration:  40% 1501/3742 [08:22<48:51,  1.31s/it]  \u001b[A\n",
            "Iteration:  40% 1502/3742 [08:22<37:51,  1.01s/it]\u001b[A\n",
            "Iteration:  40% 1503/3742 [08:22<30:08,  1.24it/s]\u001b[A\n",
            "Iteration:  40% 1504/3742 [08:23<24:44,  1.51it/s]\u001b[A\n",
            "Iteration:  40% 1505/3742 [08:23<20:56,  1.78it/s]\u001b[A\n",
            "Iteration:  40% 1506/3742 [08:23<18:17,  2.04it/s]\u001b[A\n",
            "Iteration:  40% 1507/3742 [08:24<16:29,  2.26it/s]\u001b[A\n",
            "Iteration:  40% 1508/3742 [08:24<15:11,  2.45it/s]\u001b[A\n",
            "Iteration:  40% 1509/3742 [08:24<14:16,  2.61it/s]\u001b[A\n",
            "Iteration:  40% 1510/3742 [08:25<13:37,  2.73it/s]\u001b[A\n",
            "Iteration:  40% 1511/3742 [08:25<13:10,  2.82it/s]\u001b[A\n",
            "Iteration:  40% 1512/3742 [08:25<12:52,  2.89it/s]\u001b[A\n",
            "Iteration:  40% 1513/3742 [08:26<12:40,  2.93it/s]\u001b[A\n",
            "Iteration:  40% 1514/3742 [08:26<12:30,  2.97it/s]\u001b[A\n",
            "Iteration:  40% 1515/3742 [08:26<12:23,  3.00it/s]\u001b[A\n",
            "Iteration:  41% 1516/3742 [08:27<12:18,  3.02it/s]\u001b[A\n",
            "Iteration:  41% 1517/3742 [08:27<12:14,  3.03it/s]\u001b[A\n",
            "Iteration:  41% 1518/3742 [08:27<12:11,  3.04it/s]\u001b[A\n",
            "Iteration:  41% 1519/3742 [08:28<12:09,  3.05it/s]\u001b[A\n",
            "Iteration:  41% 1520/3742 [08:28<12:07,  3.05it/s]\u001b[A\n",
            "Iteration:  41% 1521/3742 [08:28<12:05,  3.06it/s]\u001b[A\n",
            "Iteration:  41% 1522/3742 [08:29<12:04,  3.07it/s]\u001b[A\n",
            "Iteration:  41% 1523/3742 [08:29<12:03,  3.07it/s]\u001b[A\n",
            "Iteration:  41% 1524/3742 [08:29<12:02,  3.07it/s]\u001b[A\n",
            "Iteration:  41% 1525/3742 [08:30<12:01,  3.07it/s]\u001b[A\n",
            "Iteration:  41% 1526/3742 [08:30<12:00,  3.08it/s]\u001b[A\n",
            "Iteration:  41% 1527/3742 [08:30<11:59,  3.08it/s]\u001b[A\n",
            "Iteration:  41% 1528/3742 [08:31<11:58,  3.08it/s]\u001b[A\n",
            "Iteration:  41% 1529/3742 [08:31<12:02,  3.06it/s]\u001b[A\n",
            "Iteration:  41% 1530/3742 [08:31<12:01,  3.07it/s]\u001b[A\n",
            "Iteration:  41% 1531/3742 [08:32<11:59,  3.07it/s]\u001b[A\n",
            "Iteration:  41% 1532/3742 [08:32<11:58,  3.07it/s]\u001b[A\n",
            "Iteration:  41% 1533/3742 [08:32<11:58,  3.08it/s]\u001b[A\n",
            "Iteration:  41% 1534/3742 [08:33<12:00,  3.07it/s]\u001b[A\n",
            "Iteration:  41% 1535/3742 [08:33<11:59,  3.07it/s]\u001b[A\n",
            "Iteration:  41% 1536/3742 [08:33<11:58,  3.07it/s]\u001b[A\n",
            "Iteration:  41% 1537/3742 [08:34<11:57,  3.07it/s]\u001b[A\n",
            "Iteration:  41% 1538/3742 [08:34<11:56,  3.08it/s]\u001b[A\n",
            "Iteration:  41% 1539/3742 [08:34<11:56,  3.08it/s]\u001b[A\n",
            "Iteration:  41% 1540/3742 [08:35<11:55,  3.08it/s]\u001b[A\n",
            "Iteration:  41% 1541/3742 [08:35<11:54,  3.08it/s]\u001b[A\n",
            "Iteration:  41% 1542/3742 [08:35<11:54,  3.08it/s]\u001b[A\n",
            "Iteration:  41% 1543/3742 [08:35<11:54,  3.08it/s]\u001b[A\n",
            "Iteration:  41% 1544/3742 [08:36<11:53,  3.08it/s]\u001b[A\n",
            "Iteration:  41% 1545/3742 [08:36<11:53,  3.08it/s]\u001b[A\n",
            "Iteration:  41% 1546/3742 [08:36<11:55,  3.07it/s]\u001b[A\n",
            "Iteration:  41% 1547/3742 [08:37<11:56,  3.07it/s]\u001b[A\n",
            "Iteration:  41% 1548/3742 [08:37<11:55,  3.07it/s]\u001b[A\n",
            "Iteration:  41% 1549/3742 [08:37<11:54,  3.07it/s]\u001b[A\n",
            "Iteration:  41% 1550/3742 [08:38<11:54,  3.07it/s]\u001b[A\n",
            "Iteration:  41% 1551/3742 [08:38<11:52,  3.07it/s]\u001b[A\n",
            "Iteration:  41% 1552/3742 [08:38<11:51,  3.08it/s]\u001b[A\n",
            "Iteration:  42% 1553/3742 [08:39<11:52,  3.07it/s]\u001b[A\n",
            "Iteration:  42% 1554/3742 [08:39<11:51,  3.08it/s]\u001b[A\n",
            "Iteration:  42% 1555/3742 [08:39<11:53,  3.06it/s]\u001b[A\n",
            "Iteration:  42% 1556/3742 [08:40<11:54,  3.06it/s]\u001b[A\n",
            "Iteration:  42% 1557/3742 [08:40<11:54,  3.06it/s]\u001b[A\n",
            "Iteration:  42% 1558/3742 [08:40<11:52,  3.07it/s]\u001b[A\n",
            "Iteration:  42% 1559/3742 [08:41<11:50,  3.07it/s]\u001b[A\n",
            "Iteration:  42% 1560/3742 [08:41<11:49,  3.07it/s]\u001b[A\n",
            "Iteration:  42% 1561/3742 [08:41<11:49,  3.07it/s]\u001b[A\n",
            "Iteration:  42% 1562/3742 [08:42<11:48,  3.08it/s]\u001b[A\n",
            "Iteration:  42% 1563/3742 [08:42<11:50,  3.07it/s]\u001b[A\n",
            "Iteration:  42% 1564/3742 [08:42<11:48,  3.07it/s]\u001b[A\n",
            "Iteration:  42% 1565/3742 [08:43<11:51,  3.06it/s]\u001b[A\n",
            "Iteration:  42% 1566/3742 [08:43<11:50,  3.06it/s]\u001b[A\n",
            "Iteration:  42% 1567/3742 [08:43<11:49,  3.07it/s]\u001b[A\n",
            "Iteration:  42% 1568/3742 [08:44<11:48,  3.07it/s]\u001b[A\n",
            "Iteration:  42% 1569/3742 [08:44<11:48,  3.07it/s]\u001b[A\n",
            "Iteration:  42% 1570/3742 [08:44<11:47,  3.07it/s]\u001b[A\n",
            "Iteration:  42% 1571/3742 [08:45<11:45,  3.08it/s]\u001b[A\n",
            "Iteration:  42% 1572/3742 [08:45<11:45,  3.08it/s]\u001b[A\n",
            "Iteration:  42% 1573/3742 [08:45<11:44,  3.08it/s]\u001b[A\n",
            "Iteration:  42% 1574/3742 [08:46<11:44,  3.08it/s]\u001b[A\n",
            "Iteration:  42% 1575/3742 [08:46<11:43,  3.08it/s]\u001b[A\n",
            "Iteration:  42% 1576/3742 [08:46<11:43,  3.08it/s]\u001b[A\n",
            "Iteration:  42% 1577/3742 [08:47<11:44,  3.07it/s]\u001b[A\n",
            "Iteration:  42% 1578/3742 [08:47<11:45,  3.07it/s]\u001b[A\n",
            "Iteration:  42% 1579/3742 [08:47<11:44,  3.07it/s]\u001b[A\n",
            "Iteration:  42% 1580/3742 [08:48<11:42,  3.08it/s]\u001b[A\n",
            "Iteration:  42% 1581/3742 [08:48<11:43,  3.07it/s]\u001b[A\n",
            "Iteration:  42% 1582/3742 [08:48<11:42,  3.08it/s]\u001b[A\n",
            "Iteration:  42% 1583/3742 [08:49<11:42,  3.07it/s]\u001b[A\n",
            "Iteration:  42% 1584/3742 [08:49<11:41,  3.08it/s]\u001b[A\n",
            "Iteration:  42% 1585/3742 [08:49<11:40,  3.08it/s]\u001b[A\n",
            "Iteration:  42% 1586/3742 [08:49<11:40,  3.08it/s]\u001b[A\n",
            "Iteration:  42% 1587/3742 [08:50<11:40,  3.08it/s]\u001b[A\n",
            "Iteration:  42% 1588/3742 [08:50<11:39,  3.08it/s]\u001b[A\n",
            "Iteration:  42% 1589/3742 [08:50<11:38,  3.08it/s]\u001b[A\n",
            "Iteration:  42% 1590/3742 [08:51<11:38,  3.08it/s]\u001b[A\n",
            "Iteration:  43% 1591/3742 [08:51<11:39,  3.08it/s]\u001b[A\n",
            "Iteration:  43% 1592/3742 [08:51<11:38,  3.08it/s]\u001b[A\n",
            "Iteration:  43% 1593/3742 [08:52<11:38,  3.08it/s]\u001b[A\n",
            "Iteration:  43% 1594/3742 [08:52<11:38,  3.08it/s]\u001b[A\n",
            "Iteration:  43% 1595/3742 [08:52<11:38,  3.07it/s]\u001b[A\n",
            "Iteration:  43% 1596/3742 [08:53<11:42,  3.06it/s]\u001b[A\n",
            "Iteration:  43% 1597/3742 [08:53<11:44,  3.05it/s]\u001b[A\n",
            "Iteration:  43% 1598/3742 [08:53<11:41,  3.06it/s]\u001b[A\n",
            "Iteration:  43% 1599/3742 [08:54<11:39,  3.06it/s]\u001b[A\n",
            "Iteration:  43% 1600/3742 [08:54<11:38,  3.07it/s]\u001b[A\n",
            "Iteration:  43% 1601/3742 [08:54<11:36,  3.07it/s]\u001b[A\n",
            "Iteration:  43% 1602/3742 [08:55<11:35,  3.08it/s]\u001b[A\n",
            "Iteration:  43% 1603/3742 [08:55<11:35,  3.08it/s]\u001b[A\n",
            "Iteration:  43% 1604/3742 [08:55<11:34,  3.08it/s]\u001b[A\n",
            "Iteration:  43% 1605/3742 [08:56<11:35,  3.07it/s]\u001b[A\n",
            "Iteration:  43% 1606/3742 [08:56<11:35,  3.07it/s]\u001b[A\n",
            "Iteration:  43% 1607/3742 [08:56<11:34,  3.07it/s]\u001b[A\n",
            "Iteration:  43% 1608/3742 [08:57<11:34,  3.07it/s]\u001b[A\n",
            "Iteration:  43% 1609/3742 [08:57<11:34,  3.07it/s]\u001b[A\n",
            "Iteration:  43% 1610/3742 [08:57<11:35,  3.07it/s]\u001b[A\n",
            "Iteration:  43% 1611/3742 [08:58<11:33,  3.07it/s]\u001b[A\n",
            "Iteration:  43% 1612/3742 [08:58<11:32,  3.07it/s]\u001b[A\n",
            "Iteration:  43% 1613/3742 [08:58<11:32,  3.07it/s]\u001b[A\n",
            "Iteration:  43% 1614/3742 [08:59<11:31,  3.08it/s]\u001b[A\n",
            "Iteration:  43% 1615/3742 [08:59<11:31,  3.08it/s]\u001b[A\n",
            "Iteration:  43% 1616/3742 [08:59<11:30,  3.08it/s]\u001b[A\n",
            "Iteration:  43% 1617/3742 [09:00<11:31,  3.07it/s]\u001b[A\n",
            "Iteration:  43% 1618/3742 [09:00<11:30,  3.07it/s]\u001b[A\n",
            "Iteration:  43% 1619/3742 [09:00<11:30,  3.08it/s]\u001b[A\n",
            "Iteration:  43% 1620/3742 [09:01<11:30,  3.07it/s]\u001b[A\n",
            "Iteration:  43% 1621/3742 [09:01<11:32,  3.06it/s]\u001b[A\n",
            "Iteration:  43% 1622/3742 [09:01<11:31,  3.07it/s]\u001b[A\n",
            "Iteration:  43% 1623/3742 [09:02<11:30,  3.07it/s]\u001b[A\n",
            "Iteration:  43% 1624/3742 [09:02<11:29,  3.07it/s]\u001b[A\n",
            "Iteration:  43% 1625/3742 [09:02<11:28,  3.07it/s]\u001b[A\n",
            "Iteration:  43% 1626/3742 [09:02<11:27,  3.08it/s]\u001b[A\n",
            "Iteration:  43% 1627/3742 [09:03<11:26,  3.08it/s]\u001b[A\n",
            "Iteration:  44% 1628/3742 [09:03<11:26,  3.08it/s]\u001b[A\n",
            "Iteration:  44% 1629/3742 [09:03<11:25,  3.08it/s]\u001b[A\n",
            "Iteration:  44% 1630/3742 [09:04<11:26,  3.08it/s]\u001b[A\n",
            "Iteration:  44% 1631/3742 [09:04<11:27,  3.07it/s]\u001b[A\n",
            "Iteration:  44% 1632/3742 [09:04<11:26,  3.07it/s]\u001b[A\n",
            "Iteration:  44% 1633/3742 [09:05<11:25,  3.08it/s]\u001b[A\n",
            "Iteration:  44% 1634/3742 [09:05<11:25,  3.08it/s]\u001b[A\n",
            "Iteration:  44% 1635/3742 [09:05<11:24,  3.08it/s]\u001b[A\n",
            "Iteration:  44% 1636/3742 [09:06<11:23,  3.08it/s]\u001b[A\n",
            "Iteration:  44% 1637/3742 [09:06<11:23,  3.08it/s]\u001b[A\n",
            "Iteration:  44% 1638/3742 [09:06<11:22,  3.08it/s]\u001b[A\n",
            "Iteration:  44% 1639/3742 [09:07<11:22,  3.08it/s]\u001b[A\n",
            "Iteration:  44% 1640/3742 [09:07<11:22,  3.08it/s]\u001b[A\n",
            "Iteration:  44% 1641/3742 [09:07<11:21,  3.08it/s]\u001b[A\n",
            "Iteration:  44% 1642/3742 [09:08<11:21,  3.08it/s]\u001b[A\n",
            "Iteration:  44% 1643/3742 [09:08<11:21,  3.08it/s]\u001b[A\n",
            "Iteration:  44% 1644/3742 [09:08<11:22,  3.08it/s]\u001b[A\n",
            "Iteration:  44% 1645/3742 [09:09<11:21,  3.08it/s]\u001b[A\n",
            "Iteration:  44% 1646/3742 [09:09<11:21,  3.08it/s]\u001b[A\n",
            "Iteration:  44% 1647/3742 [09:09<11:21,  3.07it/s]\u001b[A\n",
            "Iteration:  44% 1648/3742 [09:10<11:24,  3.06it/s]\u001b[A\n",
            "Iteration:  44% 1649/3742 [09:10<11:24,  3.06it/s]\u001b[A\n",
            "Iteration:  44% 1650/3742 [09:10<11:22,  3.07it/s]\u001b[A\n",
            "Iteration:  44% 1651/3742 [09:11<11:21,  3.07it/s]\u001b[A\n",
            "Iteration:  44% 1652/3742 [09:11<11:20,  3.07it/s]\u001b[A\n",
            "Iteration:  44% 1653/3742 [09:11<11:19,  3.07it/s]\u001b[A\n",
            "Iteration:  44% 1654/3742 [09:12<11:18,  3.08it/s]\u001b[A\n",
            "Iteration:  44% 1655/3742 [09:12<11:17,  3.08it/s]\u001b[A\n",
            "Iteration:  44% 1656/3742 [09:12<11:18,  3.08it/s]\u001b[A\n",
            "Iteration:  44% 1657/3742 [09:13<11:21,  3.06it/s]\u001b[A\n",
            "Iteration:  44% 1658/3742 [09:13<11:22,  3.05it/s]\u001b[A\n",
            "Iteration:  44% 1659/3742 [09:13<11:21,  3.06it/s]\u001b[A\n",
            "Iteration:  44% 1660/3742 [09:14<11:18,  3.07it/s]\u001b[A\n",
            "Iteration:  44% 1661/3742 [09:14<11:17,  3.07it/s]\u001b[A\n",
            "Iteration:  44% 1662/3742 [09:14<11:15,  3.08it/s]\u001b[A\n",
            "Iteration:  44% 1663/3742 [09:15<11:14,  3.08it/s]\u001b[A\n",
            "Iteration:  44% 1664/3742 [09:15<11:14,  3.08it/s]\u001b[A\n",
            "Iteration:  44% 1665/3742 [09:15<11:17,  3.07it/s]\u001b[A\n",
            "Iteration:  45% 1666/3742 [09:16<11:16,  3.07it/s]\u001b[A\n",
            "Iteration:  45% 1667/3742 [09:16<11:15,  3.07it/s]\u001b[A\n",
            "Iteration:  45% 1668/3742 [09:16<11:14,  3.07it/s]\u001b[A\n",
            "Iteration:  45% 1669/3742 [09:16<11:13,  3.08it/s]\u001b[A\n",
            "Iteration:  45% 1670/3742 [09:17<11:13,  3.08it/s]\u001b[A\n",
            "Iteration:  45% 1671/3742 [09:17<11:12,  3.08it/s]\u001b[A\n",
            "Iteration:  45% 1672/3742 [09:17<11:12,  3.08it/s]\u001b[A\n",
            "Iteration:  45% 1673/3742 [09:18<11:11,  3.08it/s]\u001b[A\n",
            "Iteration:  45% 1674/3742 [09:18<11:11,  3.08it/s]\u001b[A\n",
            "Iteration:  45% 1675/3742 [09:18<11:10,  3.08it/s]\u001b[A\n",
            "Iteration:  45% 1676/3742 [09:19<11:09,  3.08it/s]\u001b[A\n",
            "Iteration:  45% 1677/3742 [09:19<11:10,  3.08it/s]\u001b[A\n",
            "Iteration:  45% 1678/3742 [09:19<11:09,  3.08it/s]\u001b[A\n",
            "Iteration:  45% 1679/3742 [09:20<11:08,  3.08it/s]\u001b[A\n",
            "Iteration:  45% 1680/3742 [09:20<11:08,  3.09it/s]\u001b[A\n",
            "Iteration:  45% 1681/3742 [09:20<11:07,  3.09it/s]\u001b[A\n",
            "Iteration:  45% 1682/3742 [09:21<11:07,  3.08it/s]\u001b[A\n",
            "Iteration:  45% 1683/3742 [09:21<11:07,  3.08it/s]\u001b[A\n",
            "Iteration:  45% 1684/3742 [09:21<11:07,  3.08it/s]\u001b[A\n",
            "Iteration:  45% 1685/3742 [09:22<11:07,  3.08it/s]\u001b[A\n",
            "Iteration:  45% 1686/3742 [09:22<11:08,  3.08it/s]\u001b[A\n",
            "Iteration:  45% 1687/3742 [09:22<11:07,  3.08it/s]\u001b[A\n",
            "Iteration:  45% 1688/3742 [09:23<11:09,  3.07it/s]\u001b[A\n",
            "Iteration:  45% 1689/3742 [09:23<11:08,  3.07it/s]\u001b[A\n",
            "Iteration:  45% 1690/3742 [09:23<11:07,  3.07it/s]\u001b[A\n",
            "Iteration:  45% 1691/3742 [09:24<11:06,  3.08it/s]\u001b[A\n",
            "Iteration:  45% 1692/3742 [09:24<11:05,  3.08it/s]\u001b[A\n",
            "Iteration:  45% 1693/3742 [09:24<11:05,  3.08it/s]\u001b[A\n",
            "Iteration:  45% 1694/3742 [09:25<11:06,  3.07it/s]\u001b[A\n",
            "Iteration:  45% 1695/3742 [09:25<11:05,  3.08it/s]\u001b[A\n",
            "Iteration:  45% 1696/3742 [09:25<11:05,  3.08it/s]\u001b[A\n",
            "Iteration:  45% 1697/3742 [09:26<11:04,  3.08it/s]\u001b[A\n",
            "Iteration:  45% 1698/3742 [09:26<11:04,  3.08it/s]\u001b[A\n",
            "Iteration:  45% 1699/3742 [09:26<11:06,  3.07it/s]\u001b[A\n",
            "Iteration:  45% 1700/3742 [09:27<11:05,  3.07it/s]\u001b[A\n",
            "Iteration:  45% 1701/3742 [09:27<11:04,  3.07it/s]\u001b[A\n",
            "Iteration:  45% 1702/3742 [09:27<11:03,  3.07it/s]\u001b[A\n",
            "Iteration:  46% 1703/3742 [09:28<11:04,  3.07it/s]\u001b[A\n",
            "Iteration:  46% 1704/3742 [09:28<11:03,  3.07it/s]\u001b[A\n",
            "Iteration:  46% 1705/3742 [09:28<11:03,  3.07it/s]\u001b[A\n",
            "Iteration:  46% 1706/3742 [09:29<11:05,  3.06it/s]\u001b[A\n",
            "Iteration:  46% 1707/3742 [09:29<11:04,  3.06it/s]\u001b[A\n",
            "Iteration:  46% 1708/3742 [09:29<11:03,  3.07it/s]\u001b[A\n",
            "Iteration:  46% 1709/3742 [09:29<11:02,  3.07it/s]\u001b[A\n",
            "Iteration:  46% 1710/3742 [09:30<11:02,  3.07it/s]\u001b[A\n",
            "Iteration:  46% 1711/3742 [09:30<11:02,  3.07it/s]\u001b[A\n",
            "Iteration:  46% 1712/3742 [09:30<11:01,  3.07it/s]\u001b[A\n",
            "Iteration:  46% 1713/3742 [09:31<10:59,  3.08it/s]\u001b[A\n",
            "Iteration:  46% 1714/3742 [09:31<10:58,  3.08it/s]\u001b[A\n",
            "Iteration:  46% 1715/3742 [09:31<10:59,  3.07it/s]\u001b[A\n",
            "Iteration:  46% 1716/3742 [09:32<10:58,  3.08it/s]\u001b[A\n",
            "Iteration:  46% 1717/3742 [09:32<10:58,  3.08it/s]\u001b[A\n",
            "Iteration:  46% 1718/3742 [09:32<10:56,  3.08it/s]\u001b[A\n",
            "Iteration:  46% 1719/3742 [09:33<10:57,  3.08it/s]\u001b[A\n",
            "Iteration:  46% 1720/3742 [09:33<10:57,  3.07it/s]\u001b[A\n",
            "Iteration:  46% 1721/3742 [09:33<10:57,  3.07it/s]\u001b[A\n",
            "Iteration:  46% 1722/3742 [09:34<10:56,  3.08it/s]\u001b[A\n",
            "Iteration:  46% 1723/3742 [09:34<10:56,  3.08it/s]\u001b[A\n",
            "Iteration:  46% 1724/3742 [09:34<10:57,  3.07it/s]\u001b[A\n",
            "Iteration:  46% 1725/3742 [09:35<10:56,  3.07it/s]\u001b[A\n",
            "Iteration:  46% 1726/3742 [09:35<10:55,  3.08it/s]\u001b[A\n",
            "Iteration:  46% 1727/3742 [09:35<10:54,  3.08it/s]\u001b[A\n",
            "Iteration:  46% 1728/3742 [09:36<10:53,  3.08it/s]\u001b[A\n",
            "Iteration:  46% 1729/3742 [09:36<10:53,  3.08it/s]\u001b[A\n",
            "Iteration:  46% 1730/3742 [09:36<10:56,  3.06it/s]\u001b[A\n",
            "Iteration:  46% 1731/3742 [09:37<11:01,  3.04it/s]\u001b[A\n",
            "Iteration:  46% 1732/3742 [09:37<11:00,  3.04it/s]\u001b[A\n",
            "Iteration:  46% 1733/3742 [09:37<10:58,  3.05it/s]\u001b[A\n",
            "Iteration:  46% 1734/3742 [09:38<10:56,  3.06it/s]\u001b[A\n",
            "Iteration:  46% 1735/3742 [09:38<10:55,  3.06it/s]\u001b[A\n",
            "Iteration:  46% 1736/3742 [09:38<10:54,  3.06it/s]\u001b[A\n",
            "Iteration:  46% 1737/3742 [09:39<10:53,  3.07it/s]\u001b[A\n",
            "Iteration:  46% 1738/3742 [09:39<10:52,  3.07it/s]\u001b[A\n",
            "Iteration:  46% 1739/3742 [09:39<10:51,  3.07it/s]\u001b[A\n",
            "Iteration:  46% 1740/3742 [09:40<10:50,  3.08it/s]\u001b[A\n",
            "Iteration:  47% 1741/3742 [09:40<10:49,  3.08it/s]\u001b[A\n",
            "Iteration:  47% 1742/3742 [09:40<10:49,  3.08it/s]\u001b[A\n",
            "Iteration:  47% 1743/3742 [09:41<10:48,  3.08it/s]\u001b[A\n",
            "Iteration:  47% 1744/3742 [09:41<10:48,  3.08it/s]\u001b[A\n",
            "Iteration:  47% 1745/3742 [09:41<10:48,  3.08it/s]\u001b[A\n",
            "Iteration:  47% 1746/3742 [09:42<10:48,  3.08it/s]\u001b[A\n",
            "Iteration:  47% 1747/3742 [09:42<10:48,  3.08it/s]\u001b[A\n",
            "Iteration:  47% 1748/3742 [09:42<10:47,  3.08it/s]\u001b[A\n",
            "Iteration:  47% 1749/3742 [09:43<10:48,  3.07it/s]\u001b[A\n",
            "Iteration:  47% 1750/3742 [09:43<10:48,  3.07it/s]\u001b[A\n",
            "Iteration:  47% 1751/3742 [09:43<10:48,  3.07it/s]\u001b[A\n",
            "Iteration:  47% 1752/3742 [09:43<10:47,  3.07it/s]\u001b[A\n",
            "Iteration:  47% 1753/3742 [09:44<10:49,  3.06it/s]\u001b[A\n",
            "Iteration:  47% 1754/3742 [09:44<10:48,  3.07it/s]\u001b[A\n",
            "Iteration:  47% 1755/3742 [09:44<10:47,  3.07it/s]\u001b[A\n",
            "Iteration:  47% 1756/3742 [09:45<10:45,  3.07it/s]\u001b[A\n",
            "Iteration:  47% 1757/3742 [09:45<10:44,  3.08it/s]\u001b[A\n",
            "Iteration:  47% 1758/3742 [09:45<10:45,  3.07it/s]\u001b[A\n",
            "Iteration:  47% 1759/3742 [09:46<10:44,  3.08it/s]\u001b[A\n",
            "Iteration:  47% 1760/3742 [09:46<10:44,  3.08it/s]\u001b[A\n",
            "Iteration:  47% 1761/3742 [09:46<10:43,  3.08it/s]\u001b[A\n",
            "Iteration:  47% 1762/3742 [09:47<10:43,  3.08it/s]\u001b[A\n",
            "Iteration:  47% 1763/3742 [09:47<10:43,  3.08it/s]\u001b[A\n",
            "Iteration:  47% 1764/3742 [09:47<10:42,  3.08it/s]\u001b[A\n",
            "Iteration:  47% 1765/3742 [09:48<10:44,  3.07it/s]\u001b[A\n",
            "Iteration:  47% 1766/3742 [09:48<10:44,  3.07it/s]\u001b[A\n",
            "Iteration:  47% 1767/3742 [09:48<10:43,  3.07it/s]\u001b[A\n",
            "Iteration:  47% 1768/3742 [09:49<10:41,  3.08it/s]\u001b[A\n",
            "Iteration:  47% 1769/3742 [09:49<10:41,  3.08it/s]\u001b[A\n",
            "Iteration:  47% 1770/3742 [09:49<10:40,  3.08it/s]\u001b[A\n",
            "Iteration:  47% 1771/3742 [09:50<10:39,  3.08it/s]\u001b[A\n",
            "Iteration:  47% 1772/3742 [09:50<10:40,  3.08it/s]\u001b[A\n",
            "Iteration:  47% 1773/3742 [09:50<10:39,  3.08it/s]\u001b[A\n",
            "Iteration:  47% 1774/3742 [09:51<10:39,  3.08it/s]\u001b[A\n",
            "Iteration:  47% 1775/3742 [09:51<10:40,  3.07it/s]\u001b[A\n",
            "Iteration:  47% 1776/3742 [09:51<10:40,  3.07it/s]\u001b[A\n",
            "Iteration:  47% 1777/3742 [09:52<10:39,  3.07it/s]\u001b[A\n",
            "Iteration:  48% 1778/3742 [09:52<10:38,  3.07it/s]\u001b[A\n",
            "Iteration:  48% 1779/3742 [09:52<10:39,  3.07it/s]\u001b[A\n",
            "Iteration:  48% 1780/3742 [09:53<10:40,  3.06it/s]\u001b[A\n",
            "Iteration:  48% 1781/3742 [09:53<10:45,  3.04it/s]\u001b[A\n",
            "Iteration:  48% 1782/3742 [09:53<10:43,  3.05it/s]\u001b[A\n",
            "Iteration:  48% 1783/3742 [09:54<10:40,  3.06it/s]\u001b[A\n",
            "Iteration:  48% 1784/3742 [09:54<10:39,  3.06it/s]\u001b[A\n",
            "Iteration:  48% 1785/3742 [09:54<10:37,  3.07it/s]\u001b[A\n",
            "Iteration:  48% 1786/3742 [09:55<10:36,  3.07it/s]\u001b[A\n",
            "Iteration:  48% 1787/3742 [09:55<10:35,  3.07it/s]\u001b[A\n",
            "Iteration:  48% 1788/3742 [09:55<10:35,  3.08it/s]\u001b[A\n",
            "Iteration:  48% 1789/3742 [09:56<10:35,  3.07it/s]\u001b[A\n",
            "Iteration:  48% 1790/3742 [09:56<10:34,  3.08it/s]\u001b[A\n",
            "Iteration:  48% 1791/3742 [09:56<10:33,  3.08it/s]\u001b[A\n",
            "Iteration:  48% 1792/3742 [09:57<10:33,  3.08it/s]\u001b[A\n",
            "Iteration:  48% 1793/3742 [09:57<10:32,  3.08it/s]\u001b[A\n",
            "Iteration:  48% 1794/3742 [09:57<10:32,  3.08it/s]\u001b[A\n",
            "Iteration:  48% 1795/3742 [09:57<10:32,  3.08it/s]\u001b[A\n",
            "Iteration:  48% 1796/3742 [09:58<10:32,  3.08it/s]\u001b[A\n",
            "Iteration:  48% 1797/3742 [09:58<10:32,  3.07it/s]\u001b[A\n",
            "Iteration:  48% 1798/3742 [09:58<10:33,  3.07it/s]\u001b[A\n",
            "Iteration:  48% 1799/3742 [09:59<10:33,  3.07it/s]\u001b[A\n",
            "Iteration:  48% 1800/3742 [09:59<10:32,  3.07it/s]\u001b[A\n",
            "Iteration:  48% 1801/3742 [09:59<10:32,  3.07it/s]\u001b[A\n",
            "Iteration:  48% 1802/3742 [10:00<10:31,  3.07it/s]\u001b[A\n",
            "Iteration:  48% 1803/3742 [10:00<10:30,  3.08it/s]\u001b[A\n",
            "Iteration:  48% 1804/3742 [10:00<10:29,  3.08it/s]\u001b[A\n",
            "Iteration:  48% 1805/3742 [10:01<10:29,  3.08it/s]\u001b[A\n",
            "Iteration:  48% 1806/3742 [10:01<10:28,  3.08it/s]\u001b[A\n",
            "Iteration:  48% 1807/3742 [10:01<10:27,  3.08it/s]\u001b[A\n",
            "Iteration:  48% 1808/3742 [10:02<10:27,  3.08it/s]\u001b[A\n",
            "Iteration:  48% 1809/3742 [10:02<10:28,  3.08it/s]\u001b[A\n",
            "Iteration:  48% 1810/3742 [10:02<10:27,  3.08it/s]\u001b[A\n",
            "Iteration:  48% 1811/3742 [10:03<10:26,  3.08it/s]\u001b[A\n",
            "Iteration:  48% 1812/3742 [10:03<10:26,  3.08it/s]\u001b[A\n",
            "Iteration:  48% 1813/3742 [10:03<10:26,  3.08it/s]\u001b[A\n",
            "Iteration:  48% 1814/3742 [10:04<10:25,  3.08it/s]\u001b[A\n",
            "Iteration:  49% 1815/3742 [10:04<10:26,  3.08it/s]\u001b[A\n",
            "Iteration:  49% 1816/3742 [10:04<10:25,  3.08it/s]\u001b[A\n",
            "Iteration:  49% 1817/3742 [10:05<10:25,  3.08it/s]\u001b[A\n",
            "Iteration:  49% 1818/3742 [10:05<10:24,  3.08it/s]\u001b[A\n",
            "Iteration:  49% 1819/3742 [10:05<10:24,  3.08it/s]\u001b[A\n",
            "Iteration:  49% 1820/3742 [10:06<10:24,  3.08it/s]\u001b[A\n",
            "Iteration:  49% 1821/3742 [10:06<10:24,  3.08it/s]\u001b[A\n",
            "Iteration:  49% 1822/3742 [10:06<10:24,  3.07it/s]\u001b[A\n",
            "Iteration:  49% 1823/3742 [10:07<10:25,  3.07it/s]\u001b[A\n",
            "Iteration:  49% 1824/3742 [10:07<10:24,  3.07it/s]\u001b[A\n",
            "Iteration:  49% 1825/3742 [10:07<10:23,  3.07it/s]\u001b[A\n",
            "Iteration:  49% 1826/3742 [10:08<10:23,  3.07it/s]\u001b[A\n",
            "Iteration:  49% 1827/3742 [10:08<10:22,  3.07it/s]\u001b[A\n",
            "Iteration:  49% 1828/3742 [10:08<10:22,  3.07it/s]\u001b[A\n",
            "Iteration:  49% 1829/3742 [10:09<10:21,  3.08it/s]\u001b[A\n",
            "Iteration:  49% 1830/3742 [10:09<10:21,  3.07it/s]\u001b[A\n",
            "Iteration:  49% 1831/3742 [10:09<10:20,  3.08it/s]\u001b[A\n",
            "Iteration:  49% 1832/3742 [10:10<10:20,  3.08it/s]\u001b[A\n",
            "Iteration:  49% 1833/3742 [10:10<10:19,  3.08it/s]\u001b[A\n",
            "Iteration:  49% 1834/3742 [10:10<10:19,  3.08it/s]\u001b[A\n",
            "Iteration:  49% 1835/3742 [10:10<10:21,  3.07it/s]\u001b[A\n",
            "Iteration:  49% 1836/3742 [10:11<10:20,  3.07it/s]\u001b[A\n",
            "Iteration:  49% 1837/3742 [10:11<10:20,  3.07it/s]\u001b[A\n",
            "Iteration:  49% 1838/3742 [10:11<10:19,  3.08it/s]\u001b[A\n",
            "Iteration:  49% 1839/3742 [10:12<10:19,  3.07it/s]\u001b[A\n",
            "Iteration:  49% 1840/3742 [10:12<10:19,  3.07it/s]\u001b[A\n",
            "Iteration:  49% 1841/3742 [10:12<10:23,  3.05it/s]\u001b[A\n",
            "Iteration:  49% 1842/3742 [10:13<10:23,  3.05it/s]\u001b[A\n",
            "Iteration:  49% 1843/3742 [10:13<10:24,  3.04it/s]\u001b[A\n",
            "Iteration:  49% 1844/3742 [10:13<10:22,  3.05it/s]\u001b[A\n",
            "Iteration:  49% 1845/3742 [10:14<10:20,  3.06it/s]\u001b[A\n",
            "Iteration:  49% 1846/3742 [10:14<10:18,  3.07it/s]\u001b[A\n",
            "Iteration:  49% 1847/3742 [10:14<10:17,  3.07it/s]\u001b[A\n",
            "Iteration:  49% 1848/3742 [10:15<10:16,  3.07it/s]\u001b[A\n",
            "Iteration:  49% 1849/3742 [10:15<10:15,  3.08it/s]\u001b[A\n",
            "Iteration:  49% 1850/3742 [10:15<10:14,  3.08it/s]\u001b[A\n",
            "Iteration:  49% 1851/3742 [10:16<10:14,  3.08it/s]\u001b[A\n",
            "Iteration:  49% 1852/3742 [10:16<10:13,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1853/3742 [10:16<10:13,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1854/3742 [10:17<10:13,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1855/3742 [10:17<10:12,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1856/3742 [10:17<10:12,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1857/3742 [10:18<10:13,  3.07it/s]\u001b[A\n",
            "Iteration:  50% 1858/3742 [10:18<10:13,  3.07it/s]\u001b[A\n",
            "Iteration:  50% 1859/3742 [10:18<10:12,  3.07it/s]\u001b[A\n",
            "Iteration:  50% 1860/3742 [10:19<10:11,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1861/3742 [10:19<10:11,  3.07it/s]\u001b[A\n",
            "Iteration:  50% 1862/3742 [10:19<10:10,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1863/3742 [10:20<10:10,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1864/3742 [10:20<10:10,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1865/3742 [10:20<10:09,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1866/3742 [10:21<10:11,  3.07it/s]\u001b[A\n",
            "Iteration:  50% 1867/3742 [10:21<10:10,  3.07it/s]\u001b[A\n",
            "Iteration:  50% 1868/3742 [10:21<10:09,  3.07it/s]\u001b[A\n",
            "Iteration:  50% 1869/3742 [10:22<10:09,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1870/3742 [10:22<10:08,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1871/3742 [10:22<10:07,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1872/3742 [10:23<10:06,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1873/3742 [10:23<10:06,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1874/3742 [10:23<10:06,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1875/3742 [10:24<10:05,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1876/3742 [10:24<10:06,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1877/3742 [10:24<10:05,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1878/3742 [10:24<10:04,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1879/3742 [10:25<10:04,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1880/3742 [10:25<10:04,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1881/3742 [10:25<10:03,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1882/3742 [10:26<10:03,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1883/3742 [10:26<10:03,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1884/3742 [10:26<10:02,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1885/3742 [10:27<10:02,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1886/3742 [10:27<10:02,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1887/3742 [10:27<10:03,  3.08it/s]\u001b[A\n",
            "Iteration:  50% 1888/3742 [10:28<10:02,  3.07it/s]\u001b[A\n",
            "Iteration:  50% 1889/3742 [10:28<10:02,  3.07it/s]\u001b[A\n",
            "Iteration:  51% 1890/3742 [10:28<10:01,  3.08it/s]\u001b[A\n",
            "Iteration:  51% 1891/3742 [10:29<10:02,  3.07it/s]\u001b[A\n",
            "Iteration:  51% 1892/3742 [10:29<10:01,  3.07it/s]\u001b[A\n",
            "Iteration:  51% 1893/3742 [10:29<10:01,  3.07it/s]\u001b[A\n",
            "Iteration:  51% 1894/3742 [10:30<10:00,  3.08it/s]\u001b[A\n",
            "Iteration:  51% 1895/3742 [10:30<10:00,  3.08it/s]\u001b[A\n",
            "Iteration:  51% 1896/3742 [10:30<09:59,  3.08it/s]\u001b[A\n",
            "Iteration:  51% 1897/3742 [10:31<09:59,  3.08it/s]\u001b[A\n",
            "Iteration:  51% 1898/3742 [10:31<09:58,  3.08it/s]\u001b[A\n",
            "Iteration:  51% 1899/3742 [10:31<09:59,  3.07it/s]\u001b[A\n",
            "Iteration:  51% 1900/3742 [10:32<09:59,  3.07it/s]\u001b[A\n",
            "Iteration:  51% 1901/3742 [10:32<09:58,  3.08it/s]\u001b[A\n",
            "Iteration:  51% 1902/3742 [10:32<09:58,  3.07it/s]\u001b[A\n",
            "Iteration:  51% 1903/3742 [10:33<09:58,  3.07it/s]\u001b[A\n",
            "Iteration:  51% 1904/3742 [10:33<09:57,  3.08it/s]\u001b[A\n",
            "Iteration:  51% 1905/3742 [10:33<09:56,  3.08it/s]\u001b[A\n",
            "Iteration:  51% 1906/3742 [10:34<09:56,  3.08it/s]\u001b[A\n",
            "Iteration:  51% 1907/3742 [10:34<09:56,  3.07it/s]\u001b[A\n",
            "Iteration:  51% 1908/3742 [10:34<09:56,  3.08it/s]\u001b[A\n",
            "Iteration:  51% 1909/3742 [10:35<09:56,  3.07it/s]\u001b[A\n",
            "Iteration:  51% 1910/3742 [10:35<09:55,  3.08it/s]\u001b[A\n",
            "Iteration:  51% 1911/3742 [10:35<09:54,  3.08it/s]\u001b[A\n",
            "Iteration:  51% 1912/3742 [10:36<09:54,  3.08it/s]\u001b[A\n",
            "Iteration:  51% 1913/3742 [10:36<09:54,  3.08it/s]\u001b[A\n",
            "Iteration:  51% 1914/3742 [10:36<09:53,  3.08it/s]\u001b[A\n",
            "Iteration:  51% 1915/3742 [10:37<09:56,  3.06it/s]\u001b[A\n",
            "Iteration:  51% 1916/3742 [10:37<09:58,  3.05it/s]\u001b[A\n",
            "Iteration:  51% 1917/3742 [10:37<09:56,  3.06it/s]\u001b[A\n",
            "Iteration:  51% 1918/3742 [10:37<09:54,  3.07it/s]\u001b[A\n",
            "Iteration:  51% 1919/3742 [10:38<09:53,  3.07it/s]\u001b[A\n",
            "Iteration:  51% 1920/3742 [10:38<09:53,  3.07it/s]\u001b[A\n",
            "Iteration:  51% 1921/3742 [10:38<09:52,  3.07it/s]\u001b[A\n",
            "Iteration:  51% 1922/3742 [10:39<09:52,  3.07it/s]\u001b[A\n",
            "Iteration:  51% 1923/3742 [10:39<09:51,  3.08it/s]\u001b[A\n",
            "Iteration:  51% 1924/3742 [10:39<09:50,  3.08it/s]\u001b[A\n",
            "Iteration:  51% 1925/3742 [10:40<09:50,  3.08it/s]\u001b[A\n",
            "Iteration:  51% 1926/3742 [10:40<09:49,  3.08it/s]\u001b[A\n",
            "Iteration:  51% 1927/3742 [10:40<09:48,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1928/3742 [10:41<09:49,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1929/3742 [10:41<09:49,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1930/3742 [10:41<09:48,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1931/3742 [10:42<09:48,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1932/3742 [10:42<09:47,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1933/3742 [10:42<09:47,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1934/3742 [10:43<09:46,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1935/3742 [10:43<09:46,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1936/3742 [10:43<09:45,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1937/3742 [10:44<09:45,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1938/3742 [10:44<09:45,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1939/3742 [10:44<09:45,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1940/3742 [10:45<09:44,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1941/3742 [10:45<09:43,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1942/3742 [10:45<09:44,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1943/3742 [10:46<09:44,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1944/3742 [10:46<09:43,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1945/3742 [10:46<09:43,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1946/3742 [10:47<09:43,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1947/3742 [10:47<09:42,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1948/3742 [10:47<09:42,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1949/3742 [10:48<09:41,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1950/3742 [10:48<09:41,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1951/3742 [10:48<09:42,  3.07it/s]\u001b[A\n",
            "Iteration:  52% 1952/3742 [10:49<09:42,  3.07it/s]\u001b[A\n",
            "Iteration:  52% 1953/3742 [10:49<09:41,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1954/3742 [10:49<09:41,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1955/3742 [10:50<09:41,  3.07it/s]\u001b[A\n",
            "Iteration:  52% 1956/3742 [10:50<09:40,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1957/3742 [10:50<09:39,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1958/3742 [10:50<09:39,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1959/3742 [10:51<09:38,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1960/3742 [10:51<09:38,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1961/3742 [10:51<09:38,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1962/3742 [10:52<09:37,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1963/3742 [10:52<09:38,  3.08it/s]\u001b[A\n",
            "Iteration:  52% 1964/3742 [10:52<09:38,  3.07it/s]\u001b[A\n",
            "Iteration:  53% 1965/3742 [10:53<09:40,  3.06it/s]\u001b[A\n",
            "Iteration:  53% 1966/3742 [10:53<09:40,  3.06it/s]\u001b[A\n",
            "Iteration:  53% 1967/3742 [10:53<09:39,  3.06it/s]\u001b[A\n",
            "Iteration:  53% 1968/3742 [10:54<09:38,  3.06it/s]\u001b[A\n",
            "Iteration:  53% 1969/3742 [10:54<09:37,  3.07it/s]\u001b[A\n",
            "Iteration:  53% 1970/3742 [10:54<09:36,  3.08it/s]\u001b[A\n",
            "Iteration:  53% 1971/3742 [10:55<09:35,  3.08it/s]\u001b[A\n",
            "Iteration:  53% 1972/3742 [10:55<09:35,  3.08it/s]\u001b[A\n",
            "Iteration:  53% 1973/3742 [10:55<09:36,  3.07it/s]\u001b[A\n",
            "Iteration:  53% 1974/3742 [10:56<09:37,  3.06it/s]\u001b[A\n",
            "Iteration:  53% 1975/3742 [10:56<09:36,  3.06it/s]\u001b[A\n",
            "Iteration:  53% 1976/3742 [10:56<09:35,  3.07it/s]\u001b[A\n",
            "Iteration:  53% 1977/3742 [10:57<09:35,  3.07it/s]\u001b[A\n",
            "Iteration:  53% 1978/3742 [10:57<09:36,  3.06it/s]\u001b[A\n",
            "Iteration:  53% 1979/3742 [10:57<09:35,  3.07it/s]\u001b[A\n",
            "Iteration:  53% 1980/3742 [10:58<09:34,  3.07it/s]\u001b[A\n",
            "Iteration:  53% 1981/3742 [10:58<09:33,  3.07it/s]\u001b[A\n",
            "Iteration:  53% 1982/3742 [10:58<09:32,  3.08it/s]\u001b[A\n",
            "Iteration:  53% 1983/3742 [10:59<09:31,  3.08it/s]\u001b[A\n",
            "Iteration:  53% 1984/3742 [10:59<09:31,  3.08it/s]\u001b[A\n",
            "Iteration:  53% 1985/3742 [10:59<09:31,  3.08it/s]\u001b[A\n",
            "Iteration:  53% 1986/3742 [11:00<09:30,  3.08it/s]\u001b[A\n",
            "Iteration:  53% 1987/3742 [11:00<09:29,  3.08it/s]\u001b[A\n",
            "Iteration:  53% 1988/3742 [11:00<09:30,  3.08it/s]\u001b[A\n",
            "Iteration:  53% 1989/3742 [11:01<09:29,  3.08it/s]\u001b[A\n",
            "Iteration:  53% 1990/3742 [11:01<09:29,  3.08it/s]\u001b[A\n",
            "Iteration:  53% 1991/3742 [11:01<09:28,  3.08it/s]\u001b[A\n",
            "Iteration:  53% 1992/3742 [11:02<09:27,  3.08it/s]\u001b[A\n",
            "Iteration:  53% 1993/3742 [11:02<09:27,  3.08it/s]\u001b[A\n",
            "Iteration:  53% 1994/3742 [11:02<09:28,  3.08it/s]\u001b[A\n",
            "Iteration:  53% 1995/3742 [11:03<09:27,  3.08it/s]\u001b[A\n",
            "Iteration:  53% 1996/3742 [11:03<09:27,  3.08it/s]\u001b[A\n",
            "Iteration:  53% 1997/3742 [11:03<09:26,  3.08it/s]\u001b[A\n",
            "Iteration:  53% 1998/3742 [11:03<09:26,  3.08it/s]\u001b[A\n",
            "Iteration:  53% 1999/3742 [11:04<09:25,  3.08it/s]\u001b[A03/05/2020 07:01:49 - INFO - transformers.configuration_utils -   Configuration saved in output_roberta_US/checkpoint-2000/config.json\n",
            "03/05/2020 07:01:50 - INFO - transformers.modeling_utils -   Model weights saved in output_roberta_US/checkpoint-2000/pytorch_model.bin\n",
            "03/05/2020 07:01:50 - INFO - __main__ -   Saving model checkpoint to output_roberta_US/checkpoint-2000\n",
            "03/05/2020 07:01:54 - INFO - __main__ -   Saving optimizer and scheduler states to output_roberta_US/checkpoint-2000\n",
            "\n",
            "Iteration:  53% 2000/3742 [11:09<48:33,  1.67s/it]\u001b[A\n",
            "Iteration:  53% 2001/3742 [11:09<37:01,  1.28s/it]\u001b[A\n",
            "Iteration:  54% 2002/3742 [11:09<28:46,  1.01it/s]\u001b[A\n",
            "Iteration:  54% 2003/3742 [11:10<22:58,  1.26it/s]\u001b[A\n",
            "Iteration:  54% 2004/3742 [11:10<18:54,  1.53it/s]\u001b[A\n",
            "Iteration:  54% 2005/3742 [11:10<16:03,  1.80it/s]\u001b[A\n",
            "Iteration:  54% 2006/3742 [11:11<14:02,  2.06it/s]\u001b[A\n",
            "Iteration:  54% 2007/3742 [11:11<12:39,  2.28it/s]\u001b[A\n",
            "Iteration:  54% 2008/3742 [11:11<11:40,  2.47it/s]\u001b[A\n",
            "Iteration:  54% 2009/3742 [11:12<10:58,  2.63it/s]\u001b[A\n",
            "Iteration:  54% 2010/3742 [11:12<10:29,  2.75it/s]\u001b[A\n",
            "Iteration:  54% 2011/3742 [11:12<10:09,  2.84it/s]\u001b[A\n",
            "Iteration:  54% 2012/3742 [11:13<09:57,  2.89it/s]\u001b[A\n",
            "Iteration:  54% 2013/3742 [11:13<09:49,  2.94it/s]\u001b[A\n",
            "Iteration:  54% 2014/3742 [11:13<09:42,  2.97it/s]\u001b[A\n",
            "Iteration:  54% 2015/3742 [11:14<09:38,  2.99it/s]\u001b[A\n",
            "Iteration:  54% 2016/3742 [11:14<09:33,  3.01it/s]\u001b[A\n",
            "Iteration:  54% 2017/3742 [11:14<09:28,  3.03it/s]\u001b[A\n",
            "Iteration:  54% 2018/3742 [11:15<09:25,  3.05it/s]\u001b[A\n",
            "Iteration:  54% 2019/3742 [11:15<09:23,  3.06it/s]\u001b[A\n",
            "Iteration:  54% 2020/3742 [11:15<09:24,  3.05it/s]\u001b[A\n",
            "Iteration:  54% 2021/3742 [11:16<09:21,  3.06it/s]\u001b[A\n",
            "Iteration:  54% 2022/3742 [11:16<09:21,  3.07it/s]\u001b[A\n",
            "Iteration:  54% 2023/3742 [11:16<09:19,  3.07it/s]\u001b[A\n",
            "Iteration:  54% 2024/3742 [11:16<09:19,  3.07it/s]\u001b[A\n",
            "Iteration:  54% 2025/3742 [11:17<09:19,  3.07it/s]\u001b[A\n",
            "Iteration:  54% 2026/3742 [11:17<09:19,  3.07it/s]\u001b[A\n",
            "Iteration:  54% 2027/3742 [11:17<09:18,  3.07it/s]\u001b[A\n",
            "Iteration:  54% 2028/3742 [11:18<09:20,  3.06it/s]\u001b[A\n",
            "Iteration:  54% 2029/3742 [11:18<09:18,  3.06it/s]\u001b[A\n",
            "Iteration:  54% 2030/3742 [11:18<09:17,  3.07it/s]\u001b[A\n",
            "Iteration:  54% 2031/3742 [11:19<09:16,  3.08it/s]\u001b[A\n",
            "Iteration:  54% 2032/3742 [11:19<09:15,  3.08it/s]\u001b[A\n",
            "Iteration:  54% 2033/3742 [11:19<09:14,  3.08it/s]\u001b[A\n",
            "Iteration:  54% 2034/3742 [11:20<09:13,  3.08it/s]\u001b[A\n",
            "Iteration:  54% 2035/3742 [11:20<09:14,  3.08it/s]\u001b[A\n",
            "Iteration:  54% 2036/3742 [11:20<09:13,  3.08it/s]\u001b[A\n",
            "Iteration:  54% 2037/3742 [11:21<09:13,  3.08it/s]\u001b[A\n",
            "Iteration:  54% 2038/3742 [11:21<09:12,  3.08it/s]\u001b[A\n",
            "Iteration:  54% 2039/3742 [11:21<09:12,  3.08it/s]\u001b[A\n",
            "Iteration:  55% 2040/3742 [11:22<09:11,  3.08it/s]\u001b[A\n",
            "Iteration:  55% 2041/3742 [11:22<09:11,  3.09it/s]\u001b[A\n",
            "Iteration:  55% 2042/3742 [11:22<09:11,  3.08it/s]\u001b[A\n",
            "Iteration:  55% 2043/3742 [11:23<09:12,  3.07it/s]\u001b[A\n",
            "Iteration:  55% 2044/3742 [11:23<09:12,  3.07it/s]\u001b[A\n",
            "Iteration:  55% 2045/3742 [11:23<09:12,  3.07it/s]\u001b[A\n",
            "Iteration:  55% 2046/3742 [11:24<09:11,  3.08it/s]\u001b[A\n",
            "Iteration:  55% 2047/3742 [11:24<09:11,  3.07it/s]\u001b[A\n",
            "Iteration:  55% 2048/3742 [11:24<09:11,  3.07it/s]\u001b[A\n",
            "Iteration:  55% 2049/3742 [11:25<09:10,  3.08it/s]\u001b[A\n",
            "Iteration:  55% 2050/3742 [11:25<09:09,  3.08it/s]\u001b[A\n",
            "Iteration:  55% 2051/3742 [11:25<09:10,  3.07it/s]\u001b[A\n",
            "Iteration:  55% 2052/3742 [11:26<09:10,  3.07it/s]\u001b[A\n",
            "Iteration:  55% 2053/3742 [11:26<09:09,  3.07it/s]\u001b[A\n",
            "Iteration:  55% 2054/3742 [11:26<09:13,  3.05it/s]\u001b[A\n",
            "Iteration:  55% 2055/3742 [11:27<09:11,  3.06it/s]\u001b[A\n",
            "Iteration:  55% 2056/3742 [11:27<09:09,  3.07it/s]\u001b[A\n",
            "Iteration:  55% 2057/3742 [11:27<09:09,  3.06it/s]\u001b[A\n",
            "Iteration:  55% 2058/3742 [11:28<09:10,  3.06it/s]\u001b[A\n",
            "Iteration:  55% 2059/3742 [11:28<09:08,  3.07it/s]\u001b[A\n",
            "Iteration:  55% 2060/3742 [11:28<09:07,  3.07it/s]\u001b[A\n",
            "Iteration:  55% 2061/3742 [11:29<09:07,  3.07it/s]\u001b[A\n",
            "Iteration:  55% 2062/3742 [11:29<09:06,  3.07it/s]\u001b[A\n",
            "Iteration:  55% 2063/3742 [11:29<09:05,  3.08it/s]\u001b[A\n",
            "Iteration:  55% 2064/3742 [11:30<09:05,  3.08it/s]\u001b[A\n",
            "Iteration:  55% 2065/3742 [11:30<09:04,  3.08it/s]\u001b[A\n",
            "Iteration:  55% 2066/3742 [11:30<09:04,  3.08it/s]\u001b[A\n",
            "Iteration:  55% 2067/3742 [11:30<09:03,  3.08it/s]\u001b[A\n",
            "Iteration:  55% 2068/3742 [11:31<09:04,  3.07it/s]\u001b[A\n",
            "Iteration:  55% 2069/3742 [11:31<09:04,  3.07it/s]\u001b[A\n",
            "Iteration:  55% 2070/3742 [11:31<09:03,  3.07it/s]\u001b[A\n",
            "Iteration:  55% 2071/3742 [11:32<09:04,  3.07it/s]\u001b[A\n",
            "Iteration:  55% 2072/3742 [11:32<09:03,  3.07it/s]\u001b[A\n",
            "Iteration:  55% 2073/3742 [11:32<09:02,  3.08it/s]\u001b[A\n",
            "Iteration:  55% 2074/3742 [11:33<09:02,  3.07it/s]\u001b[A\n",
            "Iteration:  55% 2075/3742 [11:33<09:01,  3.08it/s]\u001b[A\n",
            "Iteration:  55% 2076/3742 [11:33<09:01,  3.08it/s]\u001b[A\n",
            "Iteration:  56% 2077/3742 [11:34<09:00,  3.08it/s]\u001b[A\n",
            "Iteration:  56% 2078/3742 [11:34<09:00,  3.08it/s]\u001b[A\n",
            "Iteration:  56% 2079/3742 [11:34<08:59,  3.08it/s]\u001b[A\n",
            "Iteration:  56% 2080/3742 [11:35<08:59,  3.08it/s]\u001b[A\n",
            "Iteration:  56% 2081/3742 [11:35<08:59,  3.08it/s]\u001b[A\n",
            "Iteration:  56% 2082/3742 [11:35<08:59,  3.08it/s]\u001b[A\n",
            "Iteration:  56% 2083/3742 [11:36<08:58,  3.08it/s]\u001b[A\n",
            "Iteration:  56% 2084/3742 [11:36<09:00,  3.06it/s]\u001b[A\n",
            "Iteration:  56% 2085/3742 [11:36<09:02,  3.05it/s]\u001b[A\n",
            "Iteration:  56% 2086/3742 [11:37<09:03,  3.05it/s]\u001b[A\n",
            "Iteration:  56% 2087/3742 [11:37<09:03,  3.05it/s]\u001b[A\n",
            "Iteration:  56% 2088/3742 [11:37<09:01,  3.06it/s]\u001b[A\n",
            "Iteration:  56% 2089/3742 [11:38<08:59,  3.06it/s]\u001b[A\n",
            "Iteration:  56% 2090/3742 [11:38<08:58,  3.07it/s]\u001b[A\n",
            "Iteration:  56% 2091/3742 [11:38<08:57,  3.07it/s]\u001b[A\n",
            "Iteration:  56% 2092/3742 [11:39<08:59,  3.06it/s]\u001b[A\n",
            "Iteration:  56% 2093/3742 [11:39<08:58,  3.06it/s]\u001b[A\n",
            "Iteration:  56% 2094/3742 [11:39<08:57,  3.07it/s]\u001b[A\n",
            "Iteration:  56% 2095/3742 [11:40<08:56,  3.07it/s]\u001b[A\n",
            "Iteration:  56% 2096/3742 [11:40<08:56,  3.07it/s]\u001b[A\n",
            "Iteration:  56% 2097/3742 [11:40<08:55,  3.07it/s]\u001b[A\n",
            "Iteration:  56% 2098/3742 [11:41<08:54,  3.08it/s]\u001b[A\n",
            "Iteration:  56% 2099/3742 [11:41<08:53,  3.08it/s]\u001b[A\n",
            "Iteration:  56% 2100/3742 [11:41<08:53,  3.08it/s]\u001b[A\n",
            "Iteration:  56% 2101/3742 [11:42<08:53,  3.08it/s]\u001b[A\n",
            "Iteration:  56% 2102/3742 [11:42<08:52,  3.08it/s]\u001b[A\n",
            "Iteration:  56% 2103/3742 [11:42<08:51,  3.08it/s]\u001b[A\n",
            "Iteration:  56% 2104/3742 [11:43<08:51,  3.08it/s]\u001b[A\n",
            "Iteration:  56% 2105/3742 [11:43<08:51,  3.08it/s]\u001b[A\n",
            "Iteration:  56% 2106/3742 [11:43<08:51,  3.08it/s]\u001b[A\n",
            "Iteration:  56% 2107/3742 [11:43<08:50,  3.08it/s]\u001b[A\n",
            "Iteration:  56% 2108/3742 [11:44<08:50,  3.08it/s]\u001b[A\n",
            "Iteration:  56% 2109/3742 [11:44<08:50,  3.08it/s]\u001b[A\n",
            "Iteration:  56% 2110/3742 [11:44<08:51,  3.07it/s]\u001b[A\n",
            "Iteration:  56% 2111/3742 [11:45<08:50,  3.07it/s]\u001b[A\n",
            "Iteration:  56% 2112/3742 [11:45<08:50,  3.07it/s]\u001b[A\n",
            "Iteration:  56% 2113/3742 [11:45<08:53,  3.05it/s]\u001b[A\n",
            "Iteration:  56% 2114/3742 [11:46<08:51,  3.06it/s]\u001b[A\n",
            "Iteration:  57% 2115/3742 [11:46<08:50,  3.07it/s]\u001b[A\n",
            "Iteration:  57% 2116/3742 [11:46<08:49,  3.07it/s]\u001b[A\n",
            "Iteration:  57% 2117/3742 [11:47<08:50,  3.06it/s]\u001b[A\n",
            "Iteration:  57% 2118/3742 [11:47<08:49,  3.07it/s]\u001b[A\n",
            "Iteration:  57% 2119/3742 [11:47<08:49,  3.07it/s]\u001b[A\n",
            "Iteration:  57% 2120/3742 [11:48<08:49,  3.06it/s]\u001b[A\n",
            "Iteration:  57% 2121/3742 [11:48<08:49,  3.06it/s]\u001b[A\n",
            "Iteration:  57% 2122/3742 [11:48<08:47,  3.07it/s]\u001b[A\n",
            "Iteration:  57% 2123/3742 [11:49<08:47,  3.07it/s]\u001b[A\n",
            "Iteration:  57% 2124/3742 [11:49<08:47,  3.07it/s]\u001b[A\n",
            "Iteration:  57% 2125/3742 [11:49<08:46,  3.07it/s]\u001b[A\n",
            "Iteration:  57% 2126/3742 [11:50<08:47,  3.07it/s]\u001b[A\n",
            "Iteration:  57% 2127/3742 [11:50<08:46,  3.07it/s]\u001b[A\n",
            "Iteration:  57% 2128/3742 [11:50<08:44,  3.08it/s]\u001b[A\n",
            "Iteration:  57% 2129/3742 [11:51<08:44,  3.08it/s]\u001b[A\n",
            "Iteration:  57% 2130/3742 [11:51<08:43,  3.08it/s]\u001b[A\n",
            "Iteration:  57% 2131/3742 [11:51<08:42,  3.08it/s]\u001b[A\n",
            "Iteration:  57% 2132/3742 [11:52<08:43,  3.08it/s]\u001b[A\n",
            "Iteration:  57% 2133/3742 [11:52<08:42,  3.08it/s]\u001b[A\n",
            "Iteration:  57% 2134/3742 [11:52<08:41,  3.08it/s]\u001b[A\n",
            "Iteration:  57% 2135/3742 [11:53<08:46,  3.05it/s]\u001b[A\n",
            "Iteration:  57% 2136/3742 [11:53<08:48,  3.04it/s]\u001b[A\n",
            "Iteration:  57% 2137/3742 [11:53<08:46,  3.05it/s]\u001b[A\n",
            "Iteration:  57% 2138/3742 [11:54<08:44,  3.06it/s]\u001b[A\n",
            "Iteration:  57% 2139/3742 [11:54<08:44,  3.05it/s]\u001b[A\n",
            "Iteration:  57% 2140/3742 [11:54<08:42,  3.06it/s]\u001b[A\n",
            "Iteration:  57% 2141/3742 [11:55<08:40,  3.07it/s]\u001b[A\n",
            "Iteration:  57% 2142/3742 [11:55<08:40,  3.08it/s]\u001b[A\n",
            "Iteration:  57% 2143/3742 [11:55<08:39,  3.08it/s]\u001b[A\n",
            "Iteration:  57% 2144/3742 [11:56<08:39,  3.08it/s]\u001b[A\n",
            "Iteration:  57% 2145/3742 [11:56<08:39,  3.08it/s]\u001b[A\n",
            "Iteration:  57% 2146/3742 [11:56<08:38,  3.08it/s]\u001b[A\n",
            "Iteration:  57% 2147/3742 [11:57<08:39,  3.07it/s]\u001b[A\n",
            "Iteration:  57% 2148/3742 [11:57<08:38,  3.07it/s]\u001b[A\n",
            "Iteration:  57% 2149/3742 [11:57<08:37,  3.08it/s]\u001b[A\n",
            "Iteration:  57% 2150/3742 [11:58<08:37,  3.08it/s]\u001b[A\n",
            "Iteration:  57% 2151/3742 [11:58<08:37,  3.07it/s]\u001b[A\n",
            "Iteration:  58% 2152/3742 [11:58<08:37,  3.08it/s]\u001b[A\n",
            "Iteration:  58% 2153/3742 [11:58<08:38,  3.06it/s]\u001b[A\n",
            "Iteration:  58% 2154/3742 [11:59<08:37,  3.07it/s]\u001b[A\n",
            "Iteration:  58% 2155/3742 [11:59<08:36,  3.07it/s]\u001b[A\n",
            "Iteration:  58% 2156/3742 [11:59<08:36,  3.07it/s]\u001b[A\n",
            "Iteration:  58% 2157/3742 [12:00<08:36,  3.07it/s]\u001b[A\n",
            "Iteration:  58% 2158/3742 [12:00<08:35,  3.07it/s]\u001b[A\n",
            "Iteration:  58% 2159/3742 [12:00<08:34,  3.08it/s]\u001b[A\n",
            "Iteration:  58% 2160/3742 [12:01<08:36,  3.07it/s]\u001b[A\n",
            "Iteration:  58% 2161/3742 [12:01<08:37,  3.06it/s]\u001b[A\n",
            "Iteration:  58% 2162/3742 [12:01<08:35,  3.07it/s]\u001b[A\n",
            "Iteration:  58% 2163/3742 [12:02<08:34,  3.07it/s]\u001b[A\n",
            "Iteration:  58% 2164/3742 [12:02<08:34,  3.07it/s]\u001b[A\n",
            "Iteration:  58% 2165/3742 [12:02<08:34,  3.06it/s]\u001b[A\n",
            "Iteration:  58% 2166/3742 [12:03<08:33,  3.07it/s]\u001b[A\n",
            "Iteration:  58% 2167/3742 [12:03<08:32,  3.07it/s]\u001b[A\n",
            "Iteration:  58% 2168/3742 [12:03<08:32,  3.07it/s]\u001b[A\n",
            "Iteration:  58% 2169/3742 [12:04<08:31,  3.07it/s]\u001b[A\n",
            "Iteration:  58% 2170/3742 [12:04<08:31,  3.07it/s]\u001b[A\n",
            "Iteration:  58% 2171/3742 [12:04<08:30,  3.08it/s]\u001b[A\n",
            "Iteration:  58% 2172/3742 [12:05<08:30,  3.08it/s]\u001b[A\n",
            "Iteration:  58% 2173/3742 [12:05<08:29,  3.08it/s]\u001b[A\n",
            "Iteration:  58% 2174/3742 [12:05<08:28,  3.08it/s]\u001b[A\n",
            "Iteration:  58% 2175/3742 [12:06<08:28,  3.08it/s]\u001b[A\n",
            "Iteration:  58% 2176/3742 [12:06<08:29,  3.08it/s]\u001b[A\n",
            "Iteration:  58% 2177/3742 [12:06<08:28,  3.08it/s]\u001b[A\n",
            "Iteration:  58% 2178/3742 [12:07<08:30,  3.07it/s]\u001b[A\n",
            "Iteration:  58% 2179/3742 [12:07<08:29,  3.06it/s]\u001b[A\n",
            "Iteration:  58% 2180/3742 [12:07<08:28,  3.07it/s]\u001b[A\n",
            "Iteration:  58% 2181/3742 [12:08<08:29,  3.06it/s]\u001b[ATraceback (most recent call last):\n",
            "  File \"run_language_modelling.py\", line 799, in <module>\n",
            "    main()\n",
            "  File \"run_language_modelling.py\", line 749, in main\n",
            "    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
            "  File \"run_language_modelling.py\", line 365, in train\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/tensor.py\", line 195, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\", line 99, in backward\n",
            "    allow_unreachable=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R80-_g-xpiwx",
        "colab_type": "code",
        "outputId": "4073d3d6-2de7-48fa-b2fd-12452892b998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python run_language_modelling.py --output_dir=output_roberta_GB --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=gb_blog_train --do_eval --eval_data_file=gb_blog_test --mlm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/29/2020 07:27:25 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "02/29/2020 07:27:25 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "02/29/2020 07:27:25 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "02/29/2020 07:27:26 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "02/29/2020 07:27:26 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "02/29/2020 07:27:26 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "02/29/2020 07:27:45 - INFO - transformers.modeling_utils -   Weights of RobertaForMaskedLM not initialized from pretrained model: ['lm_head.decoder.bias']\n",
            "02/29/2020 07:27:48 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='gb_blog_test', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=True, mlm_probability=0.15, model_name_or_path='roberta-base', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='output_roberta_GB', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='gb_blog_train', warmup_steps=0, weight_decay=0.0)\n",
            "02/29/2020 07:27:48 - INFO - __main__ -   Creating features from dataset file at \n",
            "02/29/2020 07:28:08 - INFO - __main__ -   Saving features into cached file roberta_cached_lm_510_gb_blog_train\n",
            "02/29/2020 07:28:08 - INFO - __main__ -   ***** Running training *****\n",
            "02/29/2020 07:28:08 - INFO - __main__ -     Num examples = 10241\n",
            "02/29/2020 07:28:08 - INFO - __main__ -     Num Epochs = 1\n",
            "02/29/2020 07:28:08 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
            "02/29/2020 07:28:08 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "02/29/2020 07:28:08 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "02/29/2020 07:28:08 - INFO - __main__ -     Total optimization steps = 2561\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/2561 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/2561 [00:00<31:38,  1.35it/s]\u001b[A\n",
            "Iteration:   0% 2/2561 [00:01<31:16,  1.36it/s]\u001b[A\n",
            "Iteration:   0% 3/2561 [00:02<30:58,  1.38it/s]\u001b[A\n",
            "Iteration:   0% 4/2561 [00:02<30:54,  1.38it/s]\u001b[A\n",
            "Iteration:   0% 5/2561 [00:03<30:45,  1.39it/s]\u001b[A\n",
            "Iteration:   0% 6/2561 [00:04<30:34,  1.39it/s]\u001b[A\n",
            "Iteration:   0% 7/2561 [00:05<30:35,  1.39it/s]\u001b[A\n",
            "Iteration:   0% 8/2561 [00:05<30:31,  1.39it/s]\u001b[A\n",
            "Iteration:   0% 9/2561 [00:06<30:29,  1.39it/s]\u001b[A\n",
            "Iteration:   0% 10/2561 [00:07<30:18,  1.40it/s]\u001b[A\n",
            "Iteration:   0% 11/2561 [00:07<30:24,  1.40it/s]\u001b[A\n",
            "Iteration:   0% 12/2561 [00:08<30:22,  1.40it/s]\u001b[A\n",
            "Iteration:   1% 13/2561 [00:09<30:23,  1.40it/s]\u001b[A\n",
            "Iteration:   1% 14/2561 [00:10<30:16,  1.40it/s]\u001b[A\n",
            "Iteration:   1% 15/2561 [00:10<30:27,  1.39it/s]\u001b[A\n",
            "Iteration:   1% 16/2561 [00:11<30:22,  1.40it/s]\u001b[A\n",
            "Iteration:   1% 17/2561 [00:12<30:19,  1.40it/s]\u001b[A\n",
            "Iteration:   1% 18/2561 [00:12<30:12,  1.40it/s]\u001b[A\n",
            "Iteration:   1% 19/2561 [00:13<30:17,  1.40it/s]\u001b[A\n",
            "Iteration:   1% 20/2561 [00:14<30:17,  1.40it/s]\u001b[A\n",
            "Iteration:   1% 21/2561 [00:15<30:20,  1.40it/s]\u001b[A\n",
            "Iteration:   1% 22/2561 [00:15<30:35,  1.38it/s]\u001b[A\n",
            "Iteration:   1% 23/2561 [00:16<30:33,  1.38it/s]\u001b[A\n",
            "Iteration:   1% 24/2561 [00:17<30:26,  1.39it/s]\u001b[A\n",
            "Iteration:   1% 25/2561 [00:17<30:19,  1.39it/s]\u001b[A\n",
            "Iteration:   1% 26/2561 [00:18<30:30,  1.39it/s]\u001b[A\n",
            "Iteration:   1% 27/2561 [00:19<30:21,  1.39it/s]\u001b[A\n",
            "Iteration:   1% 28/2561 [00:20<30:22,  1.39it/s]\u001b[A\n",
            "Iteration:   1% 29/2561 [00:20<30:23,  1.39it/s]\u001b[A\n",
            "Iteration:   1% 30/2561 [00:21<30:22,  1.39it/s]\u001b[A\n",
            "Iteration:   1% 31/2561 [00:22<30:18,  1.39it/s]\u001b[A\n",
            "Iteration:   1% 32/2561 [00:22<30:16,  1.39it/s]\u001b[A\n",
            "Iteration:   1% 33/2561 [00:23<30:34,  1.38it/s]\u001b[A\n",
            "Iteration:   1% 34/2561 [00:24<30:29,  1.38it/s]\u001b[A\n",
            "Iteration:   1% 35/2561 [00:25<30:14,  1.39it/s]\u001b[A\n",
            "Iteration:   1% 36/2561 [00:25<30:21,  1.39it/s]\u001b[A\n",
            "Iteration:   1% 37/2561 [00:26<30:24,  1.38it/s]\u001b[A\n",
            "Iteration:   1% 38/2561 [00:27<31:54,  1.32it/s]\u001b[A\n",
            "Iteration:   2% 39/2561 [00:28<31:22,  1.34it/s]\u001b[A\n",
            "Iteration:   2% 40/2561 [00:28<31:19,  1.34it/s]\u001b[A\n",
            "Iteration:   2% 41/2561 [00:29<31:06,  1.35it/s]\u001b[A\n",
            "Iteration:   2% 42/2561 [00:30<30:49,  1.36it/s]\u001b[A\n",
            "Iteration:   2% 43/2561 [00:31<30:55,  1.36it/s]\u001b[A\n",
            "Iteration:   2% 44/2561 [00:31<30:45,  1.36it/s]\u001b[A\n",
            "Iteration:   2% 45/2561 [00:32<30:36,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 46/2561 [00:33<30:40,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 47/2561 [00:33<30:34,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 48/2561 [00:34<30:29,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 49/2561 [00:35<30:33,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 50/2561 [00:36<30:34,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 51/2561 [00:36<30:25,  1.38it/s]\u001b[A\n",
            "Iteration:   2% 52/2561 [00:37<30:35,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 53/2561 [00:38<30:30,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 54/2561 [00:39<30:42,  1.36it/s]\u001b[A\n",
            "Iteration:   2% 55/2561 [00:39<30:38,  1.36it/s]\u001b[A\n",
            "Iteration:   2% 56/2561 [00:40<30:28,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 57/2561 [00:41<30:30,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 58/2561 [00:42<30:32,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 59/2561 [00:42<30:16,  1.38it/s]\u001b[A\n",
            "Iteration:   2% 60/2561 [00:43<30:27,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 61/2561 [00:44<30:30,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 62/2561 [00:44<30:20,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 63/2561 [00:45<30:28,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 64/2561 [00:46<30:21,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 65/2561 [00:47<30:11,  1.38it/s]\u001b[A\n",
            "Iteration:   3% 66/2561 [00:47<30:19,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 67/2561 [00:48<30:22,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 68/2561 [00:49<30:08,  1.38it/s]\u001b[A\n",
            "Iteration:   3% 69/2561 [00:50<30:19,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 70/2561 [00:50<30:19,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 71/2561 [00:51<30:15,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 72/2561 [00:52<30:24,  1.36it/s]\u001b[A\n",
            "Iteration:   3% 73/2561 [00:52<30:25,  1.36it/s]\u001b[A\n",
            "Iteration:   3% 74/2561 [00:53<30:27,  1.36it/s]\u001b[A\n",
            "Iteration:   3% 75/2561 [00:54<30:26,  1.36it/s]\u001b[A\n",
            "Iteration:   3% 76/2561 [00:55<30:17,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 77/2561 [00:55<30:17,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 78/2561 [00:56<30:15,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 79/2561 [00:57<30:11,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 80/2561 [00:58<30:14,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 81/2561 [00:58<30:16,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 82/2561 [00:59<30:04,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 83/2561 [01:00<30:15,  1.36it/s]\u001b[A\n",
            "Iteration:   3% 84/2561 [01:01<30:16,  1.36it/s]\u001b[A\n",
            "Iteration:   3% 85/2561 [01:01<30:12,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 86/2561 [01:02<30:12,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 87/2561 [01:03<30:10,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 88/2561 [01:03<30:16,  1.36it/s]\u001b[A\n",
            "Iteration:   3% 89/2561 [01:04<30:18,  1.36it/s]\u001b[A\n",
            "Iteration:   4% 90/2561 [01:05<30:13,  1.36it/s]\u001b[A\n",
            "Iteration:   4% 91/2561 [01:06<30:23,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 92/2561 [01:06<30:18,  1.36it/s]\u001b[A\n",
            "Iteration:   4% 93/2561 [01:07<30:25,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 94/2561 [01:08<30:26,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 95/2561 [01:09<30:16,  1.36it/s]\u001b[A\n",
            "Iteration:   4% 96/2561 [01:09<30:20,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 97/2561 [01:10<30:16,  1.36it/s]\u001b[A\n",
            "Iteration:   4% 98/2561 [01:11<30:18,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 99/2561 [01:12<30:22,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 100/2561 [01:12<30:14,  1.36it/s]\u001b[A\n",
            "Iteration:   4% 101/2561 [01:13<30:18,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 102/2561 [01:14<30:14,  1.36it/s]\u001b[A\n",
            "Iteration:   4% 103/2561 [01:15<30:22,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 104/2561 [01:15<30:25,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 105/2561 [01:16<30:14,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 106/2561 [01:17<30:15,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 107/2561 [01:18<30:11,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 108/2561 [01:18<30:19,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 109/2561 [01:19<30:17,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 110/2561 [01:20<30:09,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 111/2561 [01:20<30:13,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 112/2561 [01:21<30:08,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 113/2561 [01:22<30:12,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 114/2561 [01:23<30:10,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 115/2561 [01:23<30:04,  1.36it/s]\u001b[A\n",
            "Iteration:   5% 116/2561 [01:24<30:07,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 117/2561 [01:25<30:02,  1.36it/s]\u001b[A\n",
            "Iteration:   5% 118/2561 [01:26<30:11,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 119/2561 [01:26<30:10,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 120/2561 [01:27<30:09,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 121/2561 [01:28<30:08,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 122/2561 [01:29<30:02,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 123/2561 [01:29<30:09,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 124/2561 [01:30<30:07,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 125/2561 [01:31<30:07,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 126/2561 [01:32<30:05,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 127/2561 [01:32<29:59,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 128/2561 [01:33<30:04,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 129/2561 [01:34<29:48,  1.36it/s]\u001b[A\n",
            "Iteration:   5% 130/2561 [01:35<29:44,  1.36it/s]\u001b[A\n",
            "Iteration:   5% 131/2561 [01:35<29:54,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 132/2561 [01:36<29:51,  1.36it/s]\u001b[A\n",
            "Iteration:   5% 133/2561 [01:37<29:59,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 134/2561 [01:37<29:59,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 135/2561 [01:38<29:57,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 136/2561 [01:39<30:02,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 137/2561 [01:40<29:55,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 138/2561 [01:40<30:00,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 139/2561 [01:41<29:54,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 140/2561 [01:42<30:01,  1.34it/s]\u001b[A\n",
            "Iteration:   6% 141/2561 [01:43<30:00,  1.34it/s]\u001b[A\n",
            "Iteration:   6% 142/2561 [01:43<29:50,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 143/2561 [01:44<29:55,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 144/2561 [01:45<29:51,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 145/2561 [01:46<29:57,  1.34it/s]\u001b[A\n",
            "Iteration:   6% 146/2561 [01:46<29:54,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 147/2561 [01:47<29:44,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 148/2561 [01:48<29:47,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 149/2561 [01:49<29:41,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 150/2561 [01:49<29:49,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 151/2561 [01:50<29:47,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 152/2561 [01:51<29:47,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 153/2561 [01:52<29:47,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 154/2561 [01:52<29:42,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 155/2561 [01:53<29:48,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 156/2561 [01:54<29:41,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 157/2561 [01:55<29:49,  1.34it/s]\u001b[A\n",
            "Iteration:   6% 158/2561 [01:55<29:49,  1.34it/s]\u001b[A\n",
            "Iteration:   6% 159/2561 [01:56<29:38,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 160/2561 [01:57<29:43,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 161/2561 [01:58<29:37,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 162/2561 [01:58<29:46,  1.34it/s]\u001b[A\n",
            "Iteration:   6% 163/2561 [01:59<29:44,  1.34it/s]\u001b[A\n",
            "Iteration:   6% 164/2561 [02:00<29:34,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 165/2561 [02:01<29:39,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 166/2561 [02:01<29:32,  1.35it/s]\u001b[A\n",
            "Iteration:   7% 167/2561 [02:02<29:38,  1.35it/s]\u001b[A\n",
            "Iteration:   7% 168/2561 [02:03<29:41,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 169/2561 [02:03<29:46,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 170/2561 [02:04<29:44,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 171/2561 [02:05<29:43,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 172/2561 [02:06<29:41,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 173/2561 [02:06<29:32,  1.35it/s]\u001b[A\n",
            "Iteration:   7% 174/2561 [02:07<29:36,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 175/2561 [02:08<29:32,  1.35it/s]\u001b[A\n",
            "Iteration:   7% 176/2561 [02:09<29:30,  1.35it/s]\u001b[A\n",
            "Iteration:   7% 177/2561 [02:09<29:34,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 178/2561 [02:10<29:25,  1.35it/s]\u001b[A\n",
            "Iteration:   7% 179/2561 [02:11<29:30,  1.35it/s]\u001b[A\n",
            "Iteration:   7% 180/2561 [02:12<29:24,  1.35it/s]\u001b[A\n",
            "Iteration:   7% 181/2561 [02:12<29:34,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 182/2561 [02:13<29:33,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 183/2561 [02:14<29:38,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 184/2561 [02:15<29:36,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 185/2561 [02:15<29:28,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 186/2561 [02:16<29:29,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 187/2561 [02:17<29:23,  1.35it/s]\u001b[A\n",
            "Iteration:   7% 188/2561 [02:18<29:27,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 189/2561 [02:18<29:25,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 190/2561 [02:19<29:23,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 191/2561 [02:20<29:24,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 192/2561 [02:21<29:18,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 193/2561 [02:21<29:19,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 194/2561 [02:22<29:14,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 195/2561 [02:23<29:17,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 196/2561 [02:24<29:20,  1.34it/s]\u001b[A\n",
            "Iteration:   8% 197/2561 [02:24<29:10,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 198/2561 [02:25<29:15,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 199/2561 [02:26<29:08,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 200/2561 [02:27<29:15,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 201/2561 [02:27<29:11,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 202/2561 [02:28<29:02,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 203/2561 [02:29<29:09,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 204/2561 [02:29<29:07,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 205/2561 [02:30<29:14,  1.34it/s]\u001b[A\n",
            "Iteration:   8% 206/2561 [02:31<29:13,  1.34it/s]\u001b[A\n",
            "Iteration:   8% 207/2561 [02:32<29:18,  1.34it/s]\u001b[A\n",
            "Iteration:   8% 208/2561 [02:32<29:17,  1.34it/s]\u001b[A\n",
            "Iteration:   8% 209/2561 [02:33<29:15,  1.34it/s]\u001b[A\n",
            "Iteration:   8% 210/2561 [02:34<29:13,  1.34it/s]\u001b[A\n",
            "Iteration:   8% 211/2561 [02:35<29:06,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 212/2561 [02:35<29:12,  1.34it/s]\u001b[A\n",
            "Iteration:   8% 213/2561 [02:36<29:05,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 214/2561 [02:37<28:57,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 215/2561 [02:38<29:04,  1.34it/s]\u001b[A\n",
            "Iteration:   8% 216/2561 [02:38<28:55,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 217/2561 [02:39<28:58,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 218/2561 [02:40<28:57,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 219/2561 [02:41<29:02,  1.34it/s]\u001b[A\n",
            "Iteration:   9% 220/2561 [02:41<29:01,  1.34it/s]\u001b[A\n",
            "Iteration:   9% 221/2561 [02:42<28:48,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 222/2561 [02:43<28:54,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 223/2561 [02:44<28:51,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 224/2561 [02:44<28:55,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 225/2561 [02:45<28:52,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 226/2561 [02:46<28:54,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 227/2561 [02:47<28:55,  1.34it/s]\u001b[A\n",
            "Iteration:   9% 228/2561 [02:47<28:49,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 229/2561 [02:48<28:55,  1.34it/s]\u001b[A\n",
            "Iteration:   9% 230/2561 [02:49<28:51,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 231/2561 [02:50<28:48,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 232/2561 [02:50<28:47,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 233/2561 [02:51<28:40,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 234/2561 [02:52<28:46,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 235/2561 [02:53<28:44,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 236/2561 [02:53<28:52,  1.34it/s]\u001b[A\n",
            "Iteration:   9% 237/2561 [02:54<28:50,  1.34it/s]\u001b[A\n",
            "Iteration:   9% 238/2561 [02:55<28:42,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 239/2561 [02:55<28:45,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 240/2561 [02:56<28:38,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 241/2561 [02:57<28:43,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 242/2561 [02:58<28:39,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 243/2561 [02:58<28:46,  1.34it/s]\u001b[A\n",
            "Iteration:  10% 244/2561 [02:59<28:44,  1.34it/s]\u001b[A\n",
            "Iteration:  10% 245/2561 [03:00<28:33,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 246/2561 [03:01<28:39,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 247/2561 [03:01<28:34,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 248/2561 [03:02<28:30,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 249/2561 [03:03<28:34,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 250/2561 [03:04<28:30,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 251/2561 [03:04<28:35,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 252/2561 [03:05<28:30,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 253/2561 [03:06<28:38,  1.34it/s]\u001b[A\n",
            "Iteration:  10% 254/2561 [03:07<28:33,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 255/2561 [03:07<28:28,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 256/2561 [03:08<28:33,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 257/2561 [03:09<28:28,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 258/2561 [03:10<28:34,  1.34it/s]\u001b[A\n",
            "Iteration:  10% 259/2561 [03:10<28:30,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 260/2561 [03:11<28:29,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 261/2561 [03:12<28:30,  1.34it/s]\u001b[A\n",
            "Iteration:  10% 262/2561 [03:13<28:22,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 263/2561 [03:13<28:29,  1.34it/s]\u001b[A\n",
            "Iteration:  10% 264/2561 [03:14<28:23,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 265/2561 [03:15<28:32,  1.34it/s]\u001b[A\n",
            "Iteration:  10% 266/2561 [03:16<28:31,  1.34it/s]\u001b[A\n",
            "Iteration:  10% 267/2561 [03:16<28:24,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 268/2561 [03:17<28:28,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 269/2561 [03:18<28:25,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 270/2561 [03:19<28:30,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 271/2561 [03:19<28:26,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 272/2561 [03:20<28:30,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 273/2561 [03:21<28:29,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 274/2561 [03:22<28:19,  1.35it/s]\u001b[A\n",
            "Iteration:  11% 275/2561 [03:22<28:25,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 276/2561 [03:23<28:19,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 277/2561 [03:24<28:22,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 278/2561 [03:24<28:19,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 279/2561 [03:25<28:25,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 280/2561 [03:26<28:26,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 281/2561 [03:27<28:15,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 282/2561 [03:27<28:16,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 283/2561 [03:28<28:09,  1.35it/s]\u001b[A\n",
            "Iteration:  11% 284/2561 [03:29<28:17,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 285/2561 [03:30<28:16,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 286/2561 [03:30<28:21,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 287/2561 [03:31<28:17,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 288/2561 [03:32<28:08,  1.35it/s]\u001b[A\n",
            "Iteration:  11% 289/2561 [03:33<28:10,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 290/2561 [03:33<28:03,  1.35it/s]\u001b[A\n",
            "Iteration:  11% 291/2561 [03:34<28:08,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 292/2561 [03:35<28:05,  1.35it/s]\u001b[A\n",
            "Iteration:  11% 293/2561 [03:36<28:03,  1.35it/s]\u001b[A\n",
            "Iteration:  11% 294/2561 [03:36<28:03,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 295/2561 [03:37<27:59,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 296/2561 [03:38<28:04,  1.34it/s]\u001b[A\n",
            "Iteration:  12% 297/2561 [03:39<27:59,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 298/2561 [03:39<28:05,  1.34it/s]\u001b[A\n",
            "Iteration:  12% 299/2561 [03:40<28:05,  1.34it/s]\u001b[A\n",
            "Iteration:  12% 300/2561 [03:41<27:55,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 301/2561 [03:42<27:57,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 302/2561 [03:42<27:53,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 303/2561 [03:43<27:59,  1.34it/s]\u001b[A\n",
            "Iteration:  12% 304/2561 [03:44<27:56,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 305/2561 [03:45<28:03,  1.34it/s]\u001b[A\n",
            "Iteration:  12% 306/2561 [03:45<28:01,  1.34it/s]\u001b[A\n",
            "Iteration:  12% 307/2561 [03:46<27:54,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 308/2561 [03:47<27:57,  1.34it/s]\u001b[A\n",
            "Iteration:  12% 309/2561 [03:48<27:52,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 310/2561 [03:48<27:56,  1.34it/s]\u001b[A\n",
            "Iteration:  12% 311/2561 [03:49<27:53,  1.34it/s]\u001b[A\n",
            "Iteration:  12% 312/2561 [03:50<27:52,  1.34it/s]\u001b[A\n",
            "Iteration:  12% 313/2561 [03:51<27:51,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 314/2561 [03:51<27:40,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 315/2561 [03:52<27:46,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 316/2561 [03:53<27:47,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 317/2561 [03:54<27:54,  1.34it/s]\u001b[A\n",
            "Iteration:  12% 318/2561 [03:54<27:49,  1.34it/s]\u001b[A\n",
            "Iteration:  12% 319/2561 [03:55<27:41,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 320/2561 [03:56<27:39,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 321/2561 [03:56<27:37,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 322/2561 [03:57<27:44,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 323/2561 [03:58<27:42,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 324/2561 [03:59<27:40,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 325/2561 [03:59<27:42,  1.34it/s]\u001b[A\n",
            "Iteration:  13% 326/2561 [04:00<27:35,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 327/2561 [04:01<27:41,  1.34it/s]\u001b[A\n",
            "Iteration:  13% 328/2561 [04:02<27:38,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 329/2561 [04:02<27:39,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 330/2561 [04:03<27:37,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 331/2561 [04:04<27:31,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 332/2561 [04:05<27:35,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 333/2561 [04:05<27:33,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 334/2561 [04:06<27:40,  1.34it/s]\u001b[A\n",
            "Iteration:  13% 335/2561 [04:07<27:37,  1.34it/s]\u001b[A\n",
            "Iteration:  13% 336/2561 [04:08<27:43,  1.34it/s]\u001b[A\n",
            "Iteration:  13% 337/2561 [04:08<27:41,  1.34it/s]\u001b[A\n",
            "Iteration:  13% 338/2561 [04:09<27:31,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 339/2561 [04:10<27:34,  1.34it/s]\u001b[A\n",
            "Iteration:  13% 340/2561 [04:11<27:29,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 341/2561 [04:11<27:33,  1.34it/s]\u001b[A\n",
            "Iteration:  13% 342/2561 [04:12<27:30,  1.34it/s]\u001b[A\n",
            "Iteration:  13% 343/2561 [04:13<27:18,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 344/2561 [04:14<27:23,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 345/2561 [04:14<27:18,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 346/2561 [04:15<27:26,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 347/2561 [04:16<27:23,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 348/2561 [04:17<27:30,  1.34it/s]\u001b[A\n",
            "Iteration:  14% 349/2561 [04:17<27:29,  1.34it/s]\u001b[A\n",
            "Iteration:  14% 350/2561 [04:18<27:23,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 351/2561 [04:19<27:23,  1.34it/s]\u001b[A\n",
            "Iteration:  14% 352/2561 [04:19<27:18,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 353/2561 [04:20<27:24,  1.34it/s]\u001b[A\n",
            "Iteration:  14% 354/2561 [04:21<27:21,  1.34it/s]\u001b[A\n",
            "Iteration:  14% 355/2561 [04:22<27:16,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 356/2561 [04:22<27:19,  1.34it/s]\u001b[A\n",
            "Iteration:  14% 357/2561 [04:23<27:10,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 358/2561 [04:24<27:16,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 359/2561 [04:25<27:12,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 360/2561 [04:25<27:19,  1.34it/s]\u001b[A\n",
            "Iteration:  14% 361/2561 [04:26<27:20,  1.34it/s]\u001b[A\n",
            "Iteration:  14% 362/2561 [04:27<27:10,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 363/2561 [04:28<27:07,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 364/2561 [04:28<27:05,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 365/2561 [04:29<27:14,  1.34it/s]\u001b[A\n",
            "Iteration:  14% 366/2561 [04:30<27:14,  1.34it/s]\u001b[A\n",
            "Iteration:  14% 367/2561 [04:31<27:11,  1.34it/s]\u001b[A\n",
            "Iteration:  14% 368/2561 [04:31<27:11,  1.34it/s]\u001b[A\n",
            "Iteration:  14% 369/2561 [04:32<27:04,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 370/2561 [04:33<27:09,  1.34it/s]\u001b[A\n",
            "Iteration:  14% 371/2561 [04:34<27:06,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 372/2561 [04:34<27:11,  1.34it/s]\u001b[A\n",
            "Iteration:  15% 373/2561 [04:35<27:08,  1.34it/s]\u001b[A\n",
            "Iteration:  15% 374/2561 [04:36<27:00,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 375/2561 [04:37<27:03,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 376/2561 [04:37<27:00,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 377/2561 [04:38<27:06,  1.34it/s]\u001b[A\n",
            "Iteration:  15% 378/2561 [04:39<27:05,  1.34it/s]\u001b[A\n",
            "Iteration:  15% 379/2561 [04:40<27:09,  1.34it/s]\u001b[A\n",
            "Iteration:  15% 380/2561 [04:40<27:08,  1.34it/s]\u001b[A\n",
            "Iteration:  15% 381/2561 [04:41<26:59,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 382/2561 [04:42<27:01,  1.34it/s]\u001b[A\n",
            "Iteration:  15% 383/2561 [04:43<26:53,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 384/2561 [04:43<27:00,  1.34it/s]\u001b[A\n",
            "Iteration:  15% 385/2561 [04:44<26:58,  1.34it/s]\u001b[A\n",
            "Iteration:  15% 386/2561 [04:45<26:53,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 387/2561 [04:46<26:56,  1.34it/s]\u001b[A\n",
            "Iteration:  15% 388/2561 [04:46<26:50,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 389/2561 [04:47<26:56,  1.34it/s]\u001b[A\n",
            "Iteration:  15% 390/2561 [04:48<26:53,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 391/2561 [04:48<26:58,  1.34it/s]\u001b[A\n",
            "Iteration:  15% 392/2561 [04:49<26:52,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 393/2561 [04:50<26:44,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 394/2561 [04:51<26:47,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 395/2561 [04:51<26:44,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 396/2561 [04:52<26:51,  1.34it/s]\u001b[A\n",
            "Iteration:  16% 397/2561 [04:53<26:50,  1.34it/s]\u001b[A\n",
            "Iteration:  16% 398/2561 [04:54<26:57,  1.34it/s]\u001b[A\n",
            "Iteration:  16% 399/2561 [04:54<26:56,  1.34it/s]\u001b[A\n",
            "Iteration:  16% 400/2561 [04:55<26:43,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 401/2561 [04:56<26:46,  1.34it/s]\u001b[A\n",
            "Iteration:  16% 402/2561 [04:57<26:40,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 403/2561 [04:57<26:45,  1.34it/s]\u001b[A\n",
            "Iteration:  16% 404/2561 [04:58<26:43,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 405/2561 [04:59<26:44,  1.34it/s]\u001b[A\n",
            "Iteration:  16% 406/2561 [05:00<26:42,  1.34it/s]\u001b[A\n",
            "Iteration:  16% 407/2561 [05:00<26:35,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 408/2561 [05:01<26:40,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 409/2561 [05:02<26:37,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 410/2561 [05:03<26:42,  1.34it/s]\u001b[A\n",
            "Iteration:  16% 411/2561 [05:03<26:42,  1.34it/s]\u001b[A\n",
            "Iteration:  16% 412/2561 [05:04<26:34,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 413/2561 [05:05<26:36,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 414/2561 [05:06<26:30,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 415/2561 [05:06<26:35,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 416/2561 [05:07<26:32,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 417/2561 [05:08<26:29,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 418/2561 [05:09<26:26,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 419/2561 [05:09<26:23,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 420/2561 [05:10<26:28,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 421/2561 [05:11<26:28,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 422/2561 [05:12<26:30,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 423/2561 [05:12<26:26,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 424/2561 [05:13<26:21,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 425/2561 [05:14<26:29,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 426/2561 [05:14<26:25,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 427/2561 [05:15<26:32,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 428/2561 [05:16<26:27,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 429/2561 [05:17<26:26,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 430/2561 [05:17<26:26,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 431/2561 [05:18<26:18,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 432/2561 [05:19<26:23,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 433/2561 [05:20<26:19,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 434/2561 [05:20<26:26,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 435/2561 [05:21<26:24,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 436/2561 [05:22<26:27,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 437/2561 [05:23<26:26,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 438/2561 [05:23<26:15,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 439/2561 [05:24<26:19,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 440/2561 [05:25<26:15,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 441/2561 [05:26<26:20,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 442/2561 [05:26<26:20,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 443/2561 [05:27<26:23,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 444/2561 [05:28<26:22,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 445/2561 [05:29<26:10,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 446/2561 [05:29<26:13,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 447/2561 [05:30<26:08,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 448/2561 [05:31<26:16,  1.34it/s]\u001b[A\n",
            "Iteration:  18% 449/2561 [05:32<26:09,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 450/2561 [05:32<26:07,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 451/2561 [05:33<26:08,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 452/2561 [05:34<26:00,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 453/2561 [05:35<26:04,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 454/2561 [05:35<26:03,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 455/2561 [05:36<26:02,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 456/2561 [05:37<26:05,  1.34it/s]\u001b[A\n",
            "Iteration:  18% 457/2561 [05:38<25:58,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 458/2561 [05:38<26:02,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 459/2561 [05:39<25:58,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 460/2561 [05:40<26:03,  1.34it/s]\u001b[A\n",
            "Iteration:  18% 461/2561 [05:41<26:01,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 462/2561 [05:41<26:07,  1.34it/s]\u001b[A\n",
            "Iteration:  18% 463/2561 [05:42<26:04,  1.34it/s]\u001b[A\n",
            "Iteration:  18% 464/2561 [05:43<25:57,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 465/2561 [05:44<26:01,  1.34it/s]\u001b[A\n",
            "Iteration:  18% 466/2561 [05:44<25:56,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 467/2561 [05:45<26:05,  1.34it/s]\u001b[A\n",
            "Iteration:  18% 468/2561 [05:46<26:01,  1.34it/s]\u001b[A\n",
            "Iteration:  18% 469/2561 [05:46<26:00,  1.34it/s]\u001b[A\n",
            "Iteration:  18% 470/2561 [05:47<25:58,  1.34it/s]\u001b[A\n",
            "Iteration:  18% 471/2561 [05:48<25:51,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 472/2561 [05:49<25:52,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 473/2561 [05:49<25:48,  1.35it/s]\u001b[A\n",
            "Iteration:  19% 474/2561 [05:50<25:55,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 475/2561 [05:51<25:54,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 476/2561 [05:52<25:47,  1.35it/s]\u001b[A\n",
            "Iteration:  19% 477/2561 [05:52<25:51,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 478/2561 [05:53<25:50,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 479/2561 [05:54<25:52,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 480/2561 [05:55<25:49,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 481/2561 [05:55<25:55,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 482/2561 [05:56<25:50,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 483/2561 [05:57<25:42,  1.35it/s]\u001b[A\n",
            "Iteration:  19% 484/2561 [05:58<25:45,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 485/2561 [05:58<25:41,  1.35it/s]\u001b[A\n",
            "Iteration:  19% 486/2561 [05:59<25:46,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 487/2561 [06:00<25:41,  1.35it/s]\u001b[A\n",
            "Iteration:  19% 488/2561 [06:01<25:45,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 489/2561 [06:01<25:47,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 490/2561 [06:02<25:39,  1.35it/s]\u001b[A\n",
            "Iteration:  19% 491/2561 [06:03<25:41,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 492/2561 [06:04<25:37,  1.35it/s]\u001b[A\n",
            "Iteration:  19% 493/2561 [06:04<25:42,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 494/2561 [06:05<25:39,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 495/2561 [06:06<25:43,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 496/2561 [06:07<25:42,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 497/2561 [06:07<25:41,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 498/2561 [06:08<25:40,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 499/2561 [06:09<25:32,  1.35it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "02/29/2020 07:34:18 - INFO - transformers.configuration_utils -   Configuration saved in output_roberta_GB/checkpoint-500/config.json\n",
            "02/29/2020 07:34:20 - INFO - transformers.modeling_utils -   Model weights saved in output_roberta_GB/checkpoint-500/pytorch_model.bin\n",
            "02/29/2020 07:34:20 - INFO - __main__ -   Saving model checkpoint to output_roberta_GB/checkpoint-500\n",
            "02/29/2020 07:34:23 - INFO - __main__ -   Saving optimizer and scheduler states to output_roberta_GB/checkpoint-500\n",
            "\n",
            "Iteration:  20% 500/2561 [06:15<1:19:09,  2.30s/it]\u001b[A\n",
            "Iteration:  20% 501/2561 [06:16<1:03:05,  1.84s/it]\u001b[A\n",
            "Iteration:  20% 502/2561 [06:16<51:51,  1.51s/it]  \u001b[A\n",
            "Iteration:  20% 503/2561 [06:17<43:52,  1.28s/it]\u001b[A\n",
            "Iteration:  20% 504/2561 [06:18<38:24,  1.12s/it]\u001b[A\n",
            "Iteration:  20% 505/2561 [06:19<34:30,  1.01s/it]\u001b[A\n",
            "Iteration:  20% 506/2561 [06:19<31:40,  1.08it/s]\u001b[A\n",
            "Iteration:  20% 507/2561 [06:20<29:51,  1.15it/s]\u001b[A\n",
            "Iteration:  20% 508/2561 [06:21<28:26,  1.20it/s]\u001b[A\n",
            "Iteration:  20% 509/2561 [06:21<27:36,  1.24it/s]\u001b[A\n",
            "Iteration:  20% 510/2561 [06:22<26:54,  1.27it/s]\u001b[A\n",
            "Iteration:  20% 511/2561 [06:23<26:36,  1.28it/s]\u001b[A\n",
            "Iteration:  20% 512/2561 [06:24<26:13,  1.30it/s]\u001b[A\n",
            "Iteration:  20% 513/2561 [06:24<25:52,  1.32it/s]\u001b[A\n",
            "Iteration:  20% 514/2561 [06:25<25:46,  1.32it/s]\u001b[A\n",
            "Iteration:  20% 515/2561 [06:26<25:34,  1.33it/s]\u001b[A\n",
            "Iteration:  20% 516/2561 [06:27<25:36,  1.33it/s]\u001b[A\n",
            "Iteration:  20% 517/2561 [06:27<25:28,  1.34it/s]\u001b[A\n",
            "Iteration:  20% 518/2561 [06:28<25:31,  1.33it/s]\u001b[A\n",
            "Iteration:  20% 519/2561 [06:29<25:28,  1.34it/s]\u001b[A\n",
            "Iteration:  20% 520/2561 [06:30<25:24,  1.34it/s]\u001b[A\n",
            "Iteration:  20% 521/2561 [06:30<25:23,  1.34it/s]\u001b[A\n",
            "Iteration:  20% 522/2561 [06:31<25:16,  1.34it/s]\u001b[A\n",
            "Iteration:  20% 523/2561 [06:32<25:19,  1.34it/s]\u001b[A\n",
            "Iteration:  20% 524/2561 [06:33<25:18,  1.34it/s]\u001b[A\n",
            "Iteration:  20% 525/2561 [06:33<25:21,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 526/2561 [06:34<25:17,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 527/2561 [06:35<25:08,  1.35it/s]\u001b[A\n",
            "Iteration:  21% 528/2561 [06:36<25:12,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 529/2561 [06:36<25:09,  1.35it/s]\u001b[A\n",
            "Iteration:  21% 530/2561 [06:37<25:15,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 531/2561 [06:38<25:13,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 532/2561 [06:39<25:18,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 533/2561 [06:39<25:16,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 534/2561 [06:40<25:07,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 535/2561 [06:41<25:10,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 536/2561 [06:42<25:08,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 537/2561 [06:42<25:13,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 538/2561 [06:43<25:09,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 539/2561 [06:44<25:15,  1.33it/s]\u001b[A\n",
            "Iteration:  21% 540/2561 [06:45<25:15,  1.33it/s]\u001b[A\n",
            "Iteration:  21% 541/2561 [06:45<25:17,  1.33it/s]\u001b[A\n",
            "Iteration:  21% 542/2561 [06:46<25:12,  1.33it/s]\u001b[A\n",
            "Iteration:  21% 543/2561 [06:47<25:04,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 544/2561 [06:48<25:05,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 545/2561 [06:48<25:02,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 546/2561 [06:49<25:09,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 547/2561 [06:50<25:08,  1.33it/s]\u001b[A\n",
            "Iteration:  21% 548/2561 [06:51<25:11,  1.33it/s]\u001b[A\n",
            "Iteration:  21% 549/2561 [06:51<25:07,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 550/2561 [06:52<25:12,  1.33it/s]\u001b[A\n",
            "Iteration:  22% 551/2561 [06:53<25:10,  1.33it/s]\u001b[A\n",
            "Iteration:  22% 552/2561 [06:54<25:00,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 553/2561 [06:54<25:01,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 554/2561 [06:55<24:57,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 555/2561 [06:56<25:01,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 556/2561 [06:57<24:59,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 557/2561 [06:57<25:00,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 558/2561 [06:58<24:57,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 559/2561 [06:59<24:48,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 560/2561 [07:00<24:50,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 561/2561 [07:00<24:48,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 562/2561 [07:01<24:55,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 563/2561 [07:02<24:52,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 564/2561 [07:03<24:49,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 565/2561 [07:03<24:49,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 566/2561 [07:04<24:42,  1.35it/s]\u001b[A\n",
            "Iteration:  22% 567/2561 [07:05<24:47,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 568/2561 [07:06<24:43,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 569/2561 [07:06<24:48,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 570/2561 [07:07<24:46,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 571/2561 [07:08<24:50,  1.33it/s]\u001b[A\n",
            "Iteration:  22% 572/2561 [07:09<24:47,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 573/2561 [07:09<24:38,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 574/2561 [07:10<24:40,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 575/2561 [07:11<24:35,  1.35it/s]\u001b[A\n",
            "Iteration:  22% 576/2561 [07:11<24:40,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 577/2561 [07:12<24:38,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 578/2561 [07:13<24:43,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 579/2561 [07:14<24:43,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 580/2561 [07:14<24:33,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 581/2561 [07:15<24:36,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 582/2561 [07:16<24:29,  1.35it/s]\u001b[A\n",
            "Iteration:  23% 583/2561 [07:17<24:33,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 584/2561 [07:17<24:29,  1.35it/s]\u001b[A\n",
            "Iteration:  23% 585/2561 [07:18<24:30,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 586/2561 [07:19<24:30,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 587/2561 [07:20<24:23,  1.35it/s]\u001b[A\n",
            "Iteration:  23% 588/2561 [07:20<24:28,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 589/2561 [07:21<24:21,  1.35it/s]\u001b[A\n",
            "Iteration:  23% 590/2561 [07:22<24:26,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 591/2561 [07:23<24:25,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 592/2561 [07:23<24:25,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 593/2561 [07:24<24:26,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 594/2561 [07:25<24:19,  1.35it/s]\u001b[A\n",
            "Iteration:  23% 595/2561 [07:26<24:22,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 596/2561 [07:26<24:18,  1.35it/s]\u001b[A\n",
            "Iteration:  23% 597/2561 [07:27<24:24,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 598/2561 [07:28<24:20,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 599/2561 [07:29<24:26,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 600/2561 [07:29<24:29,  1.33it/s]\u001b[A\n",
            "Iteration:  23% 601/2561 [07:30<24:16,  1.35it/s]\u001b[A\n",
            "Iteration:  24% 602/2561 [07:31<24:13,  1.35it/s]\u001b[A\n",
            "Iteration:  24% 603/2561 [07:32<24:13,  1.35it/s]\u001b[A\n",
            "Iteration:  24% 604/2561 [07:32<24:19,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 605/2561 [07:33<24:17,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 606/2561 [07:34<24:20,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 607/2561 [07:35<24:18,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 608/2561 [07:35<24:11,  1.35it/s]\u001b[A\n",
            "Iteration:  24% 609/2561 [07:36<24:13,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 610/2561 [07:37<24:08,  1.35it/s]\u001b[A\n",
            "Iteration:  24% 611/2561 [07:38<24:12,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 612/2561 [07:38<24:08,  1.35it/s]\u001b[A\n",
            "Iteration:  24% 613/2561 [07:39<24:06,  1.35it/s]\u001b[A\n",
            "Iteration:  24% 614/2561 [07:40<24:08,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 615/2561 [07:40<24:02,  1.35it/s]\u001b[A\n",
            "Iteration:  24% 616/2561 [07:41<24:05,  1.35it/s]\u001b[A\n",
            "Iteration:  24% 617/2561 [07:42<23:59,  1.35it/s]\u001b[A\n",
            "Iteration:  24% 618/2561 [07:43<24:05,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 619/2561 [07:43<24:07,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 620/2561 [07:44<24:01,  1.35it/s]\u001b[A\n",
            "Iteration:  24% 621/2561 [07:45<24:05,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 622/2561 [07:46<24:01,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 623/2561 [07:46<24:07,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 624/2561 [07:47<24:03,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 625/2561 [07:48<24:06,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 626/2561 [07:49<24:03,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 627/2561 [07:49<23:54,  1.35it/s]\u001b[A\n",
            "Iteration:  25% 628/2561 [07:50<23:58,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 629/2561 [07:51<23:55,  1.35it/s]\u001b[A\n",
            "Iteration:  25% 630/2561 [07:52<23:59,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 631/2561 [07:52<23:58,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 632/2561 [07:53<24:01,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 633/2561 [07:54<24:02,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 634/2561 [07:55<23:56,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 635/2561 [07:55<23:59,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 636/2561 [07:56<23:51,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 637/2561 [07:57<23:55,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 638/2561 [07:58<23:51,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 639/2561 [07:58<23:54,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 640/2561 [07:59<23:53,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 641/2561 [08:00<23:49,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 642/2561 [08:01<23:50,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 643/2561 [08:01<23:47,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 644/2561 [08:02<23:50,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 645/2561 [08:03<23:44,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 646/2561 [08:04<23:49,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 647/2561 [08:04<23:48,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 648/2561 [08:05<23:53,  1.33it/s]\u001b[A\n",
            "Iteration:  25% 649/2561 [08:06<23:50,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 650/2561 [08:07<23:42,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 651/2561 [08:07<23:43,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 652/2561 [08:08<23:39,  1.35it/s]\u001b[A\n",
            "Iteration:  25% 653/2561 [08:09<23:43,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 654/2561 [08:10<23:41,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 655/2561 [08:10<23:45,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 656/2561 [08:11<23:44,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 657/2561 [08:12<23:36,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 658/2561 [08:13<23:40,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 659/2561 [08:13<23:34,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 660/2561 [08:14<23:38,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 661/2561 [08:15<23:36,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 662/2561 [08:16<23:42,  1.33it/s]\u001b[A\n",
            "Iteration:  26% 663/2561 [08:16<23:39,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 664/2561 [08:17<23:37,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 665/2561 [08:18<23:36,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 666/2561 [08:19<23:29,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 667/2561 [08:19<23:31,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 668/2561 [08:20<23:26,  1.35it/s]\u001b[A\n",
            "Iteration:  26% 669/2561 [08:21<23:31,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 670/2561 [08:22<23:30,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 671/2561 [08:22<23:26,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 672/2561 [08:23<23:28,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 673/2561 [08:24<23:21,  1.35it/s]\u001b[A\n",
            "Iteration:  26% 674/2561 [08:24<23:26,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 675/2561 [08:25<23:23,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 676/2561 [08:26<23:26,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 677/2561 [08:27<23:23,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 678/2561 [08:27<23:16,  1.35it/s]\u001b[A\n",
            "Iteration:  27% 679/2561 [08:28<23:20,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 680/2561 [08:29<23:20,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 681/2561 [08:30<23:24,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 682/2561 [08:30<23:20,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 683/2561 [08:31<23:23,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 684/2561 [08:32<23:22,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 685/2561 [08:33<23:26,  1.33it/s]\u001b[A\n",
            "Iteration:  27% 686/2561 [08:33<23:26,  1.33it/s]\u001b[A\n",
            "Iteration:  27% 687/2561 [08:34<23:16,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 688/2561 [08:35<23:19,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 689/2561 [08:36<23:14,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 690/2561 [08:36<23:17,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 691/2561 [08:37<23:14,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 692/2561 [08:38<23:18,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 693/2561 [08:39<23:14,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 694/2561 [08:39<23:12,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 695/2561 [08:40<23:11,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 696/2561 [08:41<23:06,  1.35it/s]\u001b[A\n",
            "Iteration:  27% 697/2561 [08:42<23:09,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 698/2561 [08:42<23:06,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 699/2561 [08:43<23:12,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 700/2561 [08:44<23:08,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 701/2561 [08:45<23:07,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 702/2561 [08:45<23:06,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 703/2561 [08:46<22:59,  1.35it/s]\u001b[A\n",
            "Iteration:  27% 704/2561 [08:47<23:01,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 705/2561 [08:48<22:56,  1.35it/s]\u001b[A\n",
            "Iteration:  28% 706/2561 [08:48<23:03,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 707/2561 [08:49<23:02,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 708/2561 [08:50<23:02,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 709/2561 [08:51<23:01,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 710/2561 [08:51<22:53,  1.35it/s]\u001b[A\n",
            "Iteration:  28% 711/2561 [08:52<22:58,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 712/2561 [08:53<22:56,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 713/2561 [08:54<23:00,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 714/2561 [08:54<22:56,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 715/2561 [08:55<22:50,  1.35it/s]\u001b[A\n",
            "Iteration:  28% 716/2561 [08:56<22:54,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 717/2561 [08:57<22:46,  1.35it/s]\u001b[A\n",
            "Iteration:  28% 718/2561 [08:57<22:50,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 719/2561 [08:58<22:49,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 720/2561 [08:59<22:57,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 721/2561 [09:00<22:53,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 722/2561 [09:00<22:52,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 723/2561 [09:01<22:53,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 724/2561 [09:02<22:46,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 725/2561 [09:03<22:51,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 726/2561 [09:03<22:46,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 727/2561 [09:04<22:52,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 728/2561 [09:05<22:48,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 729/2561 [09:05<22:49,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 730/2561 [09:06<22:48,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 731/2561 [09:07<22:44,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 732/2561 [09:08<22:46,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 733/2561 [09:08<22:42,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 734/2561 [09:09<22:48,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 735/2561 [09:10<22:42,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 736/2561 [09:11<22:45,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 737/2561 [09:11<22:43,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 738/2561 [09:12<22:38,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 739/2561 [09:13<22:39,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 740/2561 [09:14<22:34,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 741/2561 [09:14<22:36,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 742/2561 [09:15<22:34,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 743/2561 [09:16<22:36,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 744/2561 [09:17<22:35,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 745/2561 [09:17<22:30,  1.35it/s]\u001b[A\n",
            "Iteration:  29% 746/2561 [09:18<22:32,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 747/2561 [09:19<22:27,  1.35it/s]\u001b[A\n",
            "Iteration:  29% 748/2561 [09:20<22:30,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 749/2561 [09:20<22:27,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 750/2561 [09:21<22:33,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 751/2561 [09:22<22:31,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 752/2561 [09:23<22:37,  1.33it/s]\u001b[A\n",
            "Iteration:  29% 753/2561 [09:23<22:35,  1.33it/s]\u001b[A\n",
            "Iteration:  29% 754/2561 [09:24<22:25,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 755/2561 [09:25<22:26,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 756/2561 [09:26<22:23,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 757/2561 [09:26<22:24,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 758/2561 [09:27<22:21,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 759/2561 [09:28<22:26,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 760/2561 [09:29<22:23,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 761/2561 [09:29<22:20,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 762/2561 [09:30<22:21,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 763/2561 [09:31<22:16,  1.35it/s]\u001b[A\n",
            "Iteration:  30% 764/2561 [09:32<22:20,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 765/2561 [09:32<22:17,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 766/2561 [09:33<22:23,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 767/2561 [09:34<22:21,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 768/2561 [09:35<22:17,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 769/2561 [09:35<22:19,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 770/2561 [09:36<22:12,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 771/2561 [09:37<22:18,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 772/2561 [09:38<22:13,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 773/2561 [09:38<22:18,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 774/2561 [09:39<22:14,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 775/2561 [09:40<22:17,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 776/2561 [09:41<22:14,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 777/2561 [09:41<22:06,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 778/2561 [09:42<22:11,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 779/2561 [09:43<22:04,  1.35it/s]\u001b[A\n",
            "Iteration:  30% 780/2561 [09:44<22:08,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 781/2561 [09:44<22:06,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 782/2561 [09:45<22:11,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 783/2561 [09:46<22:08,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 784/2561 [09:47<22:12,  1.33it/s]\u001b[A\n",
            "Iteration:  31% 785/2561 [09:47<22:10,  1.33it/s]\u001b[A\n",
            "Iteration:  31% 786/2561 [09:48<22:00,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 787/2561 [09:49<22:03,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 788/2561 [09:50<21:58,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 789/2561 [09:50<22:03,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 790/2561 [09:51<22:01,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 791/2561 [09:52<22:04,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 792/2561 [09:53<22:04,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 793/2561 [09:53<21:57,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 794/2561 [09:54<21:58,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 795/2561 [09:55<21:53,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 796/2561 [09:55<21:57,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 797/2561 [09:56<21:53,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 798/2561 [09:57<21:59,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 799/2561 [09:58<21:57,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 800/2561 [09:58<21:51,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 801/2561 [09:59<21:51,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 802/2561 [10:00<21:45,  1.35it/s]\u001b[A\n",
            "Iteration:  31% 803/2561 [10:01<21:50,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 804/2561 [10:01<21:46,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 805/2561 [10:02<21:52,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 806/2561 [10:03<21:52,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 807/2561 [10:04<21:46,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 808/2561 [10:04<21:46,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 809/2561 [10:05<21:41,  1.35it/s]\u001b[A\n",
            "Iteration:  32% 810/2561 [10:06<21:45,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 811/2561 [10:07<21:41,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 812/2561 [10:07<21:46,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 813/2561 [10:08<21:44,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 814/2561 [10:09<21:48,  1.33it/s]\u001b[A\n",
            "Iteration:  32% 815/2561 [10:10<21:47,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 816/2561 [10:10<21:38,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 817/2561 [10:11<21:39,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 818/2561 [10:12<21:35,  1.35it/s]\u001b[A\n",
            "Iteration:  32% 819/2561 [10:13<21:37,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 820/2561 [10:13<21:37,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 821/2561 [10:14<21:41,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 822/2561 [10:15<21:42,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 823/2561 [10:16<21:44,  1.33it/s]\u001b[A\n",
            "Iteration:  32% 824/2561 [10:16<21:43,  1.33it/s]\u001b[A\n",
            "Iteration:  32% 825/2561 [10:17<21:31,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 826/2561 [10:18<21:34,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 827/2561 [10:19<21:30,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 828/2561 [10:19<21:35,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 829/2561 [10:20<21:29,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 830/2561 [10:21<21:34,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 831/2561 [10:22<21:32,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 832/2561 [10:22<21:38,  1.33it/s]\u001b[A\n",
            "Iteration:  33% 833/2561 [10:23<21:36,  1.33it/s]\u001b[A\n",
            "Iteration:  33% 834/2561 [10:24<21:27,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 835/2561 [10:25<21:26,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 836/2561 [10:25<21:22,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 837/2561 [10:26<21:28,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 838/2561 [10:27<21:24,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 839/2561 [10:28<21:28,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 840/2561 [10:28<21:26,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 841/2561 [10:29<21:29,  1.33it/s]\u001b[A\n",
            "Iteration:  33% 842/2561 [10:30<21:29,  1.33it/s]\u001b[A\n",
            "Iteration:  33% 843/2561 [10:31<21:21,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 844/2561 [10:31<21:23,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 845/2561 [10:32<21:17,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 846/2561 [10:33<21:22,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 847/2561 [10:34<21:17,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 848/2561 [10:34<21:18,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 849/2561 [10:35<21:19,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 850/2561 [10:36<21:15,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 851/2561 [10:37<21:15,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 852/2561 [10:37<21:12,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 853/2561 [10:38<21:16,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 854/2561 [10:39<21:11,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 855/2561 [10:40<21:14,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 856/2561 [10:40<21:12,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 857/2561 [10:41<21:16,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 858/2561 [10:42<21:15,  1.33it/s]\u001b[A\n",
            "Iteration:  34% 859/2561 [10:43<21:10,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 860/2561 [10:43<21:09,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 861/2561 [10:44<21:02,  1.35it/s]\u001b[A\n",
            "Iteration:  34% 862/2561 [10:45<21:07,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 863/2561 [10:45<21:04,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 864/2561 [10:46<21:08,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 865/2561 [10:47<21:06,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 866/2561 [10:48<21:10,  1.33it/s]\u001b[A\n",
            "Iteration:  34% 867/2561 [10:48<21:09,  1.33it/s]\u001b[A\n",
            "Iteration:  34% 868/2561 [10:49<21:01,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 869/2561 [10:50<21:03,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 870/2561 [10:51<20:58,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 871/2561 [10:51<21:00,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 872/2561 [10:52<21:00,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 873/2561 [10:53<21:03,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 874/2561 [10:54<21:01,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 875/2561 [10:54<20:53,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 876/2561 [10:55<20:56,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 877/2561 [10:56<20:52,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 878/2561 [10:57<20:54,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 879/2561 [10:57<20:50,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 880/2561 [10:58<20:57,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 881/2561 [10:59<20:53,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 882/2561 [11:00<20:58,  1.33it/s]\u001b[A\n",
            "Iteration:  34% 883/2561 [11:00<20:55,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 884/2561 [11:01<20:53,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 885/2561 [11:02<20:53,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 886/2561 [11:03<20:46,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 887/2561 [11:03<20:47,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 888/2561 [11:04<20:40,  1.35it/s]\u001b[A\n",
            "Iteration:  35% 889/2561 [11:05<20:48,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 890/2561 [11:06<20:48,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 891/2561 [11:06<20:51,  1.33it/s]\u001b[A\n",
            "Iteration:  35% 892/2561 [11:07<20:48,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 893/2561 [11:08<20:41,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 894/2561 [11:09<20:44,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 895/2561 [11:09<20:39,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 896/2561 [11:10<20:42,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 897/2561 [11:11<20:38,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 898/2561 [11:12<20:37,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 899/2561 [11:12<20:40,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 900/2561 [11:13<20:34,  1.35it/s]\u001b[A\n",
            "Iteration:  35% 901/2561 [11:14<20:35,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 902/2561 [11:15<20:35,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 903/2561 [11:15<20:40,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 904/2561 [11:16<20:38,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 905/2561 [11:17<20:41,  1.33it/s]\u001b[A\n",
            "Iteration:  35% 906/2561 [11:18<20:40,  1.33it/s]\u001b[A\n",
            "Iteration:  35% 907/2561 [11:18<20:43,  1.33it/s]\u001b[A\n",
            "Iteration:  35% 908/2561 [11:19<20:41,  1.33it/s]\u001b[A\n",
            "Iteration:  35% 909/2561 [11:20<20:32,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 910/2561 [11:21<20:34,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 911/2561 [11:21<20:29,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 912/2561 [11:22<20:30,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 913/2561 [11:23<20:31,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 914/2561 [11:24<20:34,  1.33it/s]\u001b[A\n",
            "Iteration:  36% 915/2561 [11:24<20:31,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 916/2561 [11:25<20:23,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 917/2561 [11:26<20:24,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 918/2561 [11:27<20:22,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 919/2561 [11:27<20:24,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 920/2561 [11:28<20:22,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 921/2561 [11:29<20:27,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 922/2561 [11:30<20:26,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 923/2561 [11:30<20:26,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 924/2561 [11:31<20:26,  1.33it/s]\u001b[A\n",
            "Iteration:  36% 925/2561 [11:32<20:18,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 926/2561 [11:33<20:21,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 927/2561 [11:33<20:16,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 928/2561 [11:34<20:19,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 929/2561 [11:35<20:16,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 930/2561 [11:36<20:15,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 931/2561 [11:36<20:14,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 932/2561 [11:37<20:10,  1.35it/s]\u001b[A\n",
            "Iteration:  36% 933/2561 [11:38<20:13,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 934/2561 [11:38<20:08,  1.35it/s]\u001b[A\n",
            "Iteration:  37% 935/2561 [11:39<20:10,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 936/2561 [11:40<20:07,  1.35it/s]\u001b[A\n",
            "Iteration:  37% 937/2561 [11:41<20:14,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 938/2561 [11:41<20:13,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 939/2561 [11:42<20:08,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 940/2561 [11:43<20:09,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 941/2561 [11:44<20:05,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 942/2561 [11:44<20:03,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 943/2561 [11:45<20:01,  1.35it/s]\u001b[A\n",
            "Iteration:  37% 944/2561 [11:46<20:07,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 945/2561 [11:47<20:07,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 946/2561 [11:47<20:00,  1.35it/s]\u001b[A\n",
            "Iteration:  37% 947/2561 [11:48<20:00,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 948/2561 [11:49<19:58,  1.35it/s]\u001b[A\n",
            "Iteration:  37% 949/2561 [11:50<20:01,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 950/2561 [11:50<19:59,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 951/2561 [11:51<20:05,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 952/2561 [11:52<20:04,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 953/2561 [11:53<20:08,  1.33it/s]\u001b[A\n",
            "Iteration:  37% 954/2561 [11:53<20:06,  1.33it/s]\u001b[A\n",
            "Iteration:  37% 955/2561 [11:54<20:05,  1.33it/s]\u001b[A\n",
            "Iteration:  37% 956/2561 [11:55<20:03,  1.33it/s]\u001b[A\n",
            "Iteration:  37% 957/2561 [11:56<19:57,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 958/2561 [11:56<19:59,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 959/2561 [11:57<19:53,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 960/2561 [11:58<19:55,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 961/2561 [11:59<19:53,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 962/2561 [11:59<19:57,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 963/2561 [12:00<19:55,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 964/2561 [12:01<19:55,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 965/2561 [12:02<19:53,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 966/2561 [12:02<19:46,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 967/2561 [12:03<19:49,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 968/2561 [12:04<19:46,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 969/2561 [12:05<19:50,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 970/2561 [12:05<19:46,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 971/2561 [12:06<19:44,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 972/2561 [12:07<19:45,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 973/2561 [12:08<19:41,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 974/2561 [12:08<19:45,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 975/2561 [12:09<19:41,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 976/2561 [12:10<19:44,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 977/2561 [12:11<19:40,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 978/2561 [12:11<19:43,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 979/2561 [12:12<19:42,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 980/2561 [12:13<19:37,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 981/2561 [12:14<19:37,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 982/2561 [12:14<19:32,  1.35it/s]\u001b[A\n",
            "Iteration:  38% 983/2561 [12:15<19:38,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 984/2561 [12:16<19:35,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 985/2561 [12:17<19:40,  1.34it/s]\u001b[A\n",
            "Iteration:  39% 986/2561 [12:17<19:37,  1.34it/s]\u001b[A\n",
            "Iteration:  39% 987/2561 [12:18<19:40,  1.33it/s]\u001b[A\n",
            "Iteration:  39% 988/2561 [12:19<19:37,  1.34it/s]\u001b[A\n",
            "Iteration:  39% 989/2561 [12:20<19:30,  1.34it/s]\u001b[A\n",
            "Iteration:  39% 990/2561 [12:20<19:31,  1.34it/s]\u001b[A\n",
            "Iteration:  39% 991/2561 [12:21<19:27,  1.35it/s]\u001b[A\n",
            "Iteration:  39% 992/2561 [12:22<19:31,  1.34it/s]\u001b[A\n",
            "Iteration:  39% 993/2561 [12:23<19:31,  1.34it/s]\u001b[A\n",
            "Iteration:  39% 994/2561 [12:23<19:34,  1.33it/s]\u001b[A\n",
            "Iteration:  39% 995/2561 [12:24<19:30,  1.34it/s]\u001b[A\n",
            "Iteration:  39% 996/2561 [12:25<19:31,  1.34it/s]\u001b[A\n",
            "Iteration:  39% 997/2561 [12:26<19:29,  1.34it/s]\u001b[A\n",
            "Iteration:  39% 998/2561 [12:26<19:19,  1.35it/s]\u001b[A\n",
            "Iteration:  39% 999/2561 [12:27<19:18,  1.35it/s]\u001b[A02/29/2020 07:40:36 - INFO - transformers.configuration_utils -   Configuration saved in output_roberta_GB/checkpoint-1000/config.json\n",
            "02/29/2020 07:40:38 - INFO - transformers.modeling_utils -   Model weights saved in output_roberta_GB/checkpoint-1000/pytorch_model.bin\n",
            "02/29/2020 07:40:38 - INFO - __main__ -   Saving model checkpoint to output_roberta_GB/checkpoint-1000\n",
            "02/29/2020 07:40:42 - INFO - __main__ -   Saving optimizer and scheduler states to output_roberta_GB/checkpoint-1000\n",
            "\n",
            "Iteration:  39% 1000/2561 [12:33<59:47,  2.30s/it]\u001b[A\n",
            "Iteration:  39% 1001/2561 [12:34<47:44,  1.84s/it]\u001b[A\n",
            "Iteration:  39% 1002/2561 [12:34<39:10,  1.51s/it]\u001b[A\n",
            "Iteration:  39% 1003/2561 [12:35<33:12,  1.28s/it]\u001b[A\n",
            "Iteration:  39% 1004/2561 [12:36<28:55,  1.11s/it]\u001b[A\n",
            "Iteration:  39% 1005/2561 [12:37<25:57,  1.00s/it]\u001b[A\n",
            "Iteration:  39% 1006/2561 [12:37<24:00,  1.08it/s]\u001b[A\n",
            "Iteration:  39% 1007/2561 [12:38<22:32,  1.15it/s]\u001b[A\n",
            "Iteration:  39% 1008/2561 [12:39<21:35,  1.20it/s]\u001b[A\n",
            "Iteration:  39% 1009/2561 [12:40<20:53,  1.24it/s]\u001b[A\n",
            "Iteration:  39% 1010/2561 [12:40<20:28,  1.26it/s]\u001b[A\n",
            "Iteration:  39% 1011/2561 [12:41<20:06,  1.29it/s]\u001b[A\n",
            "Iteration:  40% 1012/2561 [12:42<19:45,  1.31it/s]\u001b[A\n",
            "Iteration:  40% 1013/2561 [12:43<19:31,  1.32it/s]\u001b[A\n",
            "Iteration:  40% 1014/2561 [12:43<19:24,  1.33it/s]\u001b[A\n",
            "Iteration:  40% 1015/2561 [12:44<19:24,  1.33it/s]\u001b[A\n",
            "Iteration:  40% 1016/2561 [12:45<19:19,  1.33it/s]\u001b[A\n",
            "Iteration:  40% 1017/2561 [12:46<19:21,  1.33it/s]\u001b[A\n",
            "Iteration:  40% 1018/2561 [12:46<19:18,  1.33it/s]\u001b[A\n",
            "Iteration:  40% 1019/2561 [12:47<19:12,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1020/2561 [12:48<19:07,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1021/2561 [12:49<19:04,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1022/2561 [12:49<19:08,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1023/2561 [12:50<19:06,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1024/2561 [12:51<19:01,  1.35it/s]\u001b[A\n",
            "Iteration:  40% 1025/2561 [12:52<19:03,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1026/2561 [12:52<19:01,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1027/2561 [12:53<19:04,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1028/2561 [12:54<19:02,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1029/2561 [12:55<19:06,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1030/2561 [12:55<19:04,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1031/2561 [12:56<19:01,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1032/2561 [12:57<19:01,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1033/2561 [12:58<18:57,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1034/2561 [12:58<18:58,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1035/2561 [12:59<18:55,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1036/2561 [13:00<18:59,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1037/2561 [13:00<18:57,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 1038/2561 [13:01<19:01,  1.33it/s]\u001b[A\n",
            "Iteration:  41% 1039/2561 [13:02<18:58,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 1040/2561 [13:03<18:51,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 1041/2561 [13:03<18:55,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 1042/2561 [13:04<18:51,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 1043/2561 [13:05<18:55,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 1044/2561 [13:06<18:51,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 1045/2561 [13:06<18:56,  1.33it/s]\u001b[A\n",
            "Iteration:  41% 1046/2561 [13:07<18:53,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 1047/2561 [13:08<18:56,  1.33it/s]\u001b[A\n",
            "Iteration:  41% 1048/2561 [13:09<18:56,  1.33it/s]\u001b[A\n",
            "Iteration:  41% 1049/2561 [13:09<18:49,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 1050/2561 [13:10<18:50,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 1051/2561 [13:11<18:45,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 1052/2561 [13:12<18:50,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 1053/2561 [13:12<18:48,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 1054/2561 [13:13<18:52,  1.33it/s]\u001b[A\n",
            "Iteration:  41% 1055/2561 [13:14<18:48,  1.33it/s]\u001b[A\n",
            "Iteration:  41% 1056/2561 [13:15<18:53,  1.33it/s]\u001b[A\n",
            "Iteration:  41% 1057/2561 [13:15<18:51,  1.33it/s]\u001b[A\n",
            "Iteration:  41% 1058/2561 [13:16<18:52,  1.33it/s]\u001b[A\n",
            "Iteration:  41% 1059/2561 [13:17<18:49,  1.33it/s]\u001b[A\n",
            "Iteration:  41% 1060/2561 [13:18<18:51,  1.33it/s]\u001b[A\n",
            "Iteration:  41% 1061/2561 [13:18<18:46,  1.33it/s]\u001b[A\n",
            "Iteration:  41% 1062/2561 [13:19<18:40,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1063/2561 [13:20<18:42,  1.33it/s]\u001b[A\n",
            "Iteration:  42% 1064/2561 [13:21<18:37,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1065/2561 [13:21<18:40,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1066/2561 [13:22<18:36,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1067/2561 [13:23<18:43,  1.33it/s]\u001b[A\n",
            "Iteration:  42% 1068/2561 [13:24<18:38,  1.33it/s]\u001b[A\n",
            "Iteration:  42% 1069/2561 [13:24<18:44,  1.33it/s]\u001b[A\n",
            "Iteration:  42% 1070/2561 [13:25<18:42,  1.33it/s]\u001b[A\n",
            "Iteration:  42% 1071/2561 [13:26<18:43,  1.33it/s]\u001b[A\n",
            "Iteration:  42% 1072/2561 [13:27<18:39,  1.33it/s]\u001b[A\n",
            "Iteration:  42% 1073/2561 [13:27<18:31,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1074/2561 [13:28<18:34,  1.33it/s]\u001b[A\n",
            "Iteration:  42% 1075/2561 [13:29<18:31,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1076/2561 [13:30<18:32,  1.33it/s]\u001b[A\n",
            "Iteration:  42% 1077/2561 [13:30<18:29,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1078/2561 [13:31<18:30,  1.33it/s]\u001b[A\n",
            "Iteration:  42% 1079/2561 [13:32<18:27,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1080/2561 [13:33<18:30,  1.33it/s]\u001b[A\n",
            "Iteration:  42% 1081/2561 [13:33<18:28,  1.33it/s]\u001b[A\n",
            "Iteration:  42% 1082/2561 [13:34<18:21,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1083/2561 [13:35<18:24,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1084/2561 [13:36<18:21,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1085/2561 [13:36<18:23,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1086/2561 [13:37<18:19,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1087/2561 [13:38<18:21,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1088/2561 [13:39<18:20,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1089/2561 [13:39<18:19,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1090/2561 [13:40<18:18,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1091/2561 [13:41<18:11,  1.35it/s]\u001b[A\n",
            "Iteration:  43% 1092/2561 [13:42<18:13,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1093/2561 [13:42<18:10,  1.35it/s]\u001b[A\n",
            "Iteration:  43% 1094/2561 [13:43<18:14,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1095/2561 [13:44<18:13,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1096/2561 [13:45<18:10,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1097/2561 [13:45<18:11,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1098/2561 [13:46<18:06,  1.35it/s]\u001b[A\n",
            "Iteration:  43% 1099/2561 [13:47<18:08,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1100/2561 [13:48<18:05,  1.35it/s]\u001b[A\n",
            "Iteration:  43% 1101/2561 [13:48<18:09,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1102/2561 [13:49<18:09,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1103/2561 [13:50<18:13,  1.33it/s]\u001b[A\n",
            "Iteration:  43% 1104/2561 [13:51<18:11,  1.33it/s]\u001b[A\n",
            "Iteration:  43% 1105/2561 [13:51<18:04,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1106/2561 [13:52<18:09,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1107/2561 [13:53<18:03,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1108/2561 [13:54<18:05,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1109/2561 [13:54<18:02,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1110/2561 [13:55<18:06,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1111/2561 [13:56<18:05,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1112/2561 [13:57<18:06,  1.33it/s]\u001b[A\n",
            "Iteration:  43% 1113/2561 [13:57<18:05,  1.33it/s]\u001b[A\n",
            "Iteration:  43% 1114/2561 [13:58<17:57,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1115/2561 [13:59<17:58,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1116/2561 [14:00<17:55,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1117/2561 [14:00<17:57,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1118/2561 [14:01<17:57,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1119/2561 [14:02<17:59,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1120/2561 [14:03<17:57,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1121/2561 [14:03<17:56,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1122/2561 [14:04<17:55,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1123/2561 [14:05<17:51,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1124/2561 [14:06<17:52,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1125/2561 [14:06<17:49,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1126/2561 [14:07<17:51,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1127/2561 [14:08<17:49,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1128/2561 [14:09<17:52,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1129/2561 [14:09<17:50,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1130/2561 [14:10<17:49,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1131/2561 [14:11<17:47,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1132/2561 [14:12<17:42,  1.35it/s]\u001b[A\n",
            "Iteration:  44% 1133/2561 [14:12<17:45,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1134/2561 [14:13<17:42,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1135/2561 [14:14<17:46,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1136/2561 [14:15<17:45,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1137/2561 [14:15<17:48,  1.33it/s]\u001b[A\n",
            "Iteration:  44% 1138/2561 [14:16<17:45,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1139/2561 [14:17<17:41,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1140/2561 [14:18<17:42,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1141/2561 [14:18<17:37,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1142/2561 [14:19<17:40,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1143/2561 [14:20<17:39,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1144/2561 [14:20<17:41,  1.33it/s]\u001b[A\n",
            "Iteration:  45% 1145/2561 [14:21<17:39,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1146/2561 [14:22<17:41,  1.33it/s]\u001b[A\n",
            "Iteration:  45% 1147/2561 [14:23<17:41,  1.33it/s]\u001b[A\n",
            "Iteration:  45% 1148/2561 [14:23<17:37,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1149/2561 [14:24<17:36,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1150/2561 [14:25<17:32,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1151/2561 [14:26<17:34,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1152/2561 [14:26<17:31,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1153/2561 [14:27<17:34,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1154/2561 [14:28<17:32,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1155/2561 [14:29<17:35,  1.33it/s]\u001b[A\n",
            "Iteration:  45% 1156/2561 [14:29<17:37,  1.33it/s]\u001b[A\n",
            "Iteration:  45% 1157/2561 [14:30<17:40,  1.32it/s]\u001b[A\n",
            "Iteration:  45% 1158/2561 [14:31<17:38,  1.33it/s]\u001b[A\n",
            "Iteration:  45% 1159/2561 [14:32<17:33,  1.33it/s]\u001b[A\n",
            "Iteration:  45% 1160/2561 [14:32<17:30,  1.33it/s]\u001b[A\n",
            "Iteration:  45% 1161/2561 [14:33<17:25,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1162/2561 [14:34<17:27,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1163/2561 [14:35<17:23,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1164/2561 [14:35<17:26,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1165/2561 [14:36<17:22,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1166/2561 [14:37<17:25,  1.33it/s]\u001b[A\n",
            "Iteration:  46% 1167/2561 [14:38<17:24,  1.33it/s]\u001b[A\n",
            "Iteration:  46% 1168/2561 [14:38<17:27,  1.33it/s]\u001b[A\n",
            "Iteration:  46% 1169/2561 [14:39<17:24,  1.33it/s]\u001b[A\n",
            "Iteration:  46% 1170/2561 [14:40<17:16,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1171/2561 [14:41<17:19,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1172/2561 [14:41<17:16,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1173/2561 [14:42<17:18,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1174/2561 [14:43<17:16,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1175/2561 [14:44<17:18,  1.33it/s]\u001b[A\n",
            "Iteration:  46% 1176/2561 [14:44<17:15,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1177/2561 [14:45<17:18,  1.33it/s]\u001b[A\n",
            "Iteration:  46% 1178/2561 [14:46<17:15,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1179/2561 [14:47<17:09,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1180/2561 [14:47<17:11,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1181/2561 [14:48<17:07,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1182/2561 [14:49<17:10,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1183/2561 [14:50<17:08,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1184/2561 [14:50<17:11,  1.33it/s]\u001b[A\n",
            "Iteration:  46% 1185/2561 [14:51<17:08,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1186/2561 [14:52<17:11,  1.33it/s]\u001b[A\n",
            "Iteration:  46% 1187/2561 [14:53<17:10,  1.33it/s]\u001b[A\n",
            "Iteration:  46% 1188/2561 [14:53<17:04,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1189/2561 [14:54<17:05,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1190/2561 [14:55<17:00,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1191/2561 [14:56<17:03,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1192/2561 [14:56<17:01,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1193/2561 [14:57<17:03,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1194/2561 [14:58<17:01,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1195/2561 [14:59<17:04,  1.33it/s]\u001b[A\n",
            "Iteration:  47% 1196/2561 [14:59<17:03,  1.33it/s]\u001b[A\n",
            "Iteration:  47% 1197/2561 [15:00<16:56,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1198/2561 [15:01<16:59,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1199/2561 [15:02<16:55,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1200/2561 [15:02<16:57,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1201/2561 [15:03<16:52,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1202/2561 [15:04<16:57,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1203/2561 [15:05<16:54,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1204/2561 [15:05<16:50,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1205/2561 [15:06<16:50,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1206/2561 [15:07<16:46,  1.35it/s]\u001b[A\n",
            "Iteration:  47% 1207/2561 [15:08<16:49,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1208/2561 [15:08<16:46,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1209/2561 [15:09<16:50,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1210/2561 [15:10<16:50,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1211/2561 [15:11<16:46,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1212/2561 [15:11<16:45,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1213/2561 [15:12<16:40,  1.35it/s]\u001b[A\n",
            "Iteration:  47% 1214/2561 [15:13<16:43,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1215/2561 [15:14<16:41,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1216/2561 [15:14<16:45,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1217/2561 [15:15<16:45,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1218/2561 [15:16<16:48,  1.33it/s]\u001b[A\n",
            "Iteration:  48% 1219/2561 [15:17<16:45,  1.33it/s]\u001b[A\n",
            "Iteration:  48% 1220/2561 [15:17<16:39,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1221/2561 [15:18<16:40,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1222/2561 [15:19<16:36,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1223/2561 [15:20<16:39,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1224/2561 [15:20<16:37,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1225/2561 [15:21<16:41,  1.33it/s]\u001b[A\n",
            "Iteration:  48% 1226/2561 [15:22<16:39,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1227/2561 [15:23<16:33,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1228/2561 [15:23<16:35,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1229/2561 [15:24<16:30,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1230/2561 [15:25<16:33,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1231/2561 [15:26<16:30,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1232/2561 [15:26<16:34,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1233/2561 [15:27<16:32,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1234/2561 [15:28<16:34,  1.33it/s]\u001b[A\n",
            "Iteration:  48% 1235/2561 [15:29<16:32,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1236/2561 [15:29<16:30,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1237/2561 [15:30<16:31,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1238/2561 [15:31<16:27,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1239/2561 [15:32<16:28,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1240/2561 [15:32<16:25,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1241/2561 [15:33<16:29,  1.33it/s]\u001b[A\n",
            "Iteration:  48% 1242/2561 [15:34<16:26,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1243/2561 [15:35<16:25,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1244/2561 [15:35<16:24,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1245/2561 [15:36<16:19,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1246/2561 [15:37<16:22,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1247/2561 [15:37<16:21,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1248/2561 [15:38<16:22,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1249/2561 [15:39<16:19,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1250/2561 [15:40<16:22,  1.33it/s]\u001b[A\n",
            "Iteration:  49% 1251/2561 [15:40<16:20,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1252/2561 [15:41<16:15,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1253/2561 [15:42<16:15,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1254/2561 [15:43<16:12,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1255/2561 [15:43<16:15,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1256/2561 [15:44<16:14,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1257/2561 [15:45<16:15,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1258/2561 [15:46<16:13,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1259/2561 [15:46<16:17,  1.33it/s]\u001b[A\n",
            "Iteration:  49% 1260/2561 [15:47<16:15,  1.33it/s]\u001b[A\n",
            "Iteration:  49% 1261/2561 [15:48<16:08,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1262/2561 [15:49<16:08,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1263/2561 [15:49<16:05,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1264/2561 [15:50<16:07,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1265/2561 [15:51<16:06,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1266/2561 [15:52<16:08,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1267/2561 [15:52<16:09,  1.33it/s]\u001b[A\n",
            "Iteration:  50% 1268/2561 [15:53<16:01,  1.35it/s]\u001b[A\n",
            "Iteration:  50% 1269/2561 [15:54<15:57,  1.35it/s]\u001b[A\n",
            "Iteration:  50% 1270/2561 [15:55<15:56,  1.35it/s]\u001b[A\n",
            "Iteration:  50% 1271/2561 [15:55<16:02,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1272/2561 [15:56<16:00,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1273/2561 [15:57<15:57,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1274/2561 [15:58<15:59,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1275/2561 [15:58<15:54,  1.35it/s]\u001b[A\n",
            "Iteration:  50% 1276/2561 [15:59<15:56,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1277/2561 [16:00<15:54,  1.35it/s]\u001b[A\n",
            "Iteration:  50% 1278/2561 [16:01<15:57,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1279/2561 [16:01<15:55,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1280/2561 [16:02<15:53,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1281/2561 [16:03<15:53,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1282/2561 [16:04<15:49,  1.35it/s]\u001b[A\n",
            "Iteration:  50% 1283/2561 [16:04<15:52,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1284/2561 [16:05<15:50,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1285/2561 [16:06<15:53,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1286/2561 [16:07<15:52,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1287/2561 [16:07<15:55,  1.33it/s]\u001b[A\n",
            "Iteration:  50% 1288/2561 [16:08<15:53,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1289/2561 [16:09<15:47,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1290/2561 [16:10<15:48,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1291/2561 [16:10<15:44,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1292/2561 [16:11<15:47,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1293/2561 [16:12<15:45,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1294/2561 [16:13<15:48,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1295/2561 [16:13<15:47,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1296/2561 [16:14<15:40,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1297/2561 [16:15<15:40,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1298/2561 [16:16<15:40,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1299/2561 [16:16<15:42,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1300/2561 [16:17<15:40,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1301/2561 [16:18<15:43,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1302/2561 [16:19<15:42,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1303/2561 [16:19<15:42,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1304/2561 [16:20<15:40,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1305/2561 [16:21<15:34,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1306/2561 [16:22<15:37,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1307/2561 [16:22<15:34,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1308/2561 [16:23<15:37,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1309/2561 [16:24<15:34,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1310/2561 [16:25<15:36,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1311/2561 [16:25<15:36,  1.33it/s]\u001b[A\n",
            "Iteration:  51% 1312/2561 [16:26<15:34,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1313/2561 [16:27<15:33,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1314/2561 [16:27<15:28,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1315/2561 [16:28<15:29,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1316/2561 [16:29<15:27,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1317/2561 [16:30<15:30,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1318/2561 [16:30<15:26,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1319/2561 [16:31<15:25,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1320/2561 [16:32<15:25,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1321/2561 [16:33<15:21,  1.35it/s]\u001b[A\n",
            "Iteration:  52% 1322/2561 [16:33<15:23,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1323/2561 [16:34<15:20,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1324/2561 [16:35<15:23,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1325/2561 [16:36<15:22,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1326/2561 [16:36<15:25,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1327/2561 [16:37<15:23,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1328/2561 [16:38<15:25,  1.33it/s]\u001b[A\n",
            "Iteration:  52% 1329/2561 [16:39<15:23,  1.33it/s]\u001b[A\n",
            "Iteration:  52% 1330/2561 [16:39<15:18,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1331/2561 [16:40<15:19,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1332/2561 [16:41<15:17,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1333/2561 [16:42<15:20,  1.33it/s]\u001b[A\n",
            "Iteration:  52% 1334/2561 [16:42<15:17,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1335/2561 [16:43<15:19,  1.33it/s]\u001b[A\n",
            "Iteration:  52% 1336/2561 [16:44<15:17,  1.33it/s]\u001b[A\n",
            "Iteration:  52% 1337/2561 [16:45<15:18,  1.33it/s]\u001b[A\n",
            "Iteration:  52% 1338/2561 [16:45<15:17,  1.33it/s]\u001b[A\n",
            "Iteration:  52% 1339/2561 [16:46<15:11,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1340/2561 [16:47<15:12,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1341/2561 [16:48<15:08,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1342/2561 [16:48<15:11,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1343/2561 [16:49<15:09,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1344/2561 [16:50<15:12,  1.33it/s]\u001b[A\n",
            "Iteration:  53% 1345/2561 [16:51<15:10,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1346/2561 [16:51<15:12,  1.33it/s]\u001b[A\n",
            "Iteration:  53% 1347/2561 [16:52<15:13,  1.33it/s]\u001b[A\n",
            "Iteration:  53% 1348/2561 [16:53<15:14,  1.33it/s]\u001b[A\n",
            "Iteration:  53% 1349/2561 [16:54<15:13,  1.33it/s]\u001b[A\n",
            "Iteration:  53% 1350/2561 [16:54<15:06,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1351/2561 [16:55<15:06,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1352/2561 [16:56<15:02,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1353/2561 [16:57<15:04,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1354/2561 [16:57<15:01,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1355/2561 [16:58<15:03,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1356/2561 [16:59<15:02,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1357/2561 [17:00<15:00,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1358/2561 [17:00<15:02,  1.33it/s]\u001b[A\n",
            "Iteration:  53% 1359/2561 [17:01<14:56,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1360/2561 [17:02<14:57,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1361/2561 [17:03<14:54,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1362/2561 [17:03<14:57,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1363/2561 [17:04<14:52,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1364/2561 [17:05<14:55,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1365/2561 [17:06<14:54,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1366/2561 [17:06<14:55,  1.33it/s]\u001b[A\n",
            "Iteration:  53% 1367/2561 [17:07<14:54,  1.33it/s]\u001b[A\n",
            "Iteration:  53% 1368/2561 [17:08<14:51,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1369/2561 [17:09<14:50,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1370/2561 [17:09<14:47,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1371/2561 [17:10<14:49,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1372/2561 [17:11<14:45,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1373/2561 [17:12<14:48,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1374/2561 [17:12<14:48,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1375/2561 [17:13<14:48,  1.33it/s]\u001b[A\n",
            "Iteration:  54% 1376/2561 [17:14<14:47,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1377/2561 [17:15<14:45,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1378/2561 [17:15<14:46,  1.33it/s]\u001b[A\n",
            "Iteration:  54% 1379/2561 [17:16<14:41,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1380/2561 [17:17<14:43,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1381/2561 [17:18<14:39,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1382/2561 [17:18<14:41,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1383/2561 [17:19<14:40,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1384/2561 [17:20<14:37,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1385/2561 [17:21<14:38,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1386/2561 [17:21<14:34,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1387/2561 [17:22<14:35,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1388/2561 [17:23<14:34,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1389/2561 [17:24<14:36,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1390/2561 [17:24<14:35,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1391/2561 [17:25<14:37,  1.33it/s]\u001b[A\n",
            "Iteration:  54% 1392/2561 [17:26<14:35,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1393/2561 [17:27<14:38,  1.33it/s]\u001b[A\n",
            "Iteration:  54% 1394/2561 [17:27<14:35,  1.33it/s]\u001b[A\n",
            "Iteration:  54% 1395/2561 [17:28<14:31,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1396/2561 [17:29<14:32,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1397/2561 [17:30<14:29,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1398/2561 [17:30<14:30,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1399/2561 [17:31<14:27,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1400/2561 [17:32<14:30,  1.33it/s]\u001b[A\n",
            "Iteration:  55% 1401/2561 [17:33<14:27,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1402/2561 [17:33<14:29,  1.33it/s]\u001b[A\n",
            "Iteration:  55% 1403/2561 [17:34<14:28,  1.33it/s]\u001b[A\n",
            "Iteration:  55% 1404/2561 [17:35<14:30,  1.33it/s]\u001b[A\n",
            "Iteration:  55% 1405/2561 [17:36<14:28,  1.33it/s]\u001b[A\n",
            "Iteration:  55% 1406/2561 [17:36<14:22,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1407/2561 [17:37<14:22,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1408/2561 [17:38<14:19,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1409/2561 [17:39<14:21,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1410/2561 [17:39<14:19,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1411/2561 [17:40<14:22,  1.33it/s]\u001b[A\n",
            "Iteration:  55% 1412/2561 [17:41<14:19,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1413/2561 [17:42<14:22,  1.33it/s]\u001b[A\n",
            "Iteration:  55% 1414/2561 [17:42<14:20,  1.33it/s]\u001b[A\n",
            "Iteration:  55% 1415/2561 [17:43<14:18,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1416/2561 [17:44<14:18,  1.33it/s]\u001b[A\n",
            "Iteration:  55% 1417/2561 [17:45<14:13,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1418/2561 [17:45<14:14,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1419/2561 [17:46<14:11,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1420/2561 [17:47<14:14,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1421/2561 [17:48<14:11,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1422/2561 [17:48<14:13,  1.33it/s]\u001b[A\n",
            "Iteration:  56% 1423/2561 [17:49<14:12,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1424/2561 [17:50<14:14,  1.33it/s]\u001b[A\n",
            "Iteration:  56% 1425/2561 [17:51<14:12,  1.33it/s]\u001b[A\n",
            "Iteration:  56% 1426/2561 [17:51<14:13,  1.33it/s]\u001b[A\n",
            "Iteration:  56% 1427/2561 [17:52<14:11,  1.33it/s]\u001b[A\n",
            "Iteration:  56% 1428/2561 [17:53<14:14,  1.33it/s]\u001b[A\n",
            "Iteration:  56% 1429/2561 [17:54<14:12,  1.33it/s]\u001b[A\n",
            "Iteration:  56% 1430/2561 [17:54<14:06,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1431/2561 [17:55<14:06,  1.33it/s]\u001b[A\n",
            "Iteration:  56% 1432/2561 [17:56<14:01,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1433/2561 [17:57<14:03,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1434/2561 [17:57<14:01,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1435/2561 [17:58<14:03,  1.33it/s]\u001b[A\n",
            "Iteration:  56% 1436/2561 [17:59<14:01,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1437/2561 [18:00<14:02,  1.33it/s]\u001b[A\n",
            "Iteration:  56% 1438/2561 [18:00<14:00,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1439/2561 [18:01<13:56,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1440/2561 [18:02<13:57,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1441/2561 [18:02<13:54,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1442/2561 [18:03<13:56,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1443/2561 [18:04<13:54,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1444/2561 [18:05<13:56,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1445/2561 [18:05<13:54,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1446/2561 [18:06<13:56,  1.33it/s]\u001b[A\n",
            "Iteration:  57% 1447/2561 [18:07<13:55,  1.33it/s]\u001b[A\n",
            "Iteration:  57% 1448/2561 [18:08<13:50,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1449/2561 [18:08<13:51,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1450/2561 [18:09<13:48,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1451/2561 [18:10<13:50,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1452/2561 [18:11<13:47,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1453/2561 [18:11<13:49,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1454/2561 [18:12<13:47,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1455/2561 [18:13<13:45,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1456/2561 [18:14<13:45,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1457/2561 [18:14<13:42,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1458/2561 [18:15<13:44,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1459/2561 [18:16<13:41,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1460/2561 [18:17<13:44,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1461/2561 [18:17<13:42,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1462/2561 [18:18<13:44,  1.33it/s]\u001b[A\n",
            "Iteration:  57% 1463/2561 [18:19<13:43,  1.33it/s]\u001b[A\n",
            "Iteration:  57% 1464/2561 [18:20<13:44,  1.33it/s]\u001b[A\n",
            "Iteration:  57% 1465/2561 [18:20<13:43,  1.33it/s]\u001b[A\n",
            "Iteration:  57% 1466/2561 [18:21<13:39,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1467/2561 [18:22<13:38,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1468/2561 [18:23<13:34,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1469/2561 [18:23<13:36,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1470/2561 [18:24<13:34,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1471/2561 [18:25<13:36,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1472/2561 [18:26<13:34,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1473/2561 [18:26<13:36,  1.33it/s]\u001b[A\n",
            "Iteration:  58% 1474/2561 [18:27<13:34,  1.33it/s]\u001b[A\n",
            "Iteration:  58% 1475/2561 [18:28<13:33,  1.33it/s]\u001b[A\n",
            "Iteration:  58% 1476/2561 [18:29<13:32,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1477/2561 [18:29<13:29,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1478/2561 [18:30<13:30,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1479/2561 [18:31<13:28,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1480/2561 [18:32<13:29,  1.33it/s]\u001b[A\n",
            "Iteration:  58% 1481/2561 [18:32<13:26,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1482/2561 [18:33<13:28,  1.33it/s]\u001b[A\n",
            "Iteration:  58% 1483/2561 [18:34<13:26,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1484/2561 [18:35<13:26,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1485/2561 [18:35<13:25,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1486/2561 [18:36<13:21,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1487/2561 [18:37<13:23,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1488/2561 [18:38<13:21,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1489/2561 [18:38<13:23,  1.33it/s]\u001b[A\n",
            "Iteration:  58% 1490/2561 [18:39<13:18,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1491/2561 [18:40<13:21,  1.33it/s]\u001b[A\n",
            "Iteration:  58% 1492/2561 [18:41<13:19,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1493/2561 [18:41<13:17,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1494/2561 [18:42<13:17,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1495/2561 [18:43<13:13,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1496/2561 [18:44<13:14,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1497/2561 [18:44<13:12,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1498/2561 [18:45<13:13,  1.34it/s]\u001b[A\n",
            "Iteration:  59% 1499/2561 [18:46<13:13,  1.34it/s]\u001b[A02/29/2020 07:46:55 - INFO - transformers.configuration_utils -   Configuration saved in output_roberta_GB/checkpoint-1500/config.json\n",
            "02/29/2020 07:46:57 - INFO - transformers.modeling_utils -   Model weights saved in output_roberta_GB/checkpoint-1500/pytorch_model.bin\n",
            "02/29/2020 07:46:57 - INFO - __main__ -   Saving model checkpoint to output_roberta_GB/checkpoint-1500\n",
            "02/29/2020 07:47:01 - INFO - __main__ -   Saving optimizer and scheduler states to output_roberta_GB/checkpoint-1500\n",
            "\n",
            "Iteration:  59% 1500/2561 [18:52<41:30,  2.35s/it]\u001b[A\n",
            "Iteration:  59% 1501/2561 [18:53<33:06,  1.87s/it]\u001b[A\n",
            "Iteration:  59% 1502/2561 [18:53<27:03,  1.53s/it]\u001b[A\n",
            "Iteration:  59% 1503/2561 [18:54<22:51,  1.30s/it]\u001b[A\n",
            "Iteration:  59% 1504/2561 [18:55<19:54,  1.13s/it]\u001b[A\n",
            "Iteration:  59% 1505/2561 [18:56<17:54,  1.02s/it]\u001b[A\n",
            "Iteration:  59% 1506/2561 [18:56<16:26,  1.07it/s]\u001b[A\n",
            "Iteration:  59% 1507/2561 [18:57<15:26,  1.14it/s]\u001b[A\n",
            "Iteration:  59% 1508/2561 [18:58<14:44,  1.19it/s]\u001b[A\n",
            "Iteration:  59% 1509/2561 [18:59<14:10,  1.24it/s]\u001b[A\n",
            "Iteration:  59% 1510/2561 [18:59<13:50,  1.26it/s]\u001b[A\n",
            "Iteration:  59% 1511/2561 [19:00<13:34,  1.29it/s]\u001b[A\n",
            "Iteration:  59% 1512/2561 [19:01<13:27,  1.30it/s]\u001b[A\n",
            "Iteration:  59% 1513/2561 [19:02<13:18,  1.31it/s]\u001b[A\n",
            "Iteration:  59% 1514/2561 [19:02<13:14,  1.32it/s]\u001b[A\n",
            "Iteration:  59% 1515/2561 [19:03<13:10,  1.32it/s]\u001b[A\n",
            "Iteration:  59% 1516/2561 [19:04<13:09,  1.32it/s]\u001b[A\n",
            "Iteration:  59% 1517/2561 [19:05<13:06,  1.33it/s]\u001b[A\n",
            "Iteration:  59% 1518/2561 [19:05<13:00,  1.34it/s]\u001b[A\n",
            "Iteration:  59% 1519/2561 [19:06<13:00,  1.34it/s]\u001b[A\n",
            "Iteration:  59% 1520/2561 [19:07<12:57,  1.34it/s]\u001b[A\n",
            "Iteration:  59% 1521/2561 [19:08<12:59,  1.33it/s]\u001b[A\n",
            "Iteration:  59% 1522/2561 [19:08<12:55,  1.34it/s]\u001b[A\n",
            "Iteration:  59% 1523/2561 [19:09<12:56,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1524/2561 [19:10<12:54,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1525/2561 [19:11<12:54,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1526/2561 [19:11<12:54,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1527/2561 [19:12<12:51,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1528/2561 [19:13<12:52,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1529/2561 [19:14<12:48,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1530/2561 [19:14<12:50,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1531/2561 [19:15<12:49,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1532/2561 [19:16<12:51,  1.33it/s]\u001b[A\n",
            "Iteration:  60% 1533/2561 [19:17<12:49,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1534/2561 [19:17<12:48,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1535/2561 [19:18<12:48,  1.33it/s]\u001b[A\n",
            "Iteration:  60% 1536/2561 [19:19<12:44,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1537/2561 [19:20<12:44,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1538/2561 [19:20<12:41,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1539/2561 [19:21<12:43,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1540/2561 [19:22<12:41,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1541/2561 [19:23<12:46,  1.33it/s]\u001b[A\n",
            "Iteration:  60% 1542/2561 [19:23<12:47,  1.33it/s]\u001b[A\n",
            "Iteration:  60% 1543/2561 [19:24<12:48,  1.32it/s]\u001b[A\n",
            "Iteration:  60% 1544/2561 [19:25<12:46,  1.33it/s]\u001b[A\n",
            "Iteration:  60% 1545/2561 [19:26<12:47,  1.32it/s]\u001b[A\n",
            "Iteration:  60% 1546/2561 [19:26<12:44,  1.33it/s]\u001b[A\n",
            "Iteration:  60% 1547/2561 [19:27<12:43,  1.33it/s]\u001b[A\n",
            "Iteration:  60% 1548/2561 [19:28<12:40,  1.33it/s]\u001b[A\n",
            "Iteration:  60% 1549/2561 [19:29<12:36,  1.34it/s]\u001b[A\n",
            "Iteration:  61% 1550/2561 [19:29<12:39,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1551/2561 [19:30<12:34,  1.34it/s]\u001b[A\n",
            "Iteration:  61% 1552/2561 [19:31<12:37,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1553/2561 [19:32<12:34,  1.34it/s]\u001b[A\n",
            "Iteration:  61% 1554/2561 [19:32<12:36,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1555/2561 [19:33<12:35,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1556/2561 [19:34<12:36,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1557/2561 [19:35<12:35,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1558/2561 [19:35<12:33,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1559/2561 [19:36<12:33,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1560/2561 [19:37<12:28,  1.34it/s]\u001b[A\n",
            "Iteration:  61% 1561/2561 [19:38<12:29,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1562/2561 [19:38<12:26,  1.34it/s]\u001b[A\n",
            "Iteration:  61% 1563/2561 [19:39<12:27,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1564/2561 [19:40<12:25,  1.34it/s]\u001b[A\n",
            "Iteration:  61% 1565/2561 [19:41<12:27,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1566/2561 [19:41<12:26,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1567/2561 [19:42<12:24,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1568/2561 [19:43<12:23,  1.34it/s]\u001b[A\n",
            "Iteration:  61% 1569/2561 [19:44<12:19,  1.34it/s]\u001b[A\n",
            "Iteration:  61% 1570/2561 [19:44<12:20,  1.34it/s]\u001b[A\n",
            "Iteration:  61% 1571/2561 [19:45<12:18,  1.34it/s]\u001b[A\n",
            "Iteration:  61% 1572/2561 [19:46<12:19,  1.34it/s]\u001b[A\n",
            "Iteration:  61% 1573/2561 [19:47<12:18,  1.34it/s]\u001b[A\n",
            "Iteration:  61% 1574/2561 [19:47<12:20,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1575/2561 [19:48<12:19,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 1576/2561 [19:49<12:20,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 1577/2561 [19:50<12:19,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 1578/2561 [19:50<12:14,  1.34it/s]\u001b[A\n",
            "Iteration:  62% 1579/2561 [19:51<12:14,  1.34it/s]\u001b[A\n",
            "Iteration:  62% 1580/2561 [19:52<12:10,  1.34it/s]\u001b[A\n",
            "Iteration:  62% 1581/2561 [19:53<12:13,  1.34it/s]\u001b[A\n",
            "Iteration:  62% 1582/2561 [19:53<12:11,  1.34it/s]\u001b[A\n",
            "Iteration:  62% 1583/2561 [19:54<12:13,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 1584/2561 [19:55<12:10,  1.34it/s]\u001b[A\n",
            "Iteration:  62% 1585/2561 [19:56<12:12,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 1586/2561 [19:56<12:10,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 1587/2561 [19:57<12:11,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 1588/2561 [19:58<12:10,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 1589/2561 [19:59<12:04,  1.34it/s]\u001b[A\n",
            "Iteration:  62% 1590/2561 [19:59<12:05,  1.34it/s]\u001b[A\n",
            "Iteration:  62% 1591/2561 [20:00<12:02,  1.34it/s]\u001b[A\n",
            "Iteration:  62% 1592/2561 [20:01<12:05,  1.34it/s]\u001b[A\n",
            "Iteration:  62% 1593/2561 [20:02<12:02,  1.34it/s]\u001b[A\n",
            "Iteration:  62% 1594/2561 [20:02<12:04,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 1595/2561 [20:03<12:03,  1.34it/s]\u001b[A\n",
            "Iteration:  62% 1596/2561 [20:04<12:05,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 1597/2561 [20:05<12:02,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 1598/2561 [20:05<12:05,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 1599/2561 [20:06<12:04,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 1600/2561 [20:07<11:58,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1601/2561 [20:08<11:58,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1602/2561 [20:08<11:55,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1603/2561 [20:09<11:57,  1.33it/s]\u001b[A\n",
            "Iteration:  63% 1604/2561 [20:10<11:54,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1605/2561 [20:11<11:56,  1.33it/s]\u001b[A\n",
            "Iteration:  63% 1606/2561 [20:11<11:54,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1607/2561 [20:12<11:55,  1.33it/s]\u001b[A\n",
            "Iteration:  63% 1608/2561 [20:13<11:54,  1.33it/s]\u001b[A\n",
            "Iteration:  63% 1609/2561 [20:14<11:55,  1.33it/s]\u001b[A\n",
            "Iteration:  63% 1610/2561 [20:14<11:53,  1.33it/s]\u001b[A\n",
            "Iteration:  63% 1611/2561 [20:15<11:50,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1612/2561 [20:16<11:51,  1.33it/s]\u001b[A\n",
            "Iteration:  63% 1613/2561 [20:17<11:48,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1614/2561 [20:17<11:48,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1615/2561 [20:18<11:46,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1616/2561 [20:19<11:48,  1.33it/s]\u001b[A\n",
            "Iteration:  63% 1617/2561 [20:20<11:46,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1618/2561 [20:20<11:41,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1619/2561 [20:21<11:42,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1620/2561 [20:22<11:39,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1621/2561 [20:23<11:43,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1622/2561 [20:23<11:42,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1623/2561 [20:24<11:44,  1.33it/s]\u001b[A\n",
            "Iteration:  63% 1624/2561 [20:25<11:43,  1.33it/s]\u001b[A\n",
            "Iteration:  63% 1625/2561 [20:26<11:44,  1.33it/s]\u001b[A\n",
            "Iteration:  63% 1626/2561 [20:26<11:42,  1.33it/s]\u001b[A\n",
            "Iteration:  64% 1627/2561 [20:27<11:42,  1.33it/s]\u001b[A\n",
            "Iteration:  64% 1628/2561 [20:28<11:41,  1.33it/s]\u001b[A\n",
            "Iteration:  64% 1629/2561 [20:29<11:45,  1.32it/s]\u001b[A\n",
            "Iteration:  64% 1630/2561 [20:29<11:41,  1.33it/s]\u001b[A\n",
            "Iteration:  64% 1631/2561 [20:30<11:42,  1.32it/s]\u001b[A\n",
            "Iteration:  64% 1632/2561 [20:31<11:39,  1.33it/s]\u001b[A\n",
            "Iteration:  64% 1633/2561 [20:32<11:40,  1.32it/s]\u001b[A\n",
            "Iteration:  64% 1634/2561 [20:32<11:37,  1.33it/s]\u001b[A\n",
            "Iteration:  64% 1635/2561 [20:33<11:32,  1.34it/s]\u001b[A\n",
            "Iteration:  64% 1636/2561 [20:34<11:32,  1.33it/s]\u001b[A\n",
            "Iteration:  64% 1637/2561 [20:35<11:29,  1.34it/s]\u001b[A\n",
            "Iteration:  64% 1638/2561 [20:35<11:29,  1.34it/s]\u001b[A\n",
            "Iteration:  64% 1639/2561 [20:36<11:28,  1.34it/s]\u001b[A\n",
            "Iteration:  64% 1640/2561 [20:37<11:30,  1.33it/s]\u001b[A\n",
            "Iteration:  64% 1641/2561 [20:38<11:28,  1.34it/s]\u001b[A\n",
            "Iteration:  64% 1642/2561 [20:38<11:29,  1.33it/s]\u001b[A\n",
            "Iteration:  64% 1643/2561 [20:39<11:27,  1.33it/s]\u001b[A\n",
            "Iteration:  64% 1644/2561 [20:40<11:22,  1.34it/s]\u001b[A\n",
            "Iteration:  64% 1645/2561 [20:41<11:23,  1.34it/s]\u001b[A\n",
            "Iteration:  64% 1646/2561 [20:41<11:21,  1.34it/s]\u001b[A\n",
            "Iteration:  64% 1647/2561 [20:42<11:24,  1.34it/s]\u001b[A\n",
            "Iteration:  64% 1648/2561 [20:43<11:21,  1.34it/s]\u001b[A\n",
            "Iteration:  64% 1649/2561 [20:44<11:22,  1.34it/s]\u001b[A\n",
            "Iteration:  64% 1650/2561 [20:44<11:21,  1.34it/s]\u001b[A\n",
            "Iteration:  64% 1651/2561 [20:45<11:23,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1652/2561 [20:46<11:23,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1653/2561 [20:47<11:24,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1654/2561 [20:47<11:22,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1655/2561 [20:48<11:23,  1.32it/s]\u001b[A\n",
            "Iteration:  65% 1656/2561 [20:49<11:20,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1657/2561 [20:50<11:15,  1.34it/s]\u001b[A\n",
            "Iteration:  65% 1658/2561 [20:50<11:16,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1659/2561 [20:51<11:14,  1.34it/s]\u001b[A\n",
            "Iteration:  65% 1660/2561 [20:52<11:15,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1661/2561 [20:53<11:14,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1662/2561 [20:53<11:15,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1663/2561 [20:54<11:12,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1664/2561 [20:55<11:14,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1665/2561 [20:56<11:12,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1666/2561 [20:56<11:15,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1667/2561 [20:57<11:12,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1668/2561 [20:58<11:07,  1.34it/s]\u001b[A\n",
            "Iteration:  65% 1669/2561 [20:59<11:08,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1670/2561 [20:59<11:05,  1.34it/s]\u001b[A\n",
            "Iteration:  65% 1671/2561 [21:00<11:07,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1672/2561 [21:01<11:04,  1.34it/s]\u001b[A\n",
            "Iteration:  65% 1673/2561 [21:02<11:05,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1674/2561 [21:02<11:03,  1.34it/s]\u001b[A\n",
            "Iteration:  65% 1675/2561 [21:03<11:04,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1676/2561 [21:04<11:02,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1677/2561 [21:05<11:04,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 1678/2561 [21:05<11:02,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 1679/2561 [21:06<11:03,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 1680/2561 [21:07<11:03,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 1681/2561 [21:08<10:59,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 1682/2561 [21:08<10:58,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 1683/2561 [21:09<10:55,  1.34it/s]\u001b[A\n",
            "Iteration:  66% 1684/2561 [21:10<10:56,  1.34it/s]\u001b[A\n",
            "Iteration:  66% 1685/2561 [21:11<10:52,  1.34it/s]\u001b[A\n",
            "Iteration:  66% 1686/2561 [21:11<10:54,  1.34it/s]\u001b[A\n",
            "Iteration:  66% 1687/2561 [21:12<10:52,  1.34it/s]\u001b[A\n",
            "Iteration:  66% 1688/2561 [21:13<10:55,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 1689/2561 [21:14<10:53,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 1690/2561 [21:14<10:49,  1.34it/s]\u001b[A\n",
            "Iteration:  66% 1691/2561 [21:15<10:50,  1.34it/s]\u001b[A\n",
            "Iteration:  66% 1692/2561 [21:16<10:48,  1.34it/s]\u001b[A\n",
            "Iteration:  66% 1693/2561 [21:17<10:49,  1.34it/s]\u001b[A\n",
            "Iteration:  66% 1694/2561 [21:17<10:47,  1.34it/s]\u001b[A\n",
            "Iteration:  66% 1695/2561 [21:18<10:48,  1.34it/s]\u001b[A\n",
            "Iteration:  66% 1696/2561 [21:19<10:46,  1.34it/s]\u001b[A\n",
            "Iteration:  66% 1697/2561 [21:20<10:48,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 1698/2561 [21:20<10:46,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 1699/2561 [21:21<10:47,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 1700/2561 [21:22<10:46,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 1701/2561 [21:23<10:42,  1.34it/s]\u001b[A\n",
            "Iteration:  66% 1702/2561 [21:23<10:43,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 1703/2561 [21:24<10:40,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1704/2561 [21:25<10:41,  1.33it/s]\u001b[A\n",
            "Iteration:  67% 1705/2561 [21:25<10:38,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1706/2561 [21:26<10:40,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1707/2561 [21:27<10:37,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1708/2561 [21:28<10:38,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1709/2561 [21:28<10:38,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1710/2561 [21:29<10:41,  1.33it/s]\u001b[A\n",
            "Iteration:  67% 1711/2561 [21:30<10:39,  1.33it/s]\u001b[A\n",
            "Iteration:  67% 1712/2561 [21:31<10:40,  1.33it/s]\u001b[A\n",
            "Iteration:  67% 1713/2561 [21:32<10:38,  1.33it/s]\u001b[A\n",
            "Iteration:  67% 1714/2561 [21:32<10:38,  1.33it/s]\u001b[A\n",
            "Iteration:  67% 1715/2561 [21:33<10:35,  1.33it/s]\u001b[A\n",
            "Iteration:  67% 1716/2561 [21:34<10:31,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1717/2561 [21:35<10:31,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1718/2561 [21:35<10:28,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1719/2561 [21:36<10:30,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1720/2561 [21:37<10:27,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1721/2561 [21:37<10:29,  1.33it/s]\u001b[A\n",
            "Iteration:  67% 1722/2561 [21:38<10:27,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1723/2561 [21:39<10:28,  1.33it/s]\u001b[A\n",
            "Iteration:  67% 1724/2561 [21:40<10:27,  1.33it/s]\u001b[A\n",
            "Iteration:  67% 1725/2561 [21:40<10:22,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1726/2561 [21:41<10:23,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1727/2561 [21:42<10:20,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1728/2561 [21:43<10:21,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1729/2561 [21:43<10:20,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1730/2561 [21:44<10:21,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1731/2561 [21:45<10:19,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1732/2561 [21:46<10:21,  1.33it/s]\u001b[A\n",
            "Iteration:  68% 1733/2561 [21:46<10:21,  1.33it/s]\u001b[A\n",
            "Iteration:  68% 1734/2561 [21:47<10:16,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1735/2561 [21:48<10:16,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1736/2561 [21:49<10:14,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1737/2561 [21:49<10:15,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1738/2561 [21:50<10:13,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1739/2561 [21:51<10:15,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1740/2561 [21:52<10:14,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1741/2561 [21:52<10:17,  1.33it/s]\u001b[A\n",
            "Iteration:  68% 1742/2561 [21:53<10:15,  1.33it/s]\u001b[A\n",
            "Iteration:  68% 1743/2561 [21:54<10:16,  1.33it/s]\u001b[A\n",
            "Iteration:  68% 1744/2561 [21:55<10:14,  1.33it/s]\u001b[A\n",
            "Iteration:  68% 1745/2561 [21:55<10:10,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1746/2561 [21:56<10:09,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1747/2561 [21:57<10:06,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1748/2561 [21:58<10:08,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1749/2561 [21:58<10:05,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1750/2561 [21:59<10:07,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1751/2561 [22:00<10:05,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1752/2561 [22:01<10:04,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1753/2561 [22:01<10:03,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1754/2561 [22:02<09:59,  1.35it/s]\u001b[A\n",
            "Iteration:  69% 1755/2561 [22:03<10:01,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1756/2561 [22:04<09:59,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1757/2561 [22:04<10:00,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1758/2561 [22:05<09:59,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1759/2561 [22:06<10:00,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1760/2561 [22:07<09:59,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1761/2561 [22:07<09:58,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1762/2561 [22:08<09:58,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1763/2561 [22:09<09:54,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1764/2561 [22:10<09:54,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1765/2561 [22:10<09:52,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1766/2561 [22:11<09:54,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1767/2561 [22:12<09:52,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1768/2561 [22:13<09:53,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1769/2561 [22:13<09:51,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1770/2561 [22:14<09:50,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1771/2561 [22:15<09:50,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1772/2561 [22:16<09:47,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1773/2561 [22:16<09:49,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1774/2561 [22:17<09:46,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1775/2561 [22:18<09:47,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1776/2561 [22:19<09:45,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1777/2561 [22:19<09:47,  1.33it/s]\u001b[A\n",
            "Iteration:  69% 1778/2561 [22:20<09:46,  1.33it/s]\u001b[A\n",
            "Iteration:  69% 1779/2561 [22:21<09:43,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1780/2561 [22:22<09:43,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1781/2561 [22:22<09:41,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1782/2561 [22:23<09:42,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1783/2561 [22:24<09:40,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1784/2561 [22:25<09:41,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1785/2561 [22:25<09:39,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1786/2561 [22:26<09:41,  1.33it/s]\u001b[A\n",
            "Iteration:  70% 1787/2561 [22:27<09:40,  1.33it/s]\u001b[A\n",
            "Iteration:  70% 1788/2561 [22:28<09:36,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1789/2561 [22:28<09:36,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1790/2561 [22:29<09:35,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1791/2561 [22:30<09:36,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1792/2561 [22:31<09:33,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1793/2561 [22:31<09:34,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1794/2561 [22:32<09:33,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1795/2561 [22:33<09:35,  1.33it/s]\u001b[A\n",
            "Iteration:  70% 1796/2561 [22:34<09:34,  1.33it/s]\u001b[A\n",
            "Iteration:  70% 1797/2561 [22:34<09:29,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1798/2561 [22:35<09:29,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1799/2561 [22:36<09:27,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1800/2561 [22:37<09:28,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1801/2561 [22:37<09:27,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1802/2561 [22:38<09:28,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1803/2561 [22:39<09:26,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1804/2561 [22:40<09:28,  1.33it/s]\u001b[A\n",
            "Iteration:  70% 1805/2561 [22:40<09:27,  1.33it/s]\u001b[A\n",
            "Iteration:  71% 1806/2561 [22:41<09:28,  1.33it/s]\u001b[A\n",
            "Iteration:  71% 1807/2561 [22:42<09:26,  1.33it/s]\u001b[A\n",
            "Iteration:  71% 1808/2561 [22:43<09:21,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1809/2561 [22:43<09:21,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1810/2561 [22:44<09:18,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1811/2561 [22:45<09:20,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1812/2561 [22:46<09:18,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1813/2561 [22:46<09:19,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1814/2561 [22:47<09:17,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1815/2561 [22:48<09:18,  1.33it/s]\u001b[A\n",
            "Iteration:  71% 1816/2561 [22:49<09:19,  1.33it/s]\u001b[A\n",
            "Iteration:  71% 1817/2561 [22:49<09:19,  1.33it/s]\u001b[A\n",
            "Iteration:  71% 1818/2561 [22:50<09:18,  1.33it/s]\u001b[A\n",
            "Iteration:  71% 1819/2561 [22:51<09:13,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1820/2561 [22:52<09:13,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1821/2561 [22:52<09:12,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1822/2561 [22:53<09:13,  1.33it/s]\u001b[A\n",
            "Iteration:  71% 1823/2561 [22:54<09:11,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1824/2561 [22:55<09:12,  1.33it/s]\u001b[A\n",
            "Iteration:  71% 1825/2561 [22:55<09:10,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1826/2561 [22:56<09:11,  1.33it/s]\u001b[A\n",
            "Iteration:  71% 1827/2561 [22:57<09:09,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1828/2561 [22:58<09:10,  1.33it/s]\u001b[A\n",
            "Iteration:  71% 1829/2561 [22:58<09:08,  1.33it/s]\u001b[A\n",
            "Iteration:  71% 1830/2561 [22:59<09:04,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1831/2561 [23:00<09:05,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1832/2561 [23:00<09:02,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1833/2561 [23:01<09:03,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1834/2561 [23:02<09:01,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1835/2561 [23:03<09:03,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1836/2561 [23:03<09:02,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1837/2561 [23:04<09:01,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1838/2561 [23:05<09:00,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1839/2561 [23:06<08:58,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1840/2561 [23:06<08:59,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1841/2561 [23:07<08:56,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1842/2561 [23:08<08:57,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1843/2561 [23:09<08:54,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1844/2561 [23:09<08:56,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1845/2561 [23:10<08:55,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1846/2561 [23:11<08:54,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1847/2561 [23:12<08:54,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1848/2561 [23:12<08:51,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1849/2561 [23:13<08:51,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1850/2561 [23:14<08:49,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1851/2561 [23:15<08:51,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1852/2561 [23:15<08:49,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1853/2561 [23:16<08:51,  1.33it/s]\u001b[A\n",
            "Iteration:  72% 1854/2561 [23:17<08:50,  1.33it/s]\u001b[A\n",
            "Iteration:  72% 1855/2561 [23:18<08:50,  1.33it/s]\u001b[A\n",
            "Iteration:  72% 1856/2561 [23:18<08:49,  1.33it/s]\u001b[A\n",
            "Iteration:  73% 1857/2561 [23:19<08:46,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1858/2561 [23:20<08:46,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1859/2561 [23:21<08:42,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1860/2561 [23:21<08:43,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1861/2561 [23:22<08:42,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1862/2561 [23:23<08:44,  1.33it/s]\u001b[A\n",
            "Iteration:  73% 1863/2561 [23:24<08:41,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1864/2561 [23:24<08:42,  1.33it/s]\u001b[A\n",
            "Iteration:  73% 1865/2561 [23:25<08:41,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1866/2561 [23:26<08:39,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1867/2561 [23:27<08:38,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1868/2561 [23:27<08:36,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1869/2561 [23:28<08:38,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1870/2561 [23:29<08:36,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1871/2561 [23:30<08:37,  1.33it/s]\u001b[A\n",
            "Iteration:  73% 1872/2561 [23:30<08:34,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1873/2561 [23:31<08:36,  1.33it/s]\u001b[A\n",
            "Iteration:  73% 1874/2561 [23:32<08:34,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1875/2561 [23:33<08:35,  1.33it/s]\u001b[A\n",
            "Iteration:  73% 1876/2561 [23:33<08:34,  1.33it/s]\u001b[A\n",
            "Iteration:  73% 1877/2561 [23:34<08:30,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1878/2561 [23:35<08:30,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1879/2561 [23:36<08:27,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1880/2561 [23:36<08:28,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1881/2561 [23:37<08:27,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1882/2561 [23:38<08:28,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1883/2561 [23:39<08:26,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1884/2561 [23:39<08:28,  1.33it/s]\u001b[A\n",
            "Iteration:  74% 1885/2561 [23:40<08:27,  1.33it/s]\u001b[A\n",
            "Iteration:  74% 1886/2561 [23:41<08:27,  1.33it/s]\u001b[A\n",
            "Iteration:  74% 1887/2561 [23:42<08:25,  1.33it/s]\u001b[A\n",
            "Iteration:  74% 1888/2561 [23:42<08:22,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1889/2561 [23:43<08:23,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1890/2561 [23:44<08:20,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1891/2561 [23:45<08:20,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1892/2561 [23:45<08:18,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1893/2561 [23:46<08:19,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1894/2561 [23:47<08:18,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1895/2561 [23:48<08:19,  1.33it/s]\u001b[A\n",
            "Iteration:  74% 1896/2561 [23:48<08:18,  1.33it/s]\u001b[A\n",
            "Iteration:  74% 1897/2561 [23:49<08:14,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1898/2561 [23:50<08:15,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1899/2561 [23:51<08:13,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1900/2561 [23:51<08:14,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1901/2561 [23:52<08:12,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1902/2561 [23:53<08:14,  1.33it/s]\u001b[A\n",
            "Iteration:  74% 1903/2561 [23:54<08:12,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1904/2561 [23:54<08:11,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1905/2561 [23:55<08:11,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1906/2561 [23:56<08:08,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1907/2561 [23:57<08:08,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1908/2561 [23:57<08:06,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1909/2561 [23:58<08:08,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1910/2561 [23:59<08:06,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1911/2561 [24:00<08:06,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1912/2561 [24:00<08:06,  1.33it/s]\u001b[A\n",
            "Iteration:  75% 1913/2561 [24:01<08:04,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1914/2561 [24:02<08:04,  1.33it/s]\u001b[A\n",
            "Iteration:  75% 1915/2561 [24:03<08:02,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1916/2561 [24:03<08:03,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1917/2561 [24:04<07:59,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1918/2561 [24:05<08:00,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1919/2561 [24:06<07:59,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1920/2561 [24:06<08:00,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1921/2561 [24:07<07:58,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1922/2561 [24:08<07:59,  1.33it/s]\u001b[A\n",
            "Iteration:  75% 1923/2561 [24:09<07:57,  1.33it/s]\u001b[A\n",
            "Iteration:  75% 1924/2561 [24:09<07:57,  1.33it/s]\u001b[A\n",
            "Iteration:  75% 1925/2561 [24:10<07:56,  1.33it/s]\u001b[A\n",
            "Iteration:  75% 1926/2561 [24:11<07:54,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1927/2561 [24:12<07:53,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1928/2561 [24:12<07:51,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1929/2561 [24:13<07:52,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1930/2561 [24:14<07:50,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1931/2561 [24:15<07:51,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1932/2561 [24:15<07:50,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1933/2561 [24:16<07:50,  1.33it/s]\u001b[A\n",
            "Iteration:  76% 1934/2561 [24:17<07:49,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1935/2561 [24:18<07:45,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1936/2561 [24:18<07:46,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1937/2561 [24:19<07:44,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1938/2561 [24:20<07:44,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1939/2561 [24:20<07:43,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1940/2561 [24:21<07:42,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1941/2561 [24:22<07:41,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1942/2561 [24:23<07:39,  1.35it/s]\u001b[A\n",
            "Iteration:  76% 1943/2561 [24:23<07:40,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1944/2561 [24:24<07:38,  1.35it/s]\u001b[A\n",
            "Iteration:  76% 1945/2561 [24:25<07:39,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1946/2561 [24:26<07:37,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1947/2561 [24:26<07:39,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1948/2561 [24:27<07:38,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1949/2561 [24:28<07:38,  1.33it/s]\u001b[A\n",
            "Iteration:  76% 1950/2561 [24:29<07:37,  1.33it/s]\u001b[A\n",
            "Iteration:  76% 1951/2561 [24:29<07:35,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1952/2561 [24:30<07:35,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1953/2561 [24:31<07:33,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1954/2561 [24:32<07:33,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1955/2561 [24:32<07:31,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1956/2561 [24:33<07:32,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1957/2561 [24:34<07:30,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1958/2561 [24:35<07:31,  1.33it/s]\u001b[A\n",
            "Iteration:  76% 1959/2561 [24:35<07:30,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1960/2561 [24:36<07:30,  1.33it/s]\u001b[A\n",
            "Iteration:  77% 1961/2561 [24:37<07:29,  1.33it/s]\u001b[A\n",
            "Iteration:  77% 1962/2561 [24:38<07:26,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1963/2561 [24:38<07:26,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1964/2561 [24:39<07:24,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1965/2561 [24:40<07:25,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1966/2561 [24:41<07:23,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1967/2561 [24:41<07:25,  1.33it/s]\u001b[A\n",
            "Iteration:  77% 1968/2561 [24:42<07:23,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1969/2561 [24:43<07:20,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1970/2561 [24:44<07:19,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1971/2561 [24:44<07:17,  1.35it/s]\u001b[A\n",
            "Iteration:  77% 1972/2561 [24:45<07:18,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1973/2561 [24:46<07:17,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1974/2561 [24:47<07:18,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1975/2561 [24:47<07:18,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1976/2561 [24:48<07:17,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1977/2561 [24:49<07:17,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1978/2561 [24:50<07:14,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1979/2561 [24:50<07:15,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1980/2561 [24:51<07:13,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1981/2561 [24:52<07:13,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1982/2561 [24:53<07:12,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1983/2561 [24:53<07:11,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1984/2561 [24:54<07:11,  1.34it/s]\u001b[A\n",
            "Iteration:  78% 1985/2561 [24:55<07:09,  1.34it/s]\u001b[A\n",
            "Iteration:  78% 1986/2561 [24:56<07:09,  1.34it/s]\u001b[A\n",
            "Iteration:  78% 1987/2561 [24:56<07:07,  1.34it/s]\u001b[A\n",
            "Iteration:  78% 1988/2561 [24:57<07:08,  1.34it/s]\u001b[A\n",
            "Iteration:  78% 1989/2561 [24:58<07:06,  1.34it/s]\u001b[A\n",
            "Iteration:  78% 1990/2561 [24:59<07:07,  1.34it/s]\u001b[A\n",
            "Iteration:  78% 1991/2561 [24:59<07:06,  1.34it/s]\u001b[A\n",
            "Iteration:  78% 1992/2561 [25:00<07:06,  1.33it/s]\u001b[A\n",
            "Iteration:  78% 1993/2561 [25:01<07:05,  1.33it/s]\u001b[A\n",
            "Iteration:  78% 1994/2561 [25:02<07:04,  1.34it/s]\u001b[A\n",
            "Iteration:  78% 1995/2561 [25:02<07:03,  1.34it/s]\u001b[A\n",
            "Iteration:  78% 1996/2561 [25:03<07:00,  1.34it/s]\u001b[A\n",
            "Iteration:  78% 1997/2561 [25:04<07:01,  1.34it/s]\u001b[A\n",
            "Iteration:  78% 1998/2561 [25:05<06:59,  1.34it/s]\u001b[A\n",
            "Iteration:  78% 1999/2561 [25:05<07:00,  1.34it/s]\u001b[A02/29/2020 07:53:15 - INFO - transformers.configuration_utils -   Configuration saved in output_roberta_GB/checkpoint-2000/config.json\n",
            "02/29/2020 07:53:16 - INFO - transformers.modeling_utils -   Model weights saved in output_roberta_GB/checkpoint-2000/pytorch_model.bin\n",
            "02/29/2020 07:53:16 - INFO - __main__ -   Saving model checkpoint to output_roberta_GB/checkpoint-2000\n",
            "02/29/2020 07:53:20 - INFO - __main__ -   Saving optimizer and scheduler states to output_roberta_GB/checkpoint-2000\n",
            "\n",
            "Iteration:  78% 2000/2561 [25:11<21:42,  2.32s/it]\u001b[A\n",
            "Iteration:  78% 2001/2561 [25:12<17:18,  1.85s/it]\u001b[A\n",
            "Iteration:  78% 2002/2561 [25:13<14:08,  1.52s/it]\u001b[A\n",
            "Iteration:  78% 2003/2561 [25:14<11:58,  1.29s/it]\u001b[A\n",
            "Iteration:  78% 2004/2561 [25:14<10:26,  1.12s/it]\u001b[A\n",
            "Iteration:  78% 2005/2561 [25:15<09:22,  1.01s/it]\u001b[A\n",
            "Iteration:  78% 2006/2561 [25:16<08:36,  1.07it/s]\u001b[A\n",
            "Iteration:  78% 2007/2561 [25:17<08:06,  1.14it/s]\u001b[A\n",
            "Iteration:  78% 2008/2561 [25:17<07:43,  1.19it/s]\u001b[A\n",
            "Iteration:  78% 2009/2561 [25:18<07:28,  1.23it/s]\u001b[A\n",
            "Iteration:  78% 2010/2561 [25:19<07:16,  1.26it/s]\u001b[A\n",
            "Iteration:  79% 2011/2561 [25:20<07:07,  1.29it/s]\u001b[A\n",
            "Iteration:  79% 2012/2561 [25:20<07:01,  1.30it/s]\u001b[A\n",
            "Iteration:  79% 2013/2561 [25:21<06:55,  1.32it/s]\u001b[A\n",
            "Iteration:  79% 2014/2561 [25:22<06:53,  1.32it/s]\u001b[A\n",
            "Iteration:  79% 2015/2561 [25:23<06:50,  1.33it/s]\u001b[A\n",
            "Iteration:  79% 2016/2561 [25:23<06:50,  1.33it/s]\u001b[A\n",
            "Iteration:  79% 2017/2561 [25:24<06:48,  1.33it/s]\u001b[A\n",
            "Iteration:  79% 2018/2561 [25:25<06:48,  1.33it/s]\u001b[A\n",
            "Iteration:  79% 2019/2561 [25:26<06:47,  1.33it/s]\u001b[A\n",
            "Iteration:  79% 2020/2561 [25:26<06:44,  1.34it/s]\u001b[A\n",
            "Iteration:  79% 2021/2561 [25:27<06:43,  1.34it/s]\u001b[A\n",
            "Iteration:  79% 2022/2561 [25:28<06:42,  1.34it/s]\u001b[A\n",
            "Iteration:  79% 2023/2561 [25:28<06:42,  1.34it/s]\u001b[A\n",
            "Iteration:  79% 2024/2561 [25:29<06:40,  1.34it/s]\u001b[A\n",
            "Iteration:  79% 2025/2561 [25:30<06:41,  1.33it/s]\u001b[A\n",
            "Iteration:  79% 2026/2561 [25:31<06:40,  1.33it/s]\u001b[A\n",
            "Iteration:  79% 2027/2561 [25:31<06:41,  1.33it/s]\u001b[A\n",
            "Iteration:  79% 2028/2561 [25:32<06:38,  1.34it/s]\u001b[A\n",
            "Iteration:  79% 2029/2561 [25:33<06:35,  1.34it/s]\u001b[A\n",
            "Iteration:  79% 2030/2561 [25:34<06:36,  1.34it/s]\u001b[A\n",
            "Iteration:  79% 2031/2561 [25:34<06:34,  1.34it/s]\u001b[A\n",
            "Iteration:  79% 2032/2561 [25:35<06:35,  1.34it/s]\u001b[A\n",
            "Iteration:  79% 2033/2561 [25:36<06:33,  1.34it/s]\u001b[A\n",
            "Iteration:  79% 2034/2561 [25:37<06:34,  1.33it/s]\u001b[A\n",
            "Iteration:  79% 2035/2561 [25:37<06:34,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2036/2561 [25:38<06:35,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2037/2561 [25:39<06:34,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2038/2561 [25:40<06:33,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2039/2561 [25:40<06:31,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2040/2561 [25:41<06:29,  1.34it/s]\u001b[A\n",
            "Iteration:  80% 2041/2561 [25:42<06:29,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2042/2561 [25:43<06:27,  1.34it/s]\u001b[A\n",
            "Iteration:  80% 2043/2561 [25:43<06:27,  1.34it/s]\u001b[A\n",
            "Iteration:  80% 2044/2561 [25:44<06:26,  1.34it/s]\u001b[A\n",
            "Iteration:  80% 2045/2561 [25:45<06:27,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2046/2561 [25:46<06:25,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2047/2561 [25:46<06:27,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2048/2561 [25:47<06:25,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2049/2561 [25:48<06:25,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2050/2561 [25:49<06:24,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2051/2561 [25:49<06:21,  1.34it/s]\u001b[A\n",
            "Iteration:  80% 2052/2561 [25:50<06:20,  1.34it/s]\u001b[A\n",
            "Iteration:  80% 2053/2561 [25:51<06:18,  1.34it/s]\u001b[A\n",
            "Iteration:  80% 2054/2561 [25:52<06:19,  1.34it/s]\u001b[A\n",
            "Iteration:  80% 2055/2561 [25:52<06:18,  1.34it/s]\u001b[A\n",
            "Iteration:  80% 2056/2561 [25:53<06:18,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2057/2561 [25:54<06:17,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2058/2561 [25:55<06:18,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2059/2561 [25:55<06:17,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2060/2561 [25:56<06:17,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2061/2561 [25:57<06:16,  1.33it/s]\u001b[A\n",
            "Iteration:  81% 2062/2561 [25:58<06:12,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2063/2561 [25:58<06:12,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2064/2561 [25:59<06:10,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2065/2561 [26:00<06:10,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2066/2561 [26:01<06:09,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2067/2561 [26:01<06:10,  1.33it/s]\u001b[A\n",
            "Iteration:  81% 2068/2561 [26:02<06:09,  1.33it/s]\u001b[A\n",
            "Iteration:  81% 2069/2561 [26:03<06:08,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2070/2561 [26:04<06:07,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2071/2561 [26:04<06:04,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2072/2561 [26:05<06:05,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2073/2561 [26:06<06:03,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2074/2561 [26:07<06:04,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2075/2561 [26:07<06:03,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2076/2561 [26:08<06:03,  1.33it/s]\u001b[A\n",
            "Iteration:  81% 2077/2561 [26:09<06:02,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2078/2561 [26:10<06:02,  1.33it/s]\u001b[A\n",
            "Iteration:  81% 2079/2561 [26:10<06:02,  1.33it/s]\u001b[A\n",
            "Iteration:  81% 2080/2561 [26:11<06:02,  1.33it/s]\u001b[A\n",
            "Iteration:  81% 2081/2561 [26:12<06:00,  1.33it/s]\u001b[A\n",
            "Iteration:  81% 2082/2561 [26:13<05:57,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2083/2561 [26:13<05:57,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2084/2561 [26:14<05:55,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2085/2561 [26:15<05:55,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2086/2561 [26:16<05:54,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2087/2561 [26:16<05:55,  1.33it/s]\u001b[A\n",
            "Iteration:  82% 2088/2561 [26:17<05:54,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2089/2561 [26:18<05:52,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2090/2561 [26:19<05:51,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2091/2561 [26:19<05:49,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2092/2561 [26:20<05:49,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2093/2561 [26:21<05:48,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2094/2561 [26:22<05:49,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2095/2561 [26:22<05:47,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2096/2561 [26:23<05:47,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2097/2561 [26:24<05:46,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2098/2561 [26:25<05:43,  1.35it/s]\u001b[A\n",
            "Iteration:  82% 2099/2561 [26:25<05:44,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2100/2561 [26:26<05:42,  1.35it/s]\u001b[A\n",
            "Iteration:  82% 2101/2561 [26:27<05:43,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2102/2561 [26:28<05:42,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2103/2561 [26:28<05:43,  1.33it/s]\u001b[A\n",
            "Iteration:  82% 2104/2561 [26:29<05:42,  1.33it/s]\u001b[A\n",
            "Iteration:  82% 2105/2561 [26:30<05:42,  1.33it/s]\u001b[A\n",
            "Iteration:  82% 2106/2561 [26:31<05:41,  1.33it/s]\u001b[A\n",
            "Iteration:  82% 2107/2561 [26:31<05:38,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2108/2561 [26:32<05:38,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2109/2561 [26:33<05:36,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2110/2561 [26:34<05:36,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2111/2561 [26:34<05:35,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2112/2561 [26:35<05:35,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 2113/2561 [26:36<05:35,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 2114/2561 [26:37<05:35,  1.33it/s]\u001b[A\n",
            "Iteration:  83% 2115/2561 [26:37<05:34,  1.33it/s]\u001b[A\n",
            "Iteration:  83% 2116/2561 [26:38<05:34,  1.33it/s]\u001b[A\n",
            "Iteration:  83% 2117/2561 [26:39<05:33,  1.33it/s]\u001b[A\n",
            "Iteration:  83% 2118/2561 [26:40<05:30,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 2119/2561 [26:40<05:28,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 2120/2561 [26:41<05:28,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 2121/2561 [26:42<05:29,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 2122/2561 [26:43<05:27,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 2123/2561 [26:43<05:28,  1.33it/s]\u001b[A\n",
            "Iteration:  83% 2124/2561 [26:44<05:27,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 2125/2561 [26:45<05:27,  1.33it/s]\u001b[A\n",
            "Iteration:  83% 2126/2561 [26:46<05:26,  1.33it/s]\u001b[A\n",
            "Iteration:  83% 2127/2561 [26:46<05:26,  1.33it/s]\u001b[A\n",
            "Iteration:  83% 2128/2561 [26:47<05:25,  1.33it/s]\u001b[A\n",
            "Iteration:  83% 2129/2561 [26:48<05:23,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 2130/2561 [26:49<05:22,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 2131/2561 [26:49<05:20,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 2132/2561 [26:50<05:21,  1.33it/s]\u001b[A\n",
            "Iteration:  83% 2133/2561 [26:51<05:19,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 2134/2561 [26:52<05:20,  1.33it/s]\u001b[A\n",
            "Iteration:  83% 2135/2561 [26:52<05:18,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 2136/2561 [26:53<05:19,  1.33it/s]\u001b[A\n",
            "Iteration:  83% 2137/2561 [26:54<05:17,  1.33it/s]\u001b[A\n",
            "Iteration:  83% 2138/2561 [26:55<05:18,  1.33it/s]\u001b[A\n",
            "Iteration:  84% 2139/2561 [26:55<05:16,  1.33it/s]\u001b[A\n",
            "Iteration:  84% 2140/2561 [26:56<05:14,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2141/2561 [26:57<05:14,  1.33it/s]\u001b[A\n",
            "Iteration:  84% 2142/2561 [26:58<05:12,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2143/2561 [26:58<05:12,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2144/2561 [26:59<05:11,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2145/2561 [27:00<05:12,  1.33it/s]\u001b[A\n",
            "Iteration:  84% 2146/2561 [27:01<05:10,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2147/2561 [27:01<05:10,  1.33it/s]\u001b[A\n",
            "Iteration:  84% 2148/2561 [27:02<05:09,  1.33it/s]\u001b[A\n",
            "Iteration:  84% 2149/2561 [27:03<05:07,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2150/2561 [27:04<05:07,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2151/2561 [27:04<05:06,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2152/2561 [27:05<05:06,  1.33it/s]\u001b[A\n",
            "Iteration:  84% 2153/2561 [27:06<05:05,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2154/2561 [27:07<05:05,  1.33it/s]\u001b[A\n",
            "Iteration:  84% 2155/2561 [27:07<05:04,  1.33it/s]\u001b[A\n",
            "Iteration:  84% 2156/2561 [27:08<05:04,  1.33it/s]\u001b[A\n",
            "Iteration:  84% 2157/2561 [27:09<05:03,  1.33it/s]\u001b[A\n",
            "Iteration:  84% 2158/2561 [27:10<05:00,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2159/2561 [27:10<05:00,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2160/2561 [27:11<04:58,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2161/2561 [27:12<04:58,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2162/2561 [27:13<04:57,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2163/2561 [27:13<04:57,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2164/2561 [27:14<04:57,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2165/2561 [27:15<04:55,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2166/2561 [27:16<04:55,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2167/2561 [27:16<04:53,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2168/2561 [27:17<04:53,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2169/2561 [27:18<04:52,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2170/2561 [27:18<04:51,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2171/2561 [27:19<04:50,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2172/2561 [27:20<04:51,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2173/2561 [27:21<04:49,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2174/2561 [27:21<04:49,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2175/2561 [27:22<04:48,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2176/2561 [27:23<04:47,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2177/2561 [27:24<04:46,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2178/2561 [27:24<04:44,  1.35it/s]\u001b[A\n",
            "Iteration:  85% 2179/2561 [27:25<04:45,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2180/2561 [27:26<04:44,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2181/2561 [27:27<04:44,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2182/2561 [27:27<04:43,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2183/2561 [27:28<04:42,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2184/2561 [27:29<04:42,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2185/2561 [27:30<04:40,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2186/2561 [27:30<04:40,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2187/2561 [27:31<04:39,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2188/2561 [27:32<04:39,  1.33it/s]\u001b[A\n",
            "Iteration:  85% 2189/2561 [27:33<04:38,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2190/2561 [27:33<04:38,  1.33it/s]\u001b[A\n",
            "Iteration:  86% 2191/2561 [27:34<04:37,  1.33it/s]\u001b[A\n",
            "Iteration:  86% 2192/2561 [27:35<04:35,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2193/2561 [27:36<04:35,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2194/2561 [27:36<04:33,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2195/2561 [27:37<04:33,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2196/2561 [27:38<04:31,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2197/2561 [27:39<04:31,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2198/2561 [27:39<04:31,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2199/2561 [27:40<04:31,  1.33it/s]\u001b[A\n",
            "Iteration:  86% 2200/2561 [27:41<04:30,  1.33it/s]\u001b[A\n",
            "Iteration:  86% 2201/2561 [27:42<04:28,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2202/2561 [27:42<04:28,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2203/2561 [27:43<04:26,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2204/2561 [27:44<04:26,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2205/2561 [27:45<04:25,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2206/2561 [27:45<04:25,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2207/2561 [27:46<04:24,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2208/2561 [27:47<04:24,  1.33it/s]\u001b[A\n",
            "Iteration:  86% 2209/2561 [27:48<04:23,  1.33it/s]\u001b[A\n",
            "Iteration:  86% 2210/2561 [27:48<04:21,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2211/2561 [27:49<04:21,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2212/2561 [27:50<04:19,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2213/2561 [27:51<04:19,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2214/2561 [27:51<04:18,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2215/2561 [27:52<04:19,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2216/2561 [27:53<04:18,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2217/2561 [27:54<04:18,  1.33it/s]\u001b[A\n",
            "Iteration:  87% 2218/2561 [27:54<04:17,  1.33it/s]\u001b[A\n",
            "Iteration:  87% 2219/2561 [27:55<04:15,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2220/2561 [27:56<04:14,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2221/2561 [27:57<04:13,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2222/2561 [27:57<04:12,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2223/2561 [27:58<04:11,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2224/2561 [27:59<04:11,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2225/2561 [28:00<04:11,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2226/2561 [28:00<04:11,  1.33it/s]\u001b[A\n",
            "Iteration:  87% 2227/2561 [28:01<04:10,  1.33it/s]\u001b[A\n",
            "Iteration:  87% 2228/2561 [28:02<04:08,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2229/2561 [28:03<04:08,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2230/2561 [28:03<04:06,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2231/2561 [28:04<04:06,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2232/2561 [28:05<04:05,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2233/2561 [28:06<04:06,  1.33it/s]\u001b[A\n",
            "Iteration:  87% 2234/2561 [28:06<04:05,  1.33it/s]\u001b[A\n",
            "Iteration:  87% 2235/2561 [28:07<04:03,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2236/2561 [28:08<04:03,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2237/2561 [28:09<04:01,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2238/2561 [28:09<04:01,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2239/2561 [28:10<03:59,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2240/2561 [28:11<03:59,  1.34it/s]\u001b[A\n",
            "Iteration:  88% 2241/2561 [28:12<03:59,  1.34it/s]\u001b[A\n",
            "Iteration:  88% 2242/2561 [28:12<03:59,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2243/2561 [28:13<03:58,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2244/2561 [28:14<03:56,  1.34it/s]\u001b[A\n",
            "Iteration:  88% 2245/2561 [28:15<03:56,  1.34it/s]\u001b[A\n",
            "Iteration:  88% 2246/2561 [28:15<03:54,  1.34it/s]\u001b[A\n",
            "Iteration:  88% 2247/2561 [28:16<03:55,  1.34it/s]\u001b[A\n",
            "Iteration:  88% 2248/2561 [28:17<03:53,  1.34it/s]\u001b[A\n",
            "Iteration:  88% 2249/2561 [28:18<03:54,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2250/2561 [28:18<03:53,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2251/2561 [28:19<03:53,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2252/2561 [28:20<03:51,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2253/2561 [28:21<03:51,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2254/2561 [28:21<03:50,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2255/2561 [28:22<03:48,  1.34it/s]\u001b[A\n",
            "Iteration:  88% 2256/2561 [28:23<03:48,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2257/2561 [28:24<03:47,  1.34it/s]\u001b[A\n",
            "Iteration:  88% 2258/2561 [28:24<03:47,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2259/2561 [28:25<03:45,  1.34it/s]\u001b[A\n",
            "Iteration:  88% 2260/2561 [28:26<03:45,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2261/2561 [28:27<03:44,  1.34it/s]\u001b[A\n",
            "Iteration:  88% 2262/2561 [28:27<03:44,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2263/2561 [28:28<03:43,  1.34it/s]\u001b[A\n",
            "Iteration:  88% 2264/2561 [28:29<03:43,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2265/2561 [28:30<03:42,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2266/2561 [28:30<03:42,  1.33it/s]\u001b[A\n",
            "Iteration:  89% 2267/2561 [28:31<03:41,  1.33it/s]\u001b[A\n",
            "Iteration:  89% 2268/2561 [28:32<03:39,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2269/2561 [28:33<03:38,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2270/2561 [28:33<03:37,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2271/2561 [28:34<03:37,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2272/2561 [28:35<03:36,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2273/2561 [28:36<03:36,  1.33it/s]\u001b[A\n",
            "Iteration:  89% 2274/2561 [28:36<03:34,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2275/2561 [28:37<03:34,  1.33it/s]\u001b[A\n",
            "Iteration:  89% 2276/2561 [28:38<03:33,  1.33it/s]\u001b[A\n",
            "Iteration:  89% 2277/2561 [28:39<03:34,  1.33it/s]\u001b[A\n",
            "Iteration:  89% 2278/2561 [28:39<03:33,  1.33it/s]\u001b[A\n",
            "Iteration:  89% 2279/2561 [28:40<03:31,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2280/2561 [28:41<03:30,  1.33it/s]\u001b[A\n",
            "Iteration:  89% 2281/2561 [28:42<03:28,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2282/2561 [28:42<03:28,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2283/2561 [28:43<03:27,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2284/2561 [28:44<03:27,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2285/2561 [28:45<03:26,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2286/2561 [28:45<03:26,  1.33it/s]\u001b[A\n",
            "Iteration:  89% 2287/2561 [28:46<03:25,  1.33it/s]\u001b[A\n",
            "Iteration:  89% 2288/2561 [28:47<03:25,  1.33it/s]\u001b[A\n",
            "Iteration:  89% 2289/2561 [28:48<03:24,  1.33it/s]\u001b[A\n",
            "Iteration:  89% 2290/2561 [28:48<03:22,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2291/2561 [28:49<03:21,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2292/2561 [28:50<03:20,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2293/2561 [28:51<03:20,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2294/2561 [28:51<03:19,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2295/2561 [28:52<03:19,  1.33it/s]\u001b[A\n",
            "Iteration:  90% 2296/2561 [28:53<03:18,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2297/2561 [28:54<03:18,  1.33it/s]\u001b[A\n",
            "Iteration:  90% 2298/2561 [28:54<03:17,  1.33it/s]\u001b[A\n",
            "Iteration:  90% 2299/2561 [28:55<03:15,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2300/2561 [28:56<03:15,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2301/2561 [28:56<03:13,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2302/2561 [28:57<03:13,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2303/2561 [28:58<03:12,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2304/2561 [28:59<03:12,  1.33it/s]\u001b[A\n",
            "Iteration:  90% 2305/2561 [28:59<03:11,  1.33it/s]\u001b[A\n",
            "Iteration:  90% 2306/2561 [29:00<03:11,  1.33it/s]\u001b[A\n",
            "Iteration:  90% 2307/2561 [29:01<03:10,  1.33it/s]\u001b[A\n",
            "Iteration:  90% 2308/2561 [29:02<03:10,  1.33it/s]\u001b[A\n",
            "Iteration:  90% 2309/2561 [29:03<03:09,  1.33it/s]\u001b[A\n",
            "Iteration:  90% 2310/2561 [29:03<03:09,  1.33it/s]\u001b[A\n",
            "Iteration:  90% 2311/2561 [29:04<03:08,  1.33it/s]\u001b[A\n",
            "Iteration:  90% 2312/2561 [29:05<03:06,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2313/2561 [29:06<03:05,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2314/2561 [29:06<03:04,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2315/2561 [29:07<03:04,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2316/2561 [29:08<03:03,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2317/2561 [29:08<03:03,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2318/2561 [29:09<03:01,  1.34it/s]\u001b[A\n",
            "Iteration:  91% 2319/2561 [29:10<03:01,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2320/2561 [29:11<03:00,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2321/2561 [29:12<03:00,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2322/2561 [29:12<02:59,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2323/2561 [29:13<02:57,  1.34it/s]\u001b[A\n",
            "Iteration:  91% 2324/2561 [29:14<02:56,  1.34it/s]\u001b[A\n",
            "Iteration:  91% 2325/2561 [29:14<02:56,  1.34it/s]\u001b[A\n",
            "Iteration:  91% 2326/2561 [29:15<02:56,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2327/2561 [29:16<02:54,  1.34it/s]\u001b[A\n",
            "Iteration:  91% 2328/2561 [29:17<02:54,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2329/2561 [29:17<02:53,  1.34it/s]\u001b[A\n",
            "Iteration:  91% 2330/2561 [29:18<02:53,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2331/2561 [29:19<02:52,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2332/2561 [29:20<02:52,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2333/2561 [29:20<02:51,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2334/2561 [29:21<02:49,  1.34it/s]\u001b[A\n",
            "Iteration:  91% 2335/2561 [29:22<02:49,  1.34it/s]\u001b[A\n",
            "Iteration:  91% 2336/2561 [29:23<02:48,  1.34it/s]\u001b[A\n",
            "Iteration:  91% 2337/2561 [29:23<02:48,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2338/2561 [29:24<02:46,  1.34it/s]\u001b[A\n",
            "Iteration:  91% 2339/2561 [29:25<02:46,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2340/2561 [29:26<02:45,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2341/2561 [29:26<02:45,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2342/2561 [29:27<02:44,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2343/2561 [29:28<02:44,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2344/2561 [29:29<02:43,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2345/2561 [29:30<02:43,  1.32it/s]\u001b[A\n",
            "Iteration:  92% 2346/2561 [29:30<02:41,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2347/2561 [29:31<02:40,  1.34it/s]\u001b[A\n",
            "Iteration:  92% 2348/2561 [29:32<02:39,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2349/2561 [29:32<02:38,  1.34it/s]\u001b[A\n",
            "Iteration:  92% 2350/2561 [29:33<02:38,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2351/2561 [29:34<02:37,  1.34it/s]\u001b[A\n",
            "Iteration:  92% 2352/2561 [29:35<02:36,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2353/2561 [29:35<02:35,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2354/2561 [29:36<02:35,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2355/2561 [29:37<02:34,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2356/2561 [29:38<02:34,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2357/2561 [29:38<02:33,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2358/2561 [29:39<02:32,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2359/2561 [29:40<02:31,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2360/2561 [29:41<02:30,  1.34it/s]\u001b[A\n",
            "Iteration:  92% 2361/2561 [29:41<02:29,  1.34it/s]\u001b[A\n",
            "Iteration:  92% 2362/2561 [29:42<02:28,  1.34it/s]\u001b[A\n",
            "Iteration:  92% 2363/2561 [29:43<02:28,  1.34it/s]\u001b[A\n",
            "Iteration:  92% 2364/2561 [29:44<02:27,  1.34it/s]\u001b[A\n",
            "Iteration:  92% 2365/2561 [29:44<02:27,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2366/2561 [29:45<02:26,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2367/2561 [29:46<02:25,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2368/2561 [29:47<02:24,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2369/2561 [29:47<02:24,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2370/2561 [29:48<02:23,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2371/2561 [29:49<02:21,  1.34it/s]\u001b[A\n",
            "Iteration:  93% 2372/2561 [29:50<02:21,  1.34it/s]\u001b[A\n",
            "Iteration:  93% 2373/2561 [29:50<02:20,  1.34it/s]\u001b[A\n",
            "Iteration:  93% 2374/2561 [29:51<02:19,  1.34it/s]\u001b[A\n",
            "Iteration:  93% 2375/2561 [29:52<02:19,  1.34it/s]\u001b[A\n",
            "Iteration:  93% 2376/2561 [29:53<02:18,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2377/2561 [29:53<02:17,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2378/2561 [29:54<02:17,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2379/2561 [29:55<02:16,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2380/2561 [29:56<02:16,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2381/2561 [29:56<02:15,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2382/2561 [29:57<02:13,  1.34it/s]\u001b[A\n",
            "Iteration:  93% 2383/2561 [29:58<02:13,  1.34it/s]\u001b[A\n",
            "Iteration:  93% 2384/2561 [29:59<02:12,  1.34it/s]\u001b[A\n",
            "Iteration:  93% 2385/2561 [29:59<02:11,  1.34it/s]\u001b[A\n",
            "Iteration:  93% 2386/2561 [30:00<02:10,  1.34it/s]\u001b[A\n",
            "Iteration:  93% 2387/2561 [30:01<02:10,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2388/2561 [30:02<02:09,  1.34it/s]\u001b[A\n",
            "Iteration:  93% 2389/2561 [30:02<02:09,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2390/2561 [30:03<02:08,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2391/2561 [30:04<02:07,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2392/2561 [30:05<02:07,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2393/2561 [30:05<02:06,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2394/2561 [30:06<02:05,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2395/2561 [30:07<02:04,  1.34it/s]\u001b[A\n",
            "Iteration:  94% 2396/2561 [30:08<02:03,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2397/2561 [30:08<02:02,  1.34it/s]\u001b[A\n",
            "Iteration:  94% 2398/2561 [30:09<02:02,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2399/2561 [30:10<02:01,  1.34it/s]\u001b[A\n",
            "Iteration:  94% 2400/2561 [30:11<02:00,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2401/2561 [30:11<01:59,  1.34it/s]\u001b[A\n",
            "Iteration:  94% 2402/2561 [30:12<01:59,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2403/2561 [30:13<01:58,  1.34it/s]\u001b[A\n",
            "Iteration:  94% 2404/2561 [30:14<01:58,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2405/2561 [30:14<01:57,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2406/2561 [30:15<01:55,  1.34it/s]\u001b[A\n",
            "Iteration:  94% 2407/2561 [30:16<01:55,  1.34it/s]\u001b[A\n",
            "Iteration:  94% 2408/2561 [30:17<01:54,  1.34it/s]\u001b[A\n",
            "Iteration:  94% 2409/2561 [30:17<01:53,  1.34it/s]\u001b[A\n",
            "Iteration:  94% 2410/2561 [30:18<01:52,  1.34it/s]\u001b[A\n",
            "Iteration:  94% 2411/2561 [30:19<01:52,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2412/2561 [30:20<01:51,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2413/2561 [30:20<01:51,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2414/2561 [30:21<01:50,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2415/2561 [30:22<01:49,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2416/2561 [30:23<01:49,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2417/2561 [30:23<01:47,  1.34it/s]\u001b[A\n",
            "Iteration:  94% 2418/2561 [30:24<01:47,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2419/2561 [30:25<01:46,  1.34it/s]\u001b[A\n",
            "Iteration:  94% 2420/2561 [30:26<01:45,  1.33it/s]\u001b[A\n",
            "Iteration:  95% 2421/2561 [30:26<01:44,  1.34it/s]\u001b[A\n",
            "Iteration:  95% 2422/2561 [30:27<01:44,  1.33it/s]\u001b[A\n",
            "Iteration:  95% 2423/2561 [30:28<01:43,  1.33it/s]\u001b[A\n",
            "Iteration:  95% 2424/2561 [30:29<01:43,  1.33it/s]\u001b[A\n",
            "Iteration:  95% 2425/2561 [30:29<01:42,  1.33it/s]\u001b[A\n",
            "Iteration:  95% 2426/2561 [30:30<01:41,  1.33it/s]\u001b[A\n",
            "Iteration:  95% 2427/2561 [30:31<01:40,  1.33it/s]\u001b[A\n",
            "Iteration:  95% 2428/2561 [30:32<01:39,  1.34it/s]\u001b[A\n",
            "Iteration:  95% 2429/2561 [30:32<01:38,  1.33it/s]\u001b[A\n",
            "Iteration:  95% 2430/2561 [30:33<01:37,  1.34it/s]\u001b[A\n",
            "Iteration:  95% 2431/2561 [30:34<01:37,  1.33it/s]\u001b[A\n",
            "Iteration:  95% 2432/2561 [30:35<01:36,  1.34it/s]\u001b[A\n",
            "Iteration:  95% 2433/2561 [30:35<01:36,  1.33it/s]\u001b[A\n",
            "Iteration:  95% 2434/2561 [30:36<01:35,  1.34it/s]\u001b[A\n",
            "Iteration:  95% 2435/2561 [30:37<01:34,  1.33it/s]\u001b[A\n",
            "Iteration:  95% 2436/2561 [30:38<01:33,  1.33it/s]\u001b[A\n",
            "Iteration:  95% 2437/2561 [30:38<01:32,  1.34it/s]\u001b[A\n",
            "Iteration:  95% 2438/2561 [30:39<01:32,  1.34it/s]\u001b[A\n",
            "Iteration:  95% 2439/2561 [30:40<01:30,  1.34it/s]\u001b[A\n",
            "Iteration:  95% 2440/2561 [30:41<01:30,  1.34it/s]\u001b[A\n",
            "Iteration:  95% 2441/2561 [30:41<01:29,  1.34it/s]\u001b[A\n",
            "Iteration:  95% 2442/2561 [30:42<01:28,  1.34it/s]\u001b[A\n",
            "Iteration:  95% 2443/2561 [30:43<01:27,  1.34it/s]\u001b[A\n",
            "Iteration:  95% 2444/2561 [30:44<01:27,  1.33it/s]\u001b[A\n",
            "Iteration:  95% 2445/2561 [30:44<01:26,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2446/2561 [30:45<01:26,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2447/2561 [30:46<01:25,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2448/2561 [30:47<01:24,  1.34it/s]\u001b[A\n",
            "Iteration:  96% 2449/2561 [30:47<01:23,  1.34it/s]\u001b[A\n",
            "Iteration:  96% 2450/2561 [30:48<01:22,  1.34it/s]\u001b[A\n",
            "Iteration:  96% 2451/2561 [30:49<01:22,  1.34it/s]\u001b[A\n",
            "Iteration:  96% 2452/2561 [30:50<01:21,  1.34it/s]\u001b[A\n",
            "Iteration:  96% 2453/2561 [30:50<01:21,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2454/2561 [30:51<01:20,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2455/2561 [30:52<01:19,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2456/2561 [30:53<01:18,  1.34it/s]\u001b[A\n",
            "Iteration:  96% 2457/2561 [30:53<01:18,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2458/2561 [30:54<01:17,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2459/2561 [30:55<01:16,  1.34it/s]\u001b[A\n",
            "Iteration:  96% 2460/2561 [30:56<01:15,  1.34it/s]\u001b[A\n",
            "Iteration:  96% 2461/2561 [30:56<01:14,  1.34it/s]\u001b[A\n",
            "Iteration:  96% 2462/2561 [30:57<01:14,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2463/2561 [30:58<01:13,  1.34it/s]\u001b[A\n",
            "Iteration:  96% 2464/2561 [30:59<01:12,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2465/2561 [30:59<01:11,  1.34it/s]\u001b[A\n",
            "Iteration:  96% 2466/2561 [31:00<01:11,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2467/2561 [31:01<01:10,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2468/2561 [31:02<01:10,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2469/2561 [31:02<01:09,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2470/2561 [31:03<01:08,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2471/2561 [31:04<01:07,  1.33it/s]\u001b[A\n",
            "Iteration:  97% 2472/2561 [31:05<01:07,  1.33it/s]\u001b[A\n",
            "Iteration:  97% 2473/2561 [31:05<01:06,  1.33it/s]\u001b[A\n",
            "Iteration:  97% 2474/2561 [31:06<01:05,  1.32it/s]\u001b[A\n",
            "Iteration:  97% 2475/2561 [31:07<01:04,  1.33it/s]\u001b[A\n",
            "Iteration:  97% 2476/2561 [31:08<01:03,  1.33it/s]\u001b[A\n",
            "Iteration:  97% 2477/2561 [31:08<01:03,  1.33it/s]\u001b[A\n",
            "Iteration:  97% 2478/2561 [31:09<01:02,  1.34it/s]\u001b[A\n",
            "Iteration:  97% 2479/2561 [31:10<01:01,  1.33it/s]\u001b[A\n",
            "Iteration:  97% 2480/2561 [31:11<01:00,  1.34it/s]\u001b[A\n",
            "Iteration:  97% 2481/2561 [31:11<00:59,  1.33it/s]\u001b[A\n",
            "Iteration:  97% 2482/2561 [31:12<00:59,  1.34it/s]\u001b[A\n",
            "Iteration:  97% 2483/2561 [31:13<00:58,  1.33it/s]\u001b[A\n",
            "Iteration:  97% 2484/2561 [31:14<00:57,  1.34it/s]\u001b[A\n",
            "Iteration:  97% 2485/2561 [31:14<00:57,  1.33it/s]\u001b[A\n",
            "Iteration:  97% 2486/2561 [31:15<00:56,  1.33it/s]\u001b[A\n",
            "Iteration:  97% 2487/2561 [31:16<00:55,  1.34it/s]\u001b[A\n",
            "Iteration:  97% 2488/2561 [31:17<00:54,  1.34it/s]\u001b[A\n",
            "Iteration:  97% 2489/2561 [31:17<00:53,  1.34it/s]\u001b[A\n",
            "Iteration:  97% 2490/2561 [31:18<00:53,  1.34it/s]\u001b[A\n",
            "Iteration:  97% 2491/2561 [31:19<00:52,  1.34it/s]\u001b[A\n",
            "Iteration:  97% 2492/2561 [31:20<00:51,  1.34it/s]\u001b[A\n",
            "Iteration:  97% 2493/2561 [31:20<00:50,  1.34it/s]\u001b[A\n",
            "Iteration:  97% 2494/2561 [31:21<00:50,  1.33it/s]\u001b[A\n",
            "Iteration:  97% 2495/2561 [31:22<00:49,  1.33it/s]\u001b[A\n",
            "Iteration:  97% 2496/2561 [31:23<00:48,  1.34it/s]\u001b[A\n",
            "Iteration:  98% 2497/2561 [31:23<00:47,  1.34it/s]\u001b[A\n",
            "Iteration:  98% 2498/2561 [31:24<00:46,  1.34it/s]\u001b[A\n",
            "Iteration:  98% 2499/2561 [31:25<00:46,  1.34it/s]\u001b[A02/29/2020 07:59:34 - INFO - transformers.configuration_utils -   Configuration saved in output_roberta_GB/checkpoint-2500/config.json\n",
            "02/29/2020 07:59:36 - INFO - transformers.modeling_utils -   Model weights saved in output_roberta_GB/checkpoint-2500/pytorch_model.bin\n",
            "02/29/2020 07:59:36 - INFO - __main__ -   Saving model checkpoint to output_roberta_GB/checkpoint-2500\n",
            "02/29/2020 07:59:40 - INFO - __main__ -   Saving optimizer and scheduler states to output_roberta_GB/checkpoint-2500\n",
            "\n",
            "Iteration:  98% 2500/2561 [31:31<02:20,  2.31s/it]\u001b[A\n",
            "Iteration:  98% 2501/2561 [31:32<01:50,  1.85s/it]\u001b[A\n",
            "Iteration:  98% 2502/2561 [31:32<01:29,  1.52s/it]\u001b[A\n",
            "Iteration:  98% 2503/2561 [31:33<01:14,  1.29s/it]\u001b[A\n",
            "Iteration:  98% 2504/2561 [31:34<01:04,  1.12s/it]\u001b[A\n",
            "Iteration:  98% 2505/2561 [31:35<00:56,  1.02s/it]\u001b[A\n",
            "Iteration:  98% 2506/2561 [31:35<00:51,  1.07it/s]\u001b[A\n",
            "Iteration:  98% 2507/2561 [31:36<00:47,  1.14it/s]\u001b[A\n",
            "Iteration:  98% 2508/2561 [31:37<00:44,  1.19it/s]\u001b[A\n",
            "Iteration:  98% 2509/2561 [31:38<00:42,  1.23it/s]\u001b[A\n",
            "Iteration:  98% 2510/2561 [31:38<00:40,  1.26it/s]\u001b[A\n",
            "Iteration:  98% 2511/2561 [31:39<00:39,  1.27it/s]\u001b[A\n",
            "Iteration:  98% 2512/2561 [31:40<00:37,  1.29it/s]\u001b[A\n",
            "Iteration:  98% 2513/2561 [31:41<00:36,  1.30it/s]\u001b[A\n",
            "Iteration:  98% 2514/2561 [31:41<00:35,  1.31it/s]\u001b[A\n",
            "Iteration:  98% 2515/2561 [31:42<00:34,  1.32it/s]\u001b[A\n",
            "Iteration:  98% 2516/2561 [31:43<00:33,  1.33it/s]\u001b[A\n",
            "Iteration:  98% 2517/2561 [31:44<00:33,  1.33it/s]\u001b[A\n",
            "Iteration:  98% 2518/2561 [31:44<00:32,  1.33it/s]\u001b[A\n",
            "Iteration:  98% 2519/2561 [31:45<00:31,  1.34it/s]\u001b[A\n",
            "Iteration:  98% 2520/2561 [31:46<00:30,  1.33it/s]\u001b[A\n",
            "Iteration:  98% 2521/2561 [31:47<00:30,  1.33it/s]\u001b[A\n",
            "Iteration:  98% 2522/2561 [31:47<00:29,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2523/2561 [31:48<00:28,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2524/2561 [31:49<00:27,  1.34it/s]\u001b[A\n",
            "Iteration:  99% 2525/2561 [31:50<00:26,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2526/2561 [31:50<00:26,  1.34it/s]\u001b[A\n",
            "Iteration:  99% 2527/2561 [31:51<00:25,  1.34it/s]\u001b[A\n",
            "Iteration:  99% 2528/2561 [31:52<00:24,  1.34it/s]\u001b[A\n",
            "Iteration:  99% 2529/2561 [31:53<00:23,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2530/2561 [31:53<00:23,  1.34it/s]\u001b[A\n",
            "Iteration:  99% 2531/2561 [31:54<00:22,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2532/2561 [31:55<00:21,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2533/2561 [31:56<00:21,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2534/2561 [31:56<00:20,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2535/2561 [31:57<00:19,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2536/2561 [31:58<00:18,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2537/2561 [31:59<00:17,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2538/2561 [31:59<00:17,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2539/2561 [32:00<00:16,  1.34it/s]\u001b[A\n",
            "Iteration:  99% 2540/2561 [32:01<00:15,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2541/2561 [32:02<00:14,  1.34it/s]\u001b[A\n",
            "Iteration:  99% 2542/2561 [32:02<00:14,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2543/2561 [32:03<00:13,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2544/2561 [32:04<00:12,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2545/2561 [32:05<00:12,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2546/2561 [32:05<00:11,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2547/2561 [32:06<00:10,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2548/2561 [32:07<00:09,  1.33it/s]\u001b[A\n",
            "Iteration: 100% 2549/2561 [32:08<00:09,  1.33it/s]\u001b[A\n",
            "Iteration: 100% 2550/2561 [32:08<00:08,  1.33it/s]\u001b[A\n",
            "Iteration: 100% 2551/2561 [32:09<00:07,  1.33it/s]\u001b[A\n",
            "Iteration: 100% 2552/2561 [32:10<00:06,  1.34it/s]\u001b[A\n",
            "Iteration: 100% 2553/2561 [32:11<00:05,  1.33it/s]\u001b[A\n",
            "Iteration: 100% 2554/2561 [32:11<00:05,  1.34it/s]\u001b[A\n",
            "Iteration: 100% 2555/2561 [32:12<00:04,  1.33it/s]\u001b[A\n",
            "Iteration: 100% 2556/2561 [32:13<00:03,  1.34it/s]\u001b[A\n",
            "Iteration: 100% 2557/2561 [32:14<00:03,  1.33it/s]\u001b[A\n",
            "Iteration: 100% 2558/2561 [32:14<00:02,  1.33it/s]\u001b[A\n",
            "Iteration: 100% 2559/2561 [32:15<00:01,  1.33it/s]\u001b[A\n",
            "Iteration: 100% 2560/2561 [32:16<00:00,  1.33it/s]\u001b[A\n",
            "Iteration: 100% 2561/2561 [32:16<00:00,  1.65it/s]\u001b[A\n",
            "Epoch: 100% 1/1 [32:16<00:00, 1936.69s/it]\n",
            "02/29/2020 08:00:25 - INFO - __main__ -    global_step = 2561, average loss = 2.1631468487921657\n",
            "02/29/2020 08:00:25 - INFO - __main__ -   Saving model checkpoint to output_roberta_GB\n",
            "02/29/2020 08:00:25 - INFO - transformers.configuration_utils -   Configuration saved in output_roberta_GB/config.json\n",
            "02/29/2020 08:00:26 - INFO - transformers.modeling_utils -   Model weights saved in output_roberta_GB/pytorch_model.bin\n",
            "02/29/2020 08:00:26 - INFO - transformers.configuration_utils -   loading configuration file output_roberta_GB/config.json\n",
            "02/29/2020 08:00:26 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "02/29/2020 08:00:26 - INFO - transformers.modeling_utils -   loading weights file output_roberta_GB/pytorch_model.bin\n",
            "02/29/2020 08:00:31 - INFO - transformers.tokenization_utils -   Model name 'output_roberta_GB' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'output_roberta_GB' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "02/29/2020 08:00:31 - INFO - transformers.tokenization_utils -   Didn't find file output_roberta_GB/added_tokens.json. We won't load it.\n",
            "02/29/2020 08:00:31 - INFO - transformers.tokenization_utils -   loading file output_roberta_GB/vocab.json\n",
            "02/29/2020 08:00:31 - INFO - transformers.tokenization_utils -   loading file output_roberta_GB/merges.txt\n",
            "02/29/2020 08:00:31 - INFO - transformers.tokenization_utils -   loading file None\n",
            "02/29/2020 08:00:31 - INFO - transformers.tokenization_utils -   loading file output_roberta_GB/special_tokens_map.json\n",
            "02/29/2020 08:00:31 - INFO - transformers.tokenization_utils -   loading file output_roberta_GB/tokenizer_config.json\n",
            "02/29/2020 08:00:31 - INFO - __main__ -   Evaluate the following checkpoints: ['output_roberta_GB']\n",
            "02/29/2020 08:00:31 - INFO - transformers.configuration_utils -   loading configuration file output_roberta_GB/config.json\n",
            "02/29/2020 08:00:31 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "02/29/2020 08:00:31 - INFO - transformers.modeling_utils -   loading weights file output_roberta_GB/pytorch_model.bin\n",
            "02/29/2020 08:00:36 - INFO - __main__ -   Creating features from dataset file at \n",
            "02/29/2020 08:00:42 - INFO - __main__ -   Saving features into cached file roberta_cached_lm_510_gb_blog_test\n",
            "02/29/2020 08:00:42 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "02/29/2020 08:00:42 - INFO - __main__ -     Num examples = 2571\n",
            "02/29/2020 08:00:42 - INFO - __main__ -     Batch size = 4\n",
            "Evaluating: 100% 643/643 [02:34<00:00,  4.49it/s]\n",
            "02/29/2020 08:03:17 - INFO - __main__ -   ***** Eval results  *****\n",
            "02/29/2020 08:03:17 - INFO - __main__ -     perplexity = tensor(7.1367)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nx_2Xi7sp7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r_xRgnqdQYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "roberta_us_model_embedding = RobertaModel.from_pretrained('roberta_us')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g20GsdLGdQeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "roberta_us_tokenizer = RobertaTokenizer.from_pretrained('roberta_us')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCNvvYVHdQi5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"Do you have your chips with fish or with salsa?\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA4lnI6gdQnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text1 = \"He went out in just his undershirt and pants.\" #pants are underwear in Britain; maybe closer to an undershirt\n",
        "text2 = \"His braces completed the outfit.\" #braces are suspenders (in Britain); maybe closer to an outfit\n",
        "text3 = \"Does your pencil have a rubber on it?\" #rubber is an eraser in Britain); maybe closer to a pencil\n",
        "text4 = \"Was the bog closer to the forest or the house?\" #bog is a toilen in Britain); maybe closer to a house\n",
        "text5 = \"Are you taking the trolley or the train to the grocery market\" #trolley is a food carriage; possibly closer to a market"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnT9drdidQcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp_oDVNjdQV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualise_diffs(text, model, tokenizer):\n",
        "    word_vecs = []\n",
        "    for i in range(0, len(text.split())):\n",
        "        word_vecs.append(word_vector(text, i, model, tokenizer))\n",
        "    L = []\n",
        "    for p in word_vecs:\n",
        "        l = []\n",
        "        for q in word_vecs:\n",
        "            l.append(1 - cosine(p, q))\n",
        "        L.append(l)\n",
        "    M = np.array(L)\n",
        "    fig = plt.figure()\n",
        "    div = pd.DataFrame(M, columns = list(text.split()), index = list(text.split()))\n",
        "    ax = sns.heatmap(div)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4VS0NQ3dQTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualise_diffs(text, roberta_us_model_embedding, roberta_us_tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyVf03usdfB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "roberta_gb_model_embedding = RobertaModel.from_pretrained('roberta_gb')\n",
        "roberta_gb_tokenizer = RobertaTokenizer.from_pretrained('roberta_gb')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oijojj0Qdg2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualise_diffs(text, roberta_gb_model_embedding, roberta_gb_tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tkPARtuebiU",
        "colab_type": "text"
      },
      "source": [
        "## <span style=\"color:red\">*Exercise 4*</span>\n",
        "\n",
        "<span style=\"color:red\">Construct cells immediately below this that tune BERT to at least two different textual samples. These could be from different corpora, distinct time periods, separate authors, alternative publishing outlets, etc. Then compare the meaning of words, phrases and sentences to each other across the separate models. What do they reveal about the social worlds inscribed by the distinctive samples?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NH5PqZ7eicS",
        "colab_type": "text"
      },
      "source": [
        "I am comparing the style and policy stances of Obama and Clinton when they were both senators. Again, I generated the training and test files on my local machine. The code below will not run on colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNmqDIFKee96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filter2 = ObamaClintonReleases['category'] == 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILi5AlQJevry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2 = ObamaClintonReleases[filter2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwfwNj8aevpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_text, test_text = train_test_split(df2['text'], test_size=0.2)\n",
        "\n",
        "train_text.to_frame().to_csv(r'train_text_clinton', header=None, index=None, sep=' ', mode='a')\n",
        "test_text.to_frame().to_csv(r'test_text_clinton', header=None, index=None, sep=' ', mode='a')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KudZZdffDXI",
        "colab_type": "text"
      },
      "source": [
        "#### Model for Obama\n",
        "\n",
        "Train a RoBERTa model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKBFwJcKgzn4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# need to upload gb_blog_train, test, us_blog_train/test\n",
        "# test_text_obama/clinton\n",
        "# train_text_obama/clinton\n",
        "torch.cuda.empty_cache() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62PLQpe4eaxm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "586e8bba-b34d-4963-83b4-c27f4c78a6d9"
      },
      "source": [
        "!python run_language_modelling.py --output_dir=output_roberta_obama --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=train_text_obama --do_eval --eval_data_file=test_text_obama --mlm"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/05/2020 17:38:42 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "03/05/2020 17:38:43 - INFO - filelock -   Lock 139632992463952 acquired on /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6.lock\n",
            "03/05/2020 17:38:43 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp3extxt6q\n",
            "Downloading: 100% 524/524 [00:00<00:00, 405kB/s]\n",
            "03/05/2020 17:38:44 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json in cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "03/05/2020 17:38:44 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "03/05/2020 17:38:44 - INFO - filelock -   Lock 139632992463952 released on /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6.lock\n",
            "03/05/2020 17:38:44 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "03/05/2020 17:38:44 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "03/05/2020 17:38:45 - INFO - filelock -   Lock 139632992431464 acquired on /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n",
            "03/05/2020 17:38:45 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp2is30aff\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 987kB/s]\n",
            "03/05/2020 17:38:46 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json in cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "03/05/2020 17:38:46 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "03/05/2020 17:38:46 - INFO - filelock -   Lock 139632992431464 released on /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n",
            "03/05/2020 17:38:47 - INFO - filelock -   Lock 139632992431464 acquired on /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "03/05/2020 17:38:47 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmplxx713zt\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 617kB/s]\n",
            "03/05/2020 17:38:49 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt in cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/05/2020 17:38:49 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/05/2020 17:38:49 - INFO - filelock -   Lock 139632992431464 released on /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "03/05/2020 17:38:49 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "03/05/2020 17:38:49 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/05/2020 17:38:50 - INFO - filelock -   Lock 139632954921032 acquired on /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e.lock\n",
            "03/05/2020 17:38:50 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp16uinati\n",
            "Downloading: 100% 501M/501M [00:37<00:00, 13.5MB/s]\n",
            "03/05/2020 17:39:28 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin in cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "03/05/2020 17:39:28 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "03/05/2020 17:39:28 - INFO - filelock -   Lock 139632954921032 released on /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e.lock\n",
            "03/05/2020 17:39:28 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "03/05/2020 17:39:33 - INFO - transformers.modeling_utils -   Weights of RobertaForMaskedLM not initialized from pretrained model: ['lm_head.decoder.bias']\n",
            "03/05/2020 17:39:36 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='test_text_obama', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=True, mlm_probability=0.15, model_name_or_path='roberta-base', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='output_roberta_obama', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='train_text_obama', warmup_steps=0, weight_decay=0.0)\n",
            "03/05/2020 17:39:36 - INFO - __main__ -   Creating features from dataset file at \n",
            "03/05/2020 17:39:38 - INFO - __main__ -   Saving features into cached file roberta_cached_lm_510_train_text_obama\n",
            "03/05/2020 17:39:38 - INFO - __main__ -   ***** Running training *****\n",
            "03/05/2020 17:39:38 - INFO - __main__ -     Num examples = 1012\n",
            "03/05/2020 17:39:38 - INFO - __main__ -     Num Epochs = 1\n",
            "03/05/2020 17:39:38 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
            "03/05/2020 17:39:38 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "03/05/2020 17:39:38 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "03/05/2020 17:39:38 - INFO - __main__ -     Total optimization steps = 253\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/253 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/253 [00:01<05:03,  1.20s/it]\u001b[A\n",
            "Iteration:   1% 2/253 [00:02<05:00,  1.20s/it]\u001b[A\n",
            "Iteration:   1% 3/253 [00:03<04:56,  1.19s/it]\u001b[A\n",
            "Iteration:   2% 4/253 [00:04<04:52,  1.18s/it]\u001b[A\n",
            "Iteration:   2% 5/253 [00:05<04:50,  1.17s/it]\u001b[A\n",
            "Iteration:   2% 6/253 [00:07<04:49,  1.17s/it]\u001b[A\n",
            "Iteration:   3% 7/253 [00:08<04:46,  1.17s/it]\u001b[A\n",
            "Iteration:   3% 8/253 [00:09<04:44,  1.16s/it]\u001b[A\n",
            "Iteration:   4% 9/253 [00:10<04:44,  1.17s/it]\u001b[A\n",
            "Iteration:   4% 10/253 [00:11<04:44,  1.17s/it]\u001b[A\n",
            "Iteration:   4% 11/253 [00:12<04:41,  1.16s/it]\u001b[A\n",
            "Iteration:   5% 12/253 [00:13<04:39,  1.16s/it]\u001b[A\n",
            "Iteration:   5% 13/253 [00:15<04:37,  1.16s/it]\u001b[A\n",
            "Iteration:   6% 14/253 [00:16<04:36,  1.16s/it]\u001b[A\n",
            "Iteration:   6% 15/253 [00:17<04:34,  1.15s/it]\u001b[A\n",
            "Iteration:   6% 16/253 [00:18<04:33,  1.15s/it]\u001b[A\n",
            "Iteration:   7% 17/253 [00:19<04:33,  1.16s/it]\u001b[A\n",
            "Iteration:   7% 18/253 [00:20<04:31,  1.16s/it]\u001b[A\n",
            "Iteration:   8% 19/253 [00:22<04:31,  1.16s/it]\u001b[A\n",
            "Iteration:   8% 20/253 [00:23<04:30,  1.16s/it]\u001b[A\n",
            "Iteration:   8% 21/253 [00:24<04:30,  1.16s/it]\u001b[A\n",
            "Iteration:   9% 22/253 [00:25<04:29,  1.17s/it]\u001b[A\n",
            "Iteration:   9% 23/253 [00:26<04:27,  1.16s/it]\u001b[A\n",
            "Iteration:   9% 24/253 [00:27<04:25,  1.16s/it]\u001b[A\n",
            "Iteration:  10% 25/253 [00:29<04:23,  1.16s/it]\u001b[A\n",
            "Iteration:  10% 26/253 [00:30<04:21,  1.15s/it]\u001b[A\n",
            "Iteration:  11% 27/253 [00:31<04:21,  1.16s/it]\u001b[A\n",
            "Iteration:  11% 28/253 [00:32<04:20,  1.16s/it]\u001b[A\n",
            "Iteration:  11% 29/253 [00:33<04:19,  1.16s/it]\u001b[A\n",
            "Iteration:  12% 30/253 [00:34<04:18,  1.16s/it]\u001b[A\n",
            "Iteration:  12% 31/253 [00:35<04:16,  1.16s/it]\u001b[A\n",
            "Iteration:  13% 32/253 [00:37<04:14,  1.15s/it]\u001b[A\n",
            "Iteration:  13% 33/253 [00:38<04:13,  1.15s/it]\u001b[A\n",
            "Iteration:  13% 34/253 [00:39<04:13,  1.16s/it]\u001b[A\n",
            "Iteration:  14% 35/253 [00:40<04:12,  1.16s/it]\u001b[A\n",
            "Iteration:  14% 36/253 [00:41<04:10,  1.16s/it]\u001b[A\n",
            "Iteration:  15% 37/253 [00:42<04:10,  1.16s/it]\u001b[A\n",
            "Iteration:  15% 38/253 [00:44<04:08,  1.16s/it]\u001b[A\n",
            "Iteration:  15% 39/253 [00:45<04:07,  1.15s/it]\u001b[A\n",
            "Iteration:  16% 40/253 [00:46<04:05,  1.15s/it]\u001b[A\n",
            "Iteration:  16% 41/253 [00:47<04:04,  1.15s/it]\u001b[A\n",
            "Iteration:  17% 42/253 [00:48<04:02,  1.15s/it]\u001b[A\n",
            "Iteration:  17% 43/253 [00:49<04:01,  1.15s/it]\u001b[A\n",
            "Iteration:  17% 44/253 [00:50<04:00,  1.15s/it]\u001b[A\n",
            "Iteration:  18% 45/253 [00:52<03:58,  1.15s/it]\u001b[A\n",
            "Iteration:  18% 46/253 [00:53<03:57,  1.15s/it]\u001b[A\n",
            "Iteration:  19% 47/253 [00:54<03:56,  1.15s/it]\u001b[A\n",
            "Iteration:  19% 48/253 [00:55<03:56,  1.15s/it]\u001b[A\n",
            "Iteration:  19% 49/253 [00:56<03:55,  1.15s/it]\u001b[A\n",
            "Iteration:  20% 50/253 [00:57<03:54,  1.15s/it]\u001b[A\n",
            "Iteration:  20% 51/253 [00:59<03:52,  1.15s/it]\u001b[A\n",
            "Iteration:  21% 52/253 [01:00<03:51,  1.15s/it]\u001b[A\n",
            "Iteration:  21% 53/253 [01:01<03:50,  1.15s/it]\u001b[A\n",
            "Iteration:  21% 54/253 [01:02<03:49,  1.15s/it]\u001b[A\n",
            "Iteration:  22% 55/253 [01:03<03:47,  1.15s/it]\u001b[A\n",
            "Iteration:  22% 56/253 [01:04<03:45,  1.15s/it]\u001b[A\n",
            "Iteration:  23% 57/253 [01:05<03:45,  1.15s/it]\u001b[A\n",
            "Iteration:  23% 58/253 [01:07<03:43,  1.15s/it]\u001b[A\n",
            "Iteration:  23% 59/253 [01:08<03:41,  1.14s/it]\u001b[A\n",
            "Iteration:  24% 60/253 [01:09<03:40,  1.14s/it]\u001b[A\n",
            "Iteration:  24% 61/253 [01:10<03:39,  1.14s/it]\u001b[A\n",
            "Iteration:  25% 62/253 [01:11<03:39,  1.15s/it]\u001b[A\n",
            "Iteration:  25% 63/253 [01:12<03:38,  1.15s/it]\u001b[A\n",
            "Iteration:  25% 64/253 [01:13<03:37,  1.15s/it]\u001b[A\n",
            "Iteration:  26% 65/253 [01:15<03:36,  1.15s/it]\u001b[A\n",
            "Iteration:  26% 66/253 [01:16<03:35,  1.15s/it]\u001b[A\n",
            "Iteration:  26% 67/253 [01:17<03:34,  1.16s/it]\u001b[A\n",
            "Iteration:  27% 68/253 [01:18<03:33,  1.16s/it]\u001b[A\n",
            "Iteration:  27% 69/253 [01:19<03:32,  1.16s/it]\u001b[A\n",
            "Iteration:  28% 70/253 [01:20<03:31,  1.16s/it]\u001b[A\n",
            "Iteration:  28% 71/253 [01:22<03:29,  1.15s/it]\u001b[A\n",
            "Iteration:  28% 72/253 [01:23<03:28,  1.15s/it]\u001b[A\n",
            "Iteration:  29% 73/253 [01:24<03:27,  1.15s/it]\u001b[A\n",
            "Iteration:  29% 74/253 [01:25<03:26,  1.15s/it]\u001b[A\n",
            "Iteration:  30% 75/253 [01:26<03:25,  1.15s/it]\u001b[A\n",
            "Iteration:  30% 76/253 [01:27<03:23,  1.15s/it]\u001b[A\n",
            "Iteration:  30% 77/253 [01:28<03:22,  1.15s/it]\u001b[A\n",
            "Iteration:  31% 78/253 [01:30<03:21,  1.15s/it]\u001b[A\n",
            "Iteration:  31% 79/253 [01:31<03:19,  1.15s/it]\u001b[A\n",
            "Iteration:  32% 80/253 [01:32<03:19,  1.15s/it]\u001b[A\n",
            "Iteration:  32% 81/253 [01:33<03:18,  1.15s/it]\u001b[A\n",
            "Iteration:  32% 82/253 [01:34<03:17,  1.16s/it]\u001b[A\n",
            "Iteration:  33% 83/253 [01:35<03:16,  1.16s/it]\u001b[A\n",
            "Iteration:  33% 84/253 [01:37<03:15,  1.15s/it]\u001b[A\n",
            "Iteration:  34% 85/253 [01:38<03:14,  1.16s/it]\u001b[A\n",
            "Iteration:  34% 86/253 [01:39<03:12,  1.15s/it]\u001b[A\n",
            "Iteration:  34% 87/253 [01:40<03:11,  1.15s/it]\u001b[A\n",
            "Iteration:  35% 88/253 [01:41<03:10,  1.15s/it]\u001b[A\n",
            "Iteration:  35% 89/253 [01:42<03:08,  1.15s/it]\u001b[A\n",
            "Iteration:  36% 90/253 [01:43<03:07,  1.15s/it]\u001b[A\n",
            "Iteration:  36% 91/253 [01:45<03:06,  1.15s/it]\u001b[A\n",
            "Iteration:  36% 92/253 [01:46<03:05,  1.15s/it]\u001b[A\n",
            "Iteration:  37% 93/253 [01:47<03:03,  1.15s/it]\u001b[A\n",
            "Iteration:  37% 94/253 [01:48<03:03,  1.15s/it]\u001b[A\n",
            "Iteration:  38% 95/253 [01:49<03:02,  1.15s/it]\u001b[A\n",
            "Iteration:  38% 96/253 [01:50<03:01,  1.15s/it]\u001b[A\n",
            "Iteration:  38% 97/253 [01:52<03:00,  1.15s/it]\u001b[A\n",
            "Iteration:  39% 98/253 [01:53<02:59,  1.16s/it]\u001b[A\n",
            "Iteration:  39% 99/253 [01:54<02:58,  1.16s/it]\u001b[A\n",
            "Iteration:  40% 100/253 [01:55<02:56,  1.15s/it]\u001b[A\n",
            "Iteration:  40% 101/253 [01:56<02:54,  1.15s/it]\u001b[A\n",
            "Iteration:  40% 102/253 [01:57<02:53,  1.15s/it]\u001b[A\n",
            "Iteration:  41% 103/253 [01:58<02:52,  1.15s/it]\u001b[A\n",
            "Iteration:  41% 104/253 [02:00<02:51,  1.15s/it]\u001b[A\n",
            "Iteration:  42% 105/253 [02:01<02:49,  1.15s/it]\u001b[A\n",
            "Iteration:  42% 106/253 [02:02<02:48,  1.15s/it]\u001b[A\n",
            "Iteration:  42% 107/253 [02:03<02:48,  1.15s/it]\u001b[A\n",
            "Iteration:  43% 108/253 [02:04<02:46,  1.15s/it]\u001b[A\n",
            "Iteration:  43% 109/253 [02:05<02:45,  1.15s/it]\u001b[A\n",
            "Iteration:  43% 110/253 [02:06<02:44,  1.15s/it]\u001b[A\n",
            "Iteration:  44% 111/253 [02:08<02:43,  1.15s/it]\u001b[A\n",
            "Iteration:  44% 112/253 [02:09<02:42,  1.15s/it]\u001b[A\n",
            "Iteration:  45% 113/253 [02:10<02:41,  1.15s/it]\u001b[A\n",
            "Iteration:  45% 114/253 [02:11<02:40,  1.15s/it]\u001b[A\n",
            "Iteration:  45% 115/253 [02:12<02:39,  1.16s/it]\u001b[A\n",
            "Iteration:  46% 116/253 [02:13<02:38,  1.16s/it]\u001b[A\n",
            "Iteration:  46% 117/253 [02:15<02:37,  1.15s/it]\u001b[A\n",
            "Iteration:  47% 118/253 [02:16<02:36,  1.16s/it]\u001b[A\n",
            "Iteration:  47% 119/253 [02:17<02:34,  1.15s/it]\u001b[A\n",
            "Iteration:  47% 120/253 [02:18<02:32,  1.15s/it]\u001b[A\n",
            "Iteration:  48% 121/253 [02:19<02:31,  1.15s/it]\u001b[A\n",
            "Iteration:  48% 122/253 [02:20<02:30,  1.15s/it]\u001b[A\n",
            "Iteration:  49% 123/253 [02:21<02:28,  1.15s/it]\u001b[A\n",
            "Iteration:  49% 124/253 [02:23<02:27,  1.15s/it]\u001b[A\n",
            "Iteration:  49% 125/253 [02:24<02:26,  1.15s/it]\u001b[A\n",
            "Iteration:  50% 126/253 [02:25<02:26,  1.15s/it]\u001b[A\n",
            "Iteration:  50% 127/253 [02:26<02:24,  1.15s/it]\u001b[A\n",
            "Iteration:  51% 128/253 [02:27<02:24,  1.16s/it]\u001b[A\n",
            "Iteration:  51% 129/253 [02:28<02:23,  1.16s/it]\u001b[A\n",
            "Iteration:  51% 130/253 [02:30<02:21,  1.15s/it]\u001b[A\n",
            "Iteration:  52% 131/253 [02:31<02:20,  1.15s/it]\u001b[A\n",
            "Iteration:  52% 132/253 [02:32<02:19,  1.15s/it]\u001b[A\n",
            "Iteration:  53% 133/253 [02:33<02:18,  1.15s/it]\u001b[A\n",
            "Iteration:  53% 134/253 [02:34<02:17,  1.15s/it]\u001b[A\n",
            "Iteration:  53% 135/253 [02:35<02:15,  1.15s/it]\u001b[A\n",
            "Iteration:  54% 136/253 [02:36<02:15,  1.15s/it]\u001b[A\n",
            "Iteration:  54% 137/253 [02:38<02:13,  1.15s/it]\u001b[A\n",
            "Iteration:  55% 138/253 [02:39<02:12,  1.15s/it]\u001b[A\n",
            "Iteration:  55% 139/253 [02:40<02:11,  1.15s/it]\u001b[A\n",
            "Iteration:  55% 140/253 [02:41<02:10,  1.16s/it]\u001b[A\n",
            "Iteration:  56% 141/253 [02:42<02:09,  1.16s/it]\u001b[A\n",
            "Iteration:  56% 142/253 [02:43<02:08,  1.16s/it]\u001b[A\n",
            "Iteration:  57% 143/253 [02:45<02:07,  1.15s/it]\u001b[A\n",
            "Iteration:  57% 144/253 [02:46<02:06,  1.16s/it]\u001b[A\n",
            "Iteration:  57% 145/253 [02:47<02:04,  1.15s/it]\u001b[A\n",
            "Iteration:  58% 146/253 [02:48<02:03,  1.15s/it]\u001b[A\n",
            "Iteration:  58% 147/253 [02:49<02:01,  1.15s/it]\u001b[A\n",
            "Iteration:  58% 148/253 [02:50<02:00,  1.15s/it]\u001b[A\n",
            "Iteration:  59% 149/253 [02:51<01:59,  1.15s/it]\u001b[A\n",
            "Iteration:  59% 150/253 [02:53<01:58,  1.15s/it]\u001b[A\n",
            "Iteration:  60% 151/253 [02:54<01:57,  1.15s/it]\u001b[A\n",
            "Iteration:  60% 152/253 [02:55<01:56,  1.15s/it]\u001b[A\n",
            "Iteration:  60% 153/253 [02:56<01:55,  1.15s/it]\u001b[A\n",
            "Iteration:  61% 154/253 [02:57<01:53,  1.15s/it]\u001b[A\n",
            "Iteration:  61% 155/253 [02:58<01:53,  1.15s/it]\u001b[A\n",
            "Iteration:  62% 156/253 [02:59<01:52,  1.16s/it]\u001b[A\n",
            "Iteration:  62% 157/253 [03:01<01:51,  1.16s/it]\u001b[A\n",
            "Iteration:  62% 158/253 [03:02<01:49,  1.15s/it]\u001b[A\n",
            "Iteration:  63% 159/253 [03:03<01:48,  1.15s/it]\u001b[A\n",
            "Iteration:  63% 160/253 [03:04<01:47,  1.15s/it]\u001b[A\n",
            "Iteration:  64% 161/253 [03:05<01:46,  1.15s/it]\u001b[A\n",
            "Iteration:  64% 162/253 [03:06<01:45,  1.15s/it]\u001b[A\n",
            "Iteration:  64% 163/253 [03:08<01:43,  1.16s/it]\u001b[A\n",
            "Iteration:  65% 164/253 [03:09<01:42,  1.15s/it]\u001b[A\n",
            "Iteration:  65% 165/253 [03:10<01:41,  1.15s/it]\u001b[A\n",
            "Iteration:  66% 166/253 [03:11<01:39,  1.15s/it]\u001b[A\n",
            "Iteration:  66% 167/253 [03:12<01:38,  1.15s/it]\u001b[A\n",
            "Iteration:  66% 168/253 [03:13<01:37,  1.15s/it]\u001b[A\n",
            "Iteration:  67% 169/253 [03:14<01:36,  1.15s/it]\u001b[A\n",
            "Iteration:  67% 170/253 [03:16<01:35,  1.15s/it]\u001b[A\n",
            "Iteration:  68% 171/253 [03:17<01:34,  1.15s/it]\u001b[A\n",
            "Iteration:  68% 172/253 [03:18<01:33,  1.16s/it]\u001b[A\n",
            "Iteration:  68% 173/253 [03:19<01:32,  1.15s/it]\u001b[A\n",
            "Iteration:  69% 174/253 [03:20<01:30,  1.15s/it]\u001b[A\n",
            "Iteration:  69% 175/253 [03:21<01:29,  1.15s/it]\u001b[A\n",
            "Iteration:  70% 176/253 [03:22<01:27,  1.14s/it]\u001b[A\n",
            "Iteration:  70% 177/253 [03:24<01:26,  1.14s/it]\u001b[A\n",
            "Iteration:  70% 178/253 [03:25<01:26,  1.15s/it]\u001b[A\n",
            "Iteration:  71% 179/253 [03:26<01:25,  1.15s/it]\u001b[A\n",
            "Iteration:  71% 180/253 [03:27<01:23,  1.15s/it]\u001b[A\n",
            "Iteration:  72% 181/253 [03:28<01:22,  1.15s/it]\u001b[A\n",
            "Iteration:  72% 182/253 [03:29<01:21,  1.15s/it]\u001b[A\n",
            "Iteration:  72% 183/253 [03:31<01:20,  1.15s/it]\u001b[A\n",
            "Iteration:  73% 184/253 [03:32<01:19,  1.15s/it]\u001b[A\n",
            "Iteration:  73% 185/253 [03:33<01:18,  1.15s/it]\u001b[A\n",
            "Iteration:  74% 186/253 [03:34<01:17,  1.15s/it]\u001b[A\n",
            "Iteration:  74% 187/253 [03:35<01:15,  1.15s/it]\u001b[A\n",
            "Iteration:  74% 188/253 [03:36<01:14,  1.15s/it]\u001b[A\n",
            "Iteration:  75% 189/253 [03:37<01:13,  1.15s/it]\u001b[A\n",
            "Iteration:  75% 190/253 [03:39<01:12,  1.15s/it]\u001b[A\n",
            "Iteration:  75% 191/253 [03:40<01:11,  1.15s/it]\u001b[A\n",
            "Iteration:  76% 192/253 [03:41<01:10,  1.15s/it]\u001b[A\n",
            "Iteration:  76% 193/253 [03:42<01:09,  1.15s/it]\u001b[A\n",
            "Iteration:  77% 194/253 [03:43<01:07,  1.15s/it]\u001b[A\n",
            "Iteration:  77% 195/253 [03:44<01:06,  1.15s/it]\u001b[A\n",
            "Iteration:  77% 196/253 [03:45<01:05,  1.15s/it]\u001b[A\n",
            "Iteration:  78% 197/253 [03:47<01:04,  1.15s/it]\u001b[A\n",
            "Iteration:  78% 198/253 [03:48<01:03,  1.15s/it]\u001b[A\n",
            "Iteration:  79% 199/253 [03:49<01:02,  1.15s/it]\u001b[A\n",
            "Iteration:  79% 200/253 [03:50<01:01,  1.16s/it]\u001b[A\n",
            "Iteration:  79% 201/253 [03:51<00:59,  1.15s/it]\u001b[A\n",
            "Iteration:  80% 202/253 [03:52<00:58,  1.15s/it]\u001b[A\n",
            "Iteration:  80% 203/253 [03:54<00:57,  1.15s/it]\u001b[A\n",
            "Iteration:  81% 204/253 [03:55<00:56,  1.15s/it]\u001b[A\n",
            "Iteration:  81% 205/253 [03:56<00:55,  1.15s/it]\u001b[A\n",
            "Iteration:  81% 206/253 [03:57<00:54,  1.15s/it]\u001b[A\n",
            "Iteration:  82% 207/253 [03:58<00:52,  1.15s/it]\u001b[A\n",
            "Iteration:  82% 208/253 [03:59<00:51,  1.15s/it]\u001b[A\n",
            "Iteration:  83% 209/253 [04:00<00:50,  1.15s/it]\u001b[A\n",
            "Iteration:  83% 210/253 [04:02<00:49,  1.15s/it]\u001b[A\n",
            "Iteration:  83% 211/253 [04:03<00:48,  1.15s/it]\u001b[A\n",
            "Iteration:  84% 212/253 [04:04<00:47,  1.15s/it]\u001b[A\n",
            "Iteration:  84% 213/253 [04:05<00:46,  1.15s/it]\u001b[A\n",
            "Iteration:  85% 214/253 [04:06<00:44,  1.15s/it]\u001b[A\n",
            "Iteration:  85% 215/253 [04:07<00:43,  1.16s/it]\u001b[A\n",
            "Iteration:  85% 216/253 [04:09<00:42,  1.16s/it]\u001b[A\n",
            "Iteration:  86% 217/253 [04:10<00:41,  1.15s/it]\u001b[A\n",
            "Iteration:  86% 218/253 [04:11<00:40,  1.15s/it]\u001b[A\n",
            "Iteration:  87% 219/253 [04:12<00:39,  1.15s/it]\u001b[A\n",
            "Iteration:  87% 220/253 [04:13<00:37,  1.15s/it]\u001b[A\n",
            "Iteration:  87% 221/253 [04:14<00:36,  1.15s/it]\u001b[A\n",
            "Iteration:  88% 222/253 [04:15<00:35,  1.15s/it]\u001b[A\n",
            "Iteration:  88% 223/253 [04:17<00:34,  1.15s/it]\u001b[A\n",
            "Iteration:  89% 224/253 [04:18<00:33,  1.15s/it]\u001b[A\n",
            "Iteration:  89% 225/253 [04:19<00:32,  1.15s/it]\u001b[A\n",
            "Iteration:  89% 226/253 [04:20<00:31,  1.16s/it]\u001b[A\n",
            "Iteration:  90% 227/253 [04:21<00:30,  1.16s/it]\u001b[A\n",
            "Iteration:  90% 228/253 [04:22<00:28,  1.15s/it]\u001b[A\n",
            "Iteration:  91% 229/253 [04:23<00:27,  1.15s/it]\u001b[A\n",
            "Iteration:  91% 230/253 [04:25<00:26,  1.15s/it]\u001b[A\n",
            "Iteration:  91% 231/253 [04:26<00:25,  1.15s/it]\u001b[A\n",
            "Iteration:  92% 232/253 [04:27<00:24,  1.15s/it]\u001b[A\n",
            "Iteration:  92% 233/253 [04:28<00:22,  1.15s/it]\u001b[A\n",
            "Iteration:  92% 234/253 [04:29<00:21,  1.15s/it]\u001b[A\n",
            "Iteration:  93% 235/253 [04:30<00:20,  1.15s/it]\u001b[A\n",
            "Iteration:  93% 236/253 [04:32<00:19,  1.15s/it]\u001b[A\n",
            "Iteration:  94% 237/253 [04:33<00:18,  1.15s/it]\u001b[A\n",
            "Iteration:  94% 238/253 [04:34<00:17,  1.16s/it]\u001b[A\n",
            "Iteration:  94% 239/253 [04:35<00:16,  1.15s/it]\u001b[A\n",
            "Iteration:  95% 240/253 [04:36<00:14,  1.15s/it]\u001b[A\n",
            "Iteration:  95% 241/253 [04:37<00:13,  1.15s/it]\u001b[A\n",
            "Iteration:  96% 242/253 [04:38<00:12,  1.15s/it]\u001b[A\n",
            "Iteration:  96% 243/253 [04:40<00:11,  1.15s/it]\u001b[A\n",
            "Iteration:  96% 244/253 [04:41<00:10,  1.15s/it]\u001b[A\n",
            "Iteration:  97% 245/253 [04:42<00:09,  1.16s/it]\u001b[A\n",
            "Iteration:  97% 246/253 [04:43<00:08,  1.15s/it]\u001b[A\n",
            "Iteration:  98% 247/253 [04:44<00:06,  1.15s/it]\u001b[A\n",
            "Iteration:  98% 248/253 [04:45<00:05,  1.15s/it]\u001b[A\n",
            "Iteration:  98% 249/253 [04:47<00:04,  1.15s/it]\u001b[A\n",
            "Iteration:  99% 250/253 [04:48<00:03,  1.15s/it]\u001b[A\n",
            "Iteration:  99% 251/253 [04:49<00:02,  1.15s/it]\u001b[A\n",
            "Iteration: 100% 252/253 [04:50<00:01,  1.15s/it]\u001b[A\n",
            "Iteration: 100% 253/253 [04:51<00:00,  1.14s/it]\u001b[A\n",
            "Epoch: 100% 1/1 [04:51<00:00, 291.58s/it]\n",
            "03/05/2020 17:44:30 - INFO - __main__ -    global_step = 253, average loss = 1.0246501524929001\n",
            "03/05/2020 17:44:30 - INFO - __main__ -   Saving model checkpoint to output_roberta_obama\n",
            "03/05/2020 17:44:30 - INFO - transformers.configuration_utils -   Configuration saved in output_roberta_obama/config.json\n",
            "03/05/2020 17:44:30 - INFO - transformers.modeling_utils -   Model weights saved in output_roberta_obama/pytorch_model.bin\n",
            "03/05/2020 17:44:30 - INFO - transformers.configuration_utils -   loading configuration file output_roberta_obama/config.json\n",
            "03/05/2020 17:44:30 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "03/05/2020 17:44:30 - INFO - transformers.modeling_utils -   loading weights file output_roberta_obama/pytorch_model.bin\n",
            "03/05/2020 17:44:36 - INFO - transformers.tokenization_utils -   Model name 'output_roberta_obama' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'output_roberta_obama' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/05/2020 17:44:36 - INFO - transformers.tokenization_utils -   Didn't find file output_roberta_obama/added_tokens.json. We won't load it.\n",
            "03/05/2020 17:44:36 - INFO - transformers.tokenization_utils -   loading file output_roberta_obama/vocab.json\n",
            "03/05/2020 17:44:36 - INFO - transformers.tokenization_utils -   loading file output_roberta_obama/merges.txt\n",
            "03/05/2020 17:44:36 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/05/2020 17:44:36 - INFO - transformers.tokenization_utils -   loading file output_roberta_obama/special_tokens_map.json\n",
            "03/05/2020 17:44:36 - INFO - transformers.tokenization_utils -   loading file output_roberta_obama/tokenizer_config.json\n",
            "03/05/2020 17:44:36 - INFO - __main__ -   Evaluate the following checkpoints: ['output_roberta_obama']\n",
            "03/05/2020 17:44:36 - INFO - transformers.configuration_utils -   loading configuration file output_roberta_obama/config.json\n",
            "03/05/2020 17:44:36 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "03/05/2020 17:44:36 - INFO - transformers.modeling_utils -   loading weights file output_roberta_obama/pytorch_model.bin\n",
            "03/05/2020 17:44:42 - INFO - __main__ -   Creating features from dataset file at \n",
            "03/05/2020 17:44:43 - INFO - __main__ -   Saving features into cached file roberta_cached_lm_510_test_text_obama\n",
            "03/05/2020 17:44:43 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "03/05/2020 17:44:43 - INFO - __main__ -     Num examples = 265\n",
            "03/05/2020 17:44:43 - INFO - __main__ -     Batch size = 4\n",
            "Evaluating: 100% 67/67 [00:24<00:00,  3.39it/s]\n",
            "03/05/2020 17:45:07 - INFO - __main__ -   ***** Eval results  *****\n",
            "03/05/2020 17:45:07 - INFO - __main__ -     perplexity = tensor(2.4184)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXiS_82yfGw4",
        "colab_type": "text"
      },
      "source": [
        "#### Model for Clinton"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WaHab4hea1m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "360c0104-4ad3-4b0f-d902-d120ea0748ac"
      },
      "source": [
        "!python run_language_modelling.py --output_dir=output_roberta_clinton --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=train_text_clinton --do_eval --eval_data_file=test_text_clinton --mlm"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/05/2020 17:46:38 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "03/05/2020 17:46:39 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "03/05/2020 17:46:39 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "03/05/2020 17:46:41 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "03/05/2020 17:46:41 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/05/2020 17:46:41 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "03/05/2020 17:46:47 - INFO - transformers.modeling_utils -   Weights of RobertaForMaskedLM not initialized from pretrained model: ['lm_head.decoder.bias']\n",
            "03/05/2020 17:46:49 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='test_text_clinton', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=True, mlm_probability=0.15, model_name_or_path='roberta-base', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='output_roberta_clinton', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='train_text_clinton', warmup_steps=0, weight_decay=0.0)\n",
            "03/05/2020 17:46:49 - INFO - __main__ -   Creating features from dataset file at \n",
            "03/05/2020 17:46:53 - INFO - __main__ -   Saving features into cached file roberta_cached_lm_510_train_text_clinton\n",
            "03/05/2020 17:46:53 - INFO - __main__ -   ***** Running training *****\n",
            "03/05/2020 17:46:53 - INFO - __main__ -     Num examples = 1844\n",
            "03/05/2020 17:46:53 - INFO - __main__ -     Num Epochs = 1\n",
            "03/05/2020 17:46:53 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
            "03/05/2020 17:46:53 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "03/05/2020 17:46:53 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "03/05/2020 17:46:53 - INFO - __main__ -     Total optimization steps = 461\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/461 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/461 [00:01<09:03,  1.18s/it]\u001b[A\n",
            "Iteration:   0% 2/461 [00:02<08:58,  1.17s/it]\u001b[A\n",
            "Iteration:   1% 3/461 [00:03<08:55,  1.17s/it]\u001b[A\n",
            "Iteration:   1% 4/461 [00:04<08:52,  1.17s/it]\u001b[A\n",
            "Iteration:   1% 5/461 [00:05<08:51,  1.17s/it]\u001b[A\n",
            "Iteration:   1% 6/461 [00:06<08:49,  1.16s/it]\u001b[A\n",
            "Iteration:   2% 7/461 [00:08<08:47,  1.16s/it]\u001b[A\n",
            "Iteration:   2% 8/461 [00:09<08:47,  1.16s/it]\u001b[A\n",
            "Iteration:   2% 9/461 [00:10<08:43,  1.16s/it]\u001b[A\n",
            "Iteration:   2% 10/461 [00:11<08:42,  1.16s/it]\u001b[A\n",
            "Iteration:   2% 11/461 [00:12<08:41,  1.16s/it]\u001b[A\n",
            "Iteration:   3% 12/461 [00:13<08:40,  1.16s/it]\u001b[A\n",
            "Iteration:   3% 13/461 [00:15<08:36,  1.15s/it]\u001b[A\n",
            "Iteration:   3% 14/461 [00:16<08:37,  1.16s/it]\u001b[A\n",
            "Iteration:   3% 15/461 [00:17<08:34,  1.15s/it]\u001b[A\n",
            "Iteration:   3% 16/461 [00:18<08:33,  1.15s/it]\u001b[A\n",
            "Iteration:   4% 17/461 [00:19<08:31,  1.15s/it]\u001b[A\n",
            "Iteration:   4% 18/461 [00:20<08:29,  1.15s/it]\u001b[A\n",
            "Iteration:   4% 19/461 [00:21<08:30,  1.15s/it]\u001b[A\n",
            "Iteration:   4% 20/461 [00:23<08:30,  1.16s/it]\u001b[A\n",
            "Iteration:   5% 21/461 [00:24<08:29,  1.16s/it]\u001b[A\n",
            "Iteration:   5% 22/461 [00:25<08:27,  1.16s/it]\u001b[A\n",
            "Iteration:   5% 23/461 [00:26<08:28,  1.16s/it]\u001b[A\n",
            "Iteration:   5% 24/461 [00:27<08:26,  1.16s/it]\u001b[A\n",
            "Iteration:   5% 25/461 [00:28<08:25,  1.16s/it]\u001b[A\n",
            "Iteration:   6% 26/461 [00:30<08:22,  1.16s/it]\u001b[A\n",
            "Iteration:   6% 27/461 [00:31<08:21,  1.16s/it]\u001b[A\n",
            "Iteration:   6% 28/461 [00:32<08:19,  1.15s/it]\u001b[A\n",
            "Iteration:   6% 29/461 [00:33<08:19,  1.16s/it]\u001b[A\n",
            "Iteration:   7% 30/461 [00:34<08:17,  1.15s/it]\u001b[A\n",
            "Iteration:   7% 31/461 [00:35<08:17,  1.16s/it]\u001b[A\n",
            "Iteration:   7% 32/461 [00:37<08:18,  1.16s/it]\u001b[A\n",
            "Iteration:   7% 33/461 [00:38<08:16,  1.16s/it]\u001b[A\n",
            "Iteration:   7% 34/461 [00:39<08:16,  1.16s/it]\u001b[A\n",
            "Iteration:   8% 35/461 [00:40<08:15,  1.16s/it]\u001b[A\n",
            "Iteration:   8% 36/461 [00:41<08:12,  1.16s/it]\u001b[A\n",
            "Iteration:   8% 37/461 [00:42<08:11,  1.16s/it]\u001b[A\n",
            "Iteration:   8% 38/461 [00:43<08:07,  1.15s/it]\u001b[A\n",
            "Iteration:   8% 39/461 [00:45<08:04,  1.15s/it]\u001b[A\n",
            "Iteration:   9% 40/461 [00:46<08:03,  1.15s/it]\u001b[A\n",
            "Iteration:   9% 41/461 [00:47<08:02,  1.15s/it]\u001b[A\n",
            "Iteration:   9% 42/461 [00:48<08:00,  1.15s/it]\u001b[A\n",
            "Iteration:   9% 43/461 [00:49<07:59,  1.15s/it]\u001b[A\n",
            "Iteration:  10% 44/461 [00:50<07:59,  1.15s/it]\u001b[A\n",
            "Iteration:  10% 45/461 [00:52<08:00,  1.15s/it]\u001b[A\n",
            "Iteration:  10% 46/461 [00:53<07:56,  1.15s/it]\u001b[A\n",
            "Iteration:  10% 47/461 [00:54<07:56,  1.15s/it]\u001b[A\n",
            "Iteration:  10% 48/461 [00:55<07:56,  1.15s/it]\u001b[A\n",
            "Iteration:  11% 49/461 [00:56<07:56,  1.16s/it]\u001b[A\n",
            "Iteration:  11% 50/461 [00:57<07:56,  1.16s/it]\u001b[A\n",
            "Iteration:  11% 51/461 [00:58<07:54,  1.16s/it]\u001b[A\n",
            "Iteration:  11% 52/461 [01:00<07:53,  1.16s/it]\u001b[A\n",
            "Iteration:  11% 53/461 [01:01<07:50,  1.15s/it]\u001b[A\n",
            "Iteration:  12% 54/461 [01:02<07:47,  1.15s/it]\u001b[A\n",
            "Iteration:  12% 55/461 [01:03<07:47,  1.15s/it]\u001b[A\n",
            "Iteration:  12% 56/461 [01:04<07:46,  1.15s/it]\u001b[A\n",
            "Iteration:  12% 57/461 [01:05<07:45,  1.15s/it]\u001b[A\n",
            "Iteration:  13% 58/461 [01:07<07:46,  1.16s/it]\u001b[A\n",
            "Iteration:  13% 59/461 [01:08<07:44,  1.16s/it]\u001b[A\n",
            "Iteration:  13% 60/461 [01:09<07:44,  1.16s/it]\u001b[A\n",
            "Iteration:  13% 61/461 [01:10<07:42,  1.16s/it]\u001b[A\n",
            "Iteration:  13% 62/461 [01:11<07:39,  1.15s/it]\u001b[A\n",
            "Iteration:  14% 63/461 [01:12<07:38,  1.15s/it]\u001b[A\n",
            "Iteration:  14% 64/461 [01:13<07:38,  1.16s/it]\u001b[A\n",
            "Iteration:  14% 65/461 [01:15<07:38,  1.16s/it]\u001b[A\n",
            "Iteration:  14% 66/461 [01:16<07:37,  1.16s/it]\u001b[A\n",
            "Iteration:  15% 67/461 [01:17<07:35,  1.16s/it]\u001b[A\n",
            "Iteration:  15% 68/461 [01:18<07:32,  1.15s/it]\u001b[A\n",
            "Iteration:  15% 69/461 [01:19<07:31,  1.15s/it]\u001b[A\n",
            "Iteration:  15% 70/461 [01:20<07:30,  1.15s/it]\u001b[A\n",
            "Iteration:  15% 71/461 [01:22<07:29,  1.15s/it]\u001b[A\n",
            "Iteration:  16% 72/461 [01:23<07:26,  1.15s/it]\u001b[A\n",
            "Iteration:  16% 73/461 [01:24<07:27,  1.15s/it]\u001b[A\n",
            "Iteration:  16% 74/461 [01:25<07:25,  1.15s/it]\u001b[A\n",
            "Iteration:  16% 75/461 [01:26<07:23,  1.15s/it]\u001b[A\n",
            "Iteration:  16% 76/461 [01:27<07:21,  1.15s/it]\u001b[A\n",
            "Iteration:  17% 77/461 [01:28<07:20,  1.15s/it]\u001b[A\n",
            "Iteration:  17% 78/461 [01:30<07:19,  1.15s/it]\u001b[A\n",
            "Iteration:  17% 79/461 [01:31<07:17,  1.15s/it]\u001b[A\n",
            "Iteration:  17% 80/461 [01:32<07:17,  1.15s/it]\u001b[A\n",
            "Iteration:  18% 81/461 [01:33<07:16,  1.15s/it]\u001b[A\n",
            "Iteration:  18% 82/461 [01:34<07:14,  1.15s/it]\u001b[A\n",
            "Iteration:  18% 83/461 [01:35<07:15,  1.15s/it]\u001b[A\n",
            "Iteration:  18% 84/461 [01:36<07:12,  1.15s/it]\u001b[A\n",
            "Iteration:  18% 85/461 [01:38<07:12,  1.15s/it]\u001b[A\n",
            "Iteration:  19% 86/461 [01:39<07:11,  1.15s/it]\u001b[A\n",
            "Iteration:  19% 87/461 [01:40<07:09,  1.15s/it]\u001b[A\n",
            "Iteration:  19% 88/461 [01:41<07:07,  1.15s/it]\u001b[A\n",
            "Iteration:  19% 89/461 [01:42<07:05,  1.14s/it]\u001b[A\n",
            "Iteration:  20% 90/461 [01:43<07:04,  1.15s/it]\u001b[A\n",
            "Iteration:  20% 91/461 [01:44<07:04,  1.15s/it]\u001b[A\n",
            "Iteration:  20% 92/461 [01:46<07:05,  1.15s/it]\u001b[A\n",
            "Iteration:  20% 93/461 [01:47<07:03,  1.15s/it]\u001b[A\n",
            "Iteration:  20% 94/461 [01:48<07:03,  1.15s/it]\u001b[A\n",
            "Iteration:  21% 95/461 [01:49<07:01,  1.15s/it]\u001b[A\n",
            "Iteration:  21% 96/461 [01:50<07:00,  1.15s/it]\u001b[A\n",
            "Iteration:  21% 97/461 [01:51<06:59,  1.15s/it]\u001b[A\n",
            "Iteration:  21% 98/461 [01:53<06:59,  1.15s/it]\u001b[A\n",
            "Iteration:  21% 99/461 [01:54<06:58,  1.15s/it]\u001b[A\n",
            "Iteration:  22% 100/461 [01:55<06:58,  1.16s/it]\u001b[A\n",
            "Iteration:  22% 101/461 [01:56<06:57,  1.16s/it]\u001b[A\n",
            "Iteration:  22% 102/461 [01:57<06:55,  1.16s/it]\u001b[A\n",
            "Iteration:  22% 103/461 [01:58<06:54,  1.16s/it]\u001b[A\n",
            "Iteration:  23% 104/461 [02:00<06:54,  1.16s/it]\u001b[A\n",
            "Iteration:  23% 105/461 [02:01<06:52,  1.16s/it]\u001b[A\n",
            "Iteration:  23% 106/461 [02:02<06:49,  1.15s/it]\u001b[A\n",
            "Iteration:  23% 107/461 [02:03<06:49,  1.16s/it]\u001b[A\n",
            "Iteration:  23% 108/461 [02:04<06:47,  1.15s/it]\u001b[A\n",
            "Iteration:  24% 109/461 [02:05<06:46,  1.16s/it]\u001b[A\n",
            "Iteration:  24% 110/461 [02:06<06:45,  1.15s/it]\u001b[A\n",
            "Iteration:  24% 111/461 [02:08<06:42,  1.15s/it]\u001b[A\n",
            "Iteration:  24% 112/461 [02:09<06:41,  1.15s/it]\u001b[A\n",
            "Iteration:  25% 113/461 [02:10<06:39,  1.15s/it]\u001b[A\n",
            "Iteration:  25% 114/461 [02:11<06:37,  1.15s/it]\u001b[A\n",
            "Iteration:  25% 115/461 [02:12<06:39,  1.15s/it]\u001b[A\n",
            "Iteration:  25% 116/461 [02:13<06:38,  1.15s/it]\u001b[A\n",
            "Iteration:  25% 117/461 [02:14<06:35,  1.15s/it]\u001b[A\n",
            "Iteration:  26% 118/461 [02:16<06:34,  1.15s/it]\u001b[A\n",
            "Iteration:  26% 119/461 [02:17<06:34,  1.15s/it]\u001b[A\n",
            "Iteration:  26% 120/461 [02:18<06:31,  1.15s/it]\u001b[A\n",
            "Iteration:  26% 121/461 [02:19<06:29,  1.15s/it]\u001b[A\n",
            "Iteration:  26% 122/461 [02:20<06:29,  1.15s/it]\u001b[A\n",
            "Iteration:  27% 123/461 [02:21<06:28,  1.15s/it]\u001b[A\n",
            "Iteration:  27% 124/461 [02:23<06:27,  1.15s/it]\u001b[A\n",
            "Iteration:  27% 125/461 [02:24<06:25,  1.15s/it]\u001b[A\n",
            "Iteration:  27% 126/461 [02:25<06:25,  1.15s/it]\u001b[A\n",
            "Iteration:  28% 127/461 [02:26<06:23,  1.15s/it]\u001b[A\n",
            "Iteration:  28% 128/461 [02:27<06:22,  1.15s/it]\u001b[A\n",
            "Iteration:  28% 129/461 [02:28<06:19,  1.14s/it]\u001b[A\n",
            "Iteration:  28% 130/461 [02:29<06:19,  1.15s/it]\u001b[A\n",
            "Iteration:  28% 131/461 [02:31<06:18,  1.15s/it]\u001b[A\n",
            "Iteration:  29% 132/461 [02:32<06:17,  1.15s/it]\u001b[A\n",
            "Iteration:  29% 133/461 [02:33<06:16,  1.15s/it]\u001b[A\n",
            "Iteration:  29% 134/461 [02:34<06:15,  1.15s/it]\u001b[A\n",
            "Iteration:  29% 135/461 [02:35<06:15,  1.15s/it]\u001b[A\n",
            "Iteration:  30% 136/461 [02:36<06:13,  1.15s/it]\u001b[A\n",
            "Iteration:  30% 137/461 [02:37<06:12,  1.15s/it]\u001b[A\n",
            "Iteration:  30% 138/461 [02:39<06:12,  1.15s/it]\u001b[A\n",
            "Iteration:  30% 139/461 [02:40<06:10,  1.15s/it]\u001b[A\n",
            "Iteration:  30% 140/461 [02:41<06:09,  1.15s/it]\u001b[A\n",
            "Iteration:  31% 141/461 [02:42<06:08,  1.15s/it]\u001b[A\n",
            "Iteration:  31% 142/461 [02:43<06:06,  1.15s/it]\u001b[A\n",
            "Iteration:  31% 143/461 [02:44<06:06,  1.15s/it]\u001b[A\n",
            "Iteration:  31% 144/461 [02:46<06:04,  1.15s/it]\u001b[A\n",
            "Iteration:  31% 145/461 [02:47<06:04,  1.15s/it]\u001b[A\n",
            "Iteration:  32% 146/461 [02:48<06:02,  1.15s/it]\u001b[A\n",
            "Iteration:  32% 147/461 [02:49<06:00,  1.15s/it]\u001b[A\n",
            "Iteration:  32% 148/461 [02:50<05:58,  1.14s/it]\u001b[A\n",
            "Iteration:  32% 149/461 [02:51<05:59,  1.15s/it]\u001b[A\n",
            "Iteration:  33% 150/461 [02:52<05:57,  1.15s/it]\u001b[A\n",
            "Iteration:  33% 151/461 [02:54<05:55,  1.15s/it]\u001b[A\n",
            "Iteration:  33% 152/461 [02:55<05:54,  1.15s/it]\u001b[A\n",
            "Iteration:  33% 153/461 [02:56<05:53,  1.15s/it]\u001b[A\n",
            "Iteration:  33% 154/461 [02:57<05:52,  1.15s/it]\u001b[A\n",
            "Iteration:  34% 155/461 [02:58<05:52,  1.15s/it]\u001b[A\n",
            "Iteration:  34% 156/461 [02:59<05:49,  1.15s/it]\u001b[A\n",
            "Iteration:  34% 157/461 [03:00<05:48,  1.15s/it]\u001b[A\n",
            "Iteration:  34% 158/461 [03:02<05:48,  1.15s/it]\u001b[A\n",
            "Iteration:  34% 159/461 [03:03<05:46,  1.15s/it]\u001b[A\n",
            "Iteration:  35% 160/461 [03:04<05:46,  1.15s/it]\u001b[A\n",
            "Iteration:  35% 161/461 [03:05<05:44,  1.15s/it]\u001b[A\n",
            "Iteration:  35% 162/461 [03:06<05:43,  1.15s/it]\u001b[A\n",
            "Iteration:  35% 163/461 [03:07<05:42,  1.15s/it]\u001b[A\n",
            "Iteration:  36% 164/461 [03:09<05:41,  1.15s/it]\u001b[A\n",
            "Iteration:  36% 165/461 [03:10<05:39,  1.15s/it]\u001b[A\n",
            "Iteration:  36% 166/461 [03:11<05:39,  1.15s/it]\u001b[A\n",
            "Iteration:  36% 167/461 [03:12<05:36,  1.15s/it]\u001b[A\n",
            "Iteration:  36% 168/461 [03:13<05:36,  1.15s/it]\u001b[A\n",
            "Iteration:  37% 169/461 [03:14<05:36,  1.15s/it]\u001b[A\n",
            "Iteration:  37% 170/461 [03:15<05:35,  1.15s/it]\u001b[A\n",
            "Iteration:  37% 171/461 [03:17<05:33,  1.15s/it]\u001b[A\n",
            "Iteration:  37% 172/461 [03:18<05:31,  1.15s/it]\u001b[A\n",
            "Iteration:  38% 173/461 [03:19<05:29,  1.14s/it]\u001b[A\n",
            "Iteration:  38% 174/461 [03:20<05:28,  1.15s/it]\u001b[A\n",
            "Iteration:  38% 175/461 [03:21<05:27,  1.15s/it]\u001b[A\n",
            "Iteration:  38% 176/461 [03:22<05:27,  1.15s/it]\u001b[A\n",
            "Iteration:  38% 177/461 [03:23<05:25,  1.15s/it]\u001b[A\n",
            "Iteration:  39% 178/461 [03:25<05:24,  1.15s/it]\u001b[A\n",
            "Iteration:  39% 179/461 [03:26<05:22,  1.14s/it]\u001b[A\n",
            "Iteration:  39% 180/461 [03:27<05:22,  1.15s/it]\u001b[A\n",
            "Iteration:  39% 181/461 [03:28<05:21,  1.15s/it]\u001b[A\n",
            "Iteration:  39% 182/461 [03:29<05:21,  1.15s/it]\u001b[A\n",
            "Iteration:  40% 183/461 [03:30<05:20,  1.15s/it]\u001b[A\n",
            "Iteration:  40% 184/461 [03:31<05:19,  1.15s/it]\u001b[A\n",
            "Iteration:  40% 185/461 [03:33<05:18,  1.16s/it]\u001b[A\n",
            "Iteration:  40% 186/461 [03:34<05:16,  1.15s/it]\u001b[A\n",
            "Iteration:  41% 187/461 [03:35<05:14,  1.15s/it]\u001b[A\n",
            "Iteration:  41% 188/461 [03:36<05:14,  1.15s/it]\u001b[A\n",
            "Iteration:  41% 189/461 [03:37<05:14,  1.16s/it]\u001b[A\n",
            "Iteration:  41% 190/461 [03:38<05:13,  1.16s/it]\u001b[A\n",
            "Iteration:  41% 191/461 [03:40<05:11,  1.15s/it]\u001b[A\n",
            "Iteration:  42% 192/461 [03:41<05:10,  1.15s/it]\u001b[A\n",
            "Iteration:  42% 193/461 [03:42<05:08,  1.15s/it]\u001b[A\n",
            "Iteration:  42% 194/461 [03:43<05:06,  1.15s/it]\u001b[A\n",
            "Iteration:  42% 195/461 [03:44<05:05,  1.15s/it]\u001b[A\n",
            "Iteration:  43% 196/461 [03:45<05:04,  1.15s/it]\u001b[A\n",
            "Iteration:  43% 197/461 [03:46<05:03,  1.15s/it]\u001b[A\n",
            "Iteration:  43% 198/461 [03:48<05:03,  1.15s/it]\u001b[A\n",
            "Iteration:  43% 199/461 [03:49<05:01,  1.15s/it]\u001b[A\n",
            "Iteration:  43% 200/461 [03:50<05:00,  1.15s/it]\u001b[A\n",
            "Iteration:  44% 201/461 [03:51<04:59,  1.15s/it]\u001b[A\n",
            "Iteration:  44% 202/461 [03:52<04:57,  1.15s/it]\u001b[A\n",
            "Iteration:  44% 203/461 [03:53<04:56,  1.15s/it]\u001b[A\n",
            "Iteration:  44% 204/461 [03:54<04:54,  1.15s/it]\u001b[A\n",
            "Iteration:  44% 205/461 [03:56<04:52,  1.14s/it]\u001b[A\n",
            "Iteration:  45% 206/461 [03:57<04:51,  1.14s/it]\u001b[A\n",
            "Iteration:  45% 207/461 [03:58<04:49,  1.14s/it]\u001b[A\n",
            "Iteration:  45% 208/461 [03:59<04:49,  1.14s/it]\u001b[A\n",
            "Iteration:  45% 209/461 [04:00<04:48,  1.15s/it]\u001b[A\n",
            "Iteration:  46% 210/461 [04:01<04:47,  1.14s/it]\u001b[A\n",
            "Iteration:  46% 211/461 [04:02<04:46,  1.15s/it]\u001b[A\n",
            "Iteration:  46% 212/461 [04:04<04:44,  1.14s/it]\u001b[A\n",
            "Iteration:  46% 213/461 [04:05<04:44,  1.15s/it]\u001b[A\n",
            "Iteration:  46% 214/461 [04:06<04:44,  1.15s/it]\u001b[A\n",
            "Iteration:  47% 215/461 [04:07<04:43,  1.15s/it]\u001b[A\n",
            "Iteration:  47% 216/461 [04:08<04:42,  1.15s/it]\u001b[A\n",
            "Iteration:  47% 217/461 [04:09<04:41,  1.15s/it]\u001b[A\n",
            "Iteration:  47% 218/461 [04:11<04:39,  1.15s/it]\u001b[A\n",
            "Iteration:  48% 219/461 [04:12<04:39,  1.15s/it]\u001b[A\n",
            "Iteration:  48% 220/461 [04:13<04:37,  1.15s/it]\u001b[A\n",
            "Iteration:  48% 221/461 [04:14<04:35,  1.15s/it]\u001b[A\n",
            "Iteration:  48% 222/461 [04:15<04:34,  1.15s/it]\u001b[A\n",
            "Iteration:  48% 223/461 [04:16<04:33,  1.15s/it]\u001b[A\n",
            "Iteration:  49% 224/461 [04:17<04:32,  1.15s/it]\u001b[A\n",
            "Iteration:  49% 225/461 [04:19<04:31,  1.15s/it]\u001b[A\n",
            "Iteration:  49% 226/461 [04:20<04:31,  1.16s/it]\u001b[A\n",
            "Iteration:  49% 227/461 [04:21<04:29,  1.15s/it]\u001b[A\n",
            "Iteration:  49% 228/461 [04:22<04:27,  1.15s/it]\u001b[A\n",
            "Iteration:  50% 229/461 [04:23<04:26,  1.15s/it]\u001b[A\n",
            "Iteration:  50% 230/461 [04:24<04:24,  1.15s/it]\u001b[A\n",
            "Iteration:  50% 231/461 [04:25<04:23,  1.15s/it]\u001b[A\n",
            "Iteration:  50% 232/461 [04:27<04:22,  1.15s/it]\u001b[A\n",
            "Iteration:  51% 233/461 [04:28<04:22,  1.15s/it]\u001b[A\n",
            "Iteration:  51% 234/461 [04:29<04:21,  1.15s/it]\u001b[A\n",
            "Iteration:  51% 235/461 [04:30<04:19,  1.15s/it]\u001b[A\n",
            "Iteration:  51% 236/461 [04:31<04:18,  1.15s/it]\u001b[A\n",
            "Iteration:  51% 237/461 [04:32<04:17,  1.15s/it]\u001b[A\n",
            "Iteration:  52% 238/461 [04:34<04:16,  1.15s/it]\u001b[A\n",
            "Iteration:  52% 239/461 [04:35<04:15,  1.15s/it]\u001b[A\n",
            "Iteration:  52% 240/461 [04:36<04:13,  1.15s/it]\u001b[A\n",
            "Iteration:  52% 241/461 [04:37<04:11,  1.15s/it]\u001b[A\n",
            "Iteration:  52% 242/461 [04:38<04:11,  1.15s/it]\u001b[A\n",
            "Iteration:  53% 243/461 [04:39<04:11,  1.15s/it]\u001b[A\n",
            "Iteration:  53% 244/461 [04:40<04:11,  1.16s/it]\u001b[A\n",
            "Iteration:  53% 245/461 [04:42<04:09,  1.15s/it]\u001b[A\n",
            "Iteration:  53% 246/461 [04:43<04:08,  1.16s/it]\u001b[A\n",
            "Iteration:  54% 247/461 [04:44<04:06,  1.15s/it]\u001b[A\n",
            "Iteration:  54% 248/461 [04:45<04:04,  1.15s/it]\u001b[A\n",
            "Iteration:  54% 249/461 [04:46<04:02,  1.15s/it]\u001b[A\n",
            "Iteration:  54% 250/461 [04:47<04:02,  1.15s/it]\u001b[A\n",
            "Iteration:  54% 251/461 [04:48<04:01,  1.15s/it]\u001b[A\n",
            "Iteration:  55% 252/461 [04:50<04:00,  1.15s/it]\u001b[A\n",
            "Iteration:  55% 253/461 [04:51<03:59,  1.15s/it]\u001b[A\n",
            "Iteration:  55% 254/461 [04:52<03:57,  1.15s/it]\u001b[A\n",
            "Iteration:  55% 255/461 [04:53<03:55,  1.14s/it]\u001b[A\n",
            "Iteration:  56% 256/461 [04:54<03:54,  1.14s/it]\u001b[A\n",
            "Iteration:  56% 257/461 [04:55<03:53,  1.15s/it]\u001b[A\n",
            "Iteration:  56% 258/461 [04:57<03:53,  1.15s/it]\u001b[A\n",
            "Iteration:  56% 259/461 [04:58<03:51,  1.15s/it]\u001b[A\n",
            "Iteration:  56% 260/461 [04:59<03:50,  1.15s/it]\u001b[A\n",
            "Iteration:  57% 261/461 [05:00<03:48,  1.14s/it]\u001b[A\n",
            "Iteration:  57% 262/461 [05:01<03:47,  1.14s/it]\u001b[A\n",
            "Iteration:  57% 263/461 [05:02<03:46,  1.14s/it]\u001b[A\n",
            "Iteration:  57% 264/461 [05:03<03:46,  1.15s/it]\u001b[A\n",
            "Iteration:  57% 265/461 [05:05<03:45,  1.15s/it]\u001b[A\n",
            "Iteration:  58% 266/461 [05:06<03:44,  1.15s/it]\u001b[A\n",
            "Iteration:  58% 267/461 [05:07<03:42,  1.15s/it]\u001b[A\n",
            "Iteration:  58% 268/461 [05:08<03:41,  1.15s/it]\u001b[A\n",
            "Iteration:  58% 269/461 [05:09<03:39,  1.14s/it]\u001b[A\n",
            "Iteration:  59% 270/461 [05:10<03:38,  1.14s/it]\u001b[A\n",
            "Iteration:  59% 271/461 [05:11<03:36,  1.14s/it]\u001b[A\n",
            "Iteration:  59% 272/461 [05:13<03:35,  1.14s/it]\u001b[A\n",
            "Iteration:  59% 273/461 [05:14<03:35,  1.15s/it]\u001b[A\n",
            "Iteration:  59% 274/461 [05:15<03:34,  1.15s/it]\u001b[A\n",
            "Iteration:  60% 275/461 [05:16<03:33,  1.15s/it]\u001b[A\n",
            "Iteration:  60% 276/461 [05:17<03:31,  1.15s/it]\u001b[A\n",
            "Iteration:  60% 277/461 [05:18<03:31,  1.15s/it]\u001b[A\n",
            "Iteration:  60% 278/461 [05:19<03:29,  1.15s/it]\u001b[A\n",
            "Iteration:  61% 279/461 [05:21<03:27,  1.14s/it]\u001b[A\n",
            "Iteration:  61% 280/461 [05:22<03:27,  1.14s/it]\u001b[A\n",
            "Iteration:  61% 281/461 [05:23<03:25,  1.14s/it]\u001b[A\n",
            "Iteration:  61% 282/461 [05:24<03:24,  1.14s/it]\u001b[A\n",
            "Iteration:  61% 283/461 [05:25<03:22,  1.14s/it]\u001b[A\n",
            "Iteration:  62% 284/461 [05:26<03:21,  1.14s/it]\u001b[A\n",
            "Iteration:  62% 285/461 [05:27<03:20,  1.14s/it]\u001b[A\n",
            "Iteration:  62% 286/461 [05:29<03:19,  1.14s/it]\u001b[A\n",
            "Iteration:  62% 287/461 [05:30<03:19,  1.14s/it]\u001b[A\n",
            "Iteration:  62% 288/461 [05:31<03:18,  1.15s/it]\u001b[A\n",
            "Iteration:  63% 289/461 [05:32<03:17,  1.15s/it]\u001b[A\n",
            "Iteration:  63% 290/461 [05:33<03:16,  1.15s/it]\u001b[A\n",
            "Iteration:  63% 291/461 [05:34<03:14,  1.15s/it]\u001b[A\n",
            "Iteration:  63% 292/461 [05:35<03:13,  1.15s/it]\u001b[A\n",
            "Iteration:  64% 293/461 [05:37<03:13,  1.15s/it]\u001b[A\n",
            "Iteration:  64% 294/461 [05:38<03:11,  1.15s/it]\u001b[A\n",
            "Iteration:  64% 295/461 [05:39<03:09,  1.14s/it]\u001b[A\n",
            "Iteration:  64% 296/461 [05:40<03:09,  1.15s/it]\u001b[A\n",
            "Iteration:  64% 297/461 [05:41<03:08,  1.15s/it]\u001b[A\n",
            "Iteration:  65% 298/461 [05:42<03:06,  1.14s/it]\u001b[A\n",
            "Iteration:  65% 299/461 [05:43<03:05,  1.15s/it]\u001b[A\n",
            "Iteration:  65% 300/461 [05:45<03:05,  1.15s/it]\u001b[A\n",
            "Iteration:  65% 301/461 [05:46<03:04,  1.15s/it]\u001b[A\n",
            "Iteration:  66% 302/461 [05:47<03:02,  1.15s/it]\u001b[A\n",
            "Iteration:  66% 303/461 [05:48<03:01,  1.15s/it]\u001b[A\n",
            "Iteration:  66% 304/461 [05:49<02:59,  1.14s/it]\u001b[A\n",
            "Iteration:  66% 305/461 [05:50<02:59,  1.15s/it]\u001b[A\n",
            "Iteration:  66% 306/461 [05:51<02:57,  1.14s/it]\u001b[A\n",
            "Iteration:  67% 307/461 [05:53<02:55,  1.14s/it]\u001b[A\n",
            "Iteration:  67% 308/461 [05:54<02:55,  1.15s/it]\u001b[A\n",
            "Iteration:  67% 309/461 [05:55<02:53,  1.14s/it]\u001b[A\n",
            "Iteration:  67% 310/461 [05:56<02:52,  1.14s/it]\u001b[A\n",
            "Iteration:  67% 311/461 [05:57<02:51,  1.14s/it]\u001b[A\n",
            "Iteration:  68% 312/461 [05:58<02:50,  1.14s/it]\u001b[A\n",
            "Iteration:  68% 313/461 [06:00<02:49,  1.15s/it]\u001b[A\n",
            "Iteration:  68% 314/461 [06:01<02:48,  1.14s/it]\u001b[A\n",
            "Iteration:  68% 315/461 [06:02<02:48,  1.15s/it]\u001b[A\n",
            "Iteration:  69% 316/461 [06:03<02:46,  1.15s/it]\u001b[A\n",
            "Iteration:  69% 317/461 [06:04<02:45,  1.15s/it]\u001b[A\n",
            "Iteration:  69% 318/461 [06:05<02:43,  1.15s/it]\u001b[A\n",
            "Iteration:  69% 319/461 [06:06<02:43,  1.15s/it]\u001b[A\n",
            "Iteration:  69% 320/461 [06:08<02:41,  1.15s/it]\u001b[A\n",
            "Iteration:  70% 321/461 [06:09<02:39,  1.14s/it]\u001b[A\n",
            "Iteration:  70% 322/461 [06:10<02:38,  1.14s/it]\u001b[A\n",
            "Iteration:  70% 323/461 [06:11<02:37,  1.14s/it]\u001b[A\n",
            "Iteration:  70% 324/461 [06:12<02:36,  1.14s/it]\u001b[A\n",
            "Iteration:  70% 325/461 [06:13<02:36,  1.15s/it]\u001b[A\n",
            "Iteration:  71% 326/461 [06:14<02:34,  1.15s/it]\u001b[A\n",
            "Iteration:  71% 327/461 [06:16<02:34,  1.15s/it]\u001b[A\n",
            "Iteration:  71% 328/461 [06:17<02:32,  1.15s/it]\u001b[A\n",
            "Iteration:  71% 329/461 [06:18<02:31,  1.15s/it]\u001b[A\n",
            "Iteration:  72% 330/461 [06:19<02:29,  1.14s/it]\u001b[A\n",
            "Iteration:  72% 331/461 [06:20<02:28,  1.14s/it]\u001b[A\n",
            "Iteration:  72% 332/461 [06:21<02:27,  1.14s/it]\u001b[A\n",
            "Iteration:  72% 333/461 [06:22<02:26,  1.15s/it]\u001b[A\n",
            "Iteration:  72% 334/461 [06:24<02:26,  1.15s/it]\u001b[A\n",
            "Iteration:  73% 335/461 [06:25<02:24,  1.15s/it]\u001b[A\n",
            "Iteration:  73% 336/461 [06:26<02:23,  1.15s/it]\u001b[A\n",
            "Iteration:  73% 337/461 [06:27<02:22,  1.15s/it]\u001b[A\n",
            "Iteration:  73% 338/461 [06:28<02:20,  1.14s/it]\u001b[A\n",
            "Iteration:  74% 339/461 [06:29<02:19,  1.15s/it]\u001b[A\n",
            "Iteration:  74% 340/461 [06:30<02:19,  1.15s/it]\u001b[A\n",
            "Iteration:  74% 341/461 [06:32<02:17,  1.15s/it]\u001b[A\n",
            "Iteration:  74% 342/461 [06:33<02:16,  1.15s/it]\u001b[A\n",
            "Iteration:  74% 343/461 [06:34<02:15,  1.15s/it]\u001b[A\n",
            "Iteration:  75% 344/461 [06:35<02:14,  1.15s/it]\u001b[A\n",
            "Iteration:  75% 345/461 [06:36<02:13,  1.15s/it]\u001b[A\n",
            "Iteration:  75% 346/461 [06:37<02:11,  1.15s/it]\u001b[A\n",
            "Iteration:  75% 347/461 [06:38<02:11,  1.15s/it]\u001b[A\n",
            "Iteration:  75% 348/461 [06:40<02:09,  1.15s/it]\u001b[A\n",
            "Iteration:  76% 349/461 [06:41<02:08,  1.15s/it]\u001b[A\n",
            "Iteration:  76% 350/461 [06:42<02:07,  1.15s/it]\u001b[A\n",
            "Iteration:  76% 351/461 [06:43<02:06,  1.15s/it]\u001b[A\n",
            "Iteration:  76% 352/461 [06:44<02:05,  1.15s/it]\u001b[A\n",
            "Iteration:  77% 353/461 [06:45<02:03,  1.15s/it]\u001b[A\n",
            "Iteration:  77% 354/461 [06:47<02:02,  1.15s/it]\u001b[A\n",
            "Iteration:  77% 355/461 [06:48<02:01,  1.15s/it]\u001b[A\n",
            "Iteration:  77% 356/461 [06:49<02:00,  1.15s/it]\u001b[A\n",
            "Iteration:  77% 357/461 [06:50<01:58,  1.14s/it]\u001b[A\n",
            "Iteration:  78% 358/461 [06:51<01:57,  1.14s/it]\u001b[A\n",
            "Iteration:  78% 359/461 [06:52<01:56,  1.15s/it]\u001b[A\n",
            "Iteration:  78% 360/461 [06:53<01:55,  1.15s/it]\u001b[A\n",
            "Iteration:  78% 361/461 [06:55<01:54,  1.14s/it]\u001b[A\n",
            "Iteration:  79% 362/461 [06:56<01:53,  1.14s/it]\u001b[A\n",
            "Iteration:  79% 363/461 [06:57<01:52,  1.15s/it]\u001b[A\n",
            "Iteration:  79% 364/461 [06:58<01:51,  1.15s/it]\u001b[A\n",
            "Iteration:  79% 365/461 [06:59<01:50,  1.15s/it]\u001b[A\n",
            "Iteration:  79% 366/461 [07:00<01:49,  1.15s/it]\u001b[A\n",
            "Iteration:  80% 367/461 [07:01<01:47,  1.15s/it]\u001b[A\n",
            "Iteration:  80% 368/461 [07:03<01:46,  1.15s/it]\u001b[A\n",
            "Iteration:  80% 369/461 [07:04<01:45,  1.14s/it]\u001b[A\n",
            "Iteration:  80% 370/461 [07:05<01:43,  1.14s/it]\u001b[A\n",
            "Iteration:  80% 371/461 [07:06<01:43,  1.15s/it]\u001b[A\n",
            "Iteration:  81% 372/461 [07:07<01:42,  1.15s/it]\u001b[A\n",
            "Iteration:  81% 373/461 [07:08<01:40,  1.14s/it]\u001b[A\n",
            "Iteration:  81% 374/461 [07:09<01:39,  1.14s/it]\u001b[A\n",
            "Iteration:  81% 375/461 [07:11<01:38,  1.14s/it]\u001b[A\n",
            "Iteration:  82% 376/461 [07:12<01:36,  1.14s/it]\u001b[A\n",
            "Iteration:  82% 377/461 [07:13<01:36,  1.15s/it]\u001b[A\n",
            "Iteration:  82% 378/461 [07:14<01:34,  1.14s/it]\u001b[A\n",
            "Iteration:  82% 379/461 [07:15<01:34,  1.15s/it]\u001b[A\n",
            "Iteration:  82% 380/461 [07:16<01:33,  1.15s/it]\u001b[A\n",
            "Iteration:  83% 381/461 [07:17<01:31,  1.15s/it]\u001b[A\n",
            "Iteration:  83% 382/461 [07:19<01:30,  1.15s/it]\u001b[A\n",
            "Iteration:  83% 383/461 [07:20<01:29,  1.14s/it]\u001b[A\n",
            "Iteration:  83% 384/461 [07:21<01:28,  1.15s/it]\u001b[A\n",
            "Iteration:  84% 385/461 [07:22<01:26,  1.14s/it]\u001b[A\n",
            "Iteration:  84% 386/461 [07:23<01:26,  1.15s/it]\u001b[A\n",
            "Iteration:  84% 387/461 [07:24<01:24,  1.15s/it]\u001b[A\n",
            "Iteration:  84% 388/461 [07:25<01:23,  1.15s/it]\u001b[A\n",
            "Iteration:  84% 389/461 [07:27<01:22,  1.15s/it]\u001b[A\n",
            "Iteration:  85% 390/461 [07:28<01:21,  1.15s/it]\u001b[A\n",
            "Iteration:  85% 391/461 [07:29<01:20,  1.15s/it]\u001b[A\n",
            "Iteration:  85% 392/461 [07:30<01:19,  1.15s/it]\u001b[A\n",
            "Iteration:  85% 393/461 [07:31<01:18,  1.15s/it]\u001b[A\n",
            "Iteration:  85% 394/461 [07:32<01:17,  1.15s/it]\u001b[A\n",
            "Iteration:  86% 395/461 [07:34<01:15,  1.15s/it]\u001b[A\n",
            "Iteration:  86% 396/461 [07:35<01:14,  1.15s/it]\u001b[A\n",
            "Iteration:  86% 397/461 [07:36<01:13,  1.14s/it]\u001b[A\n",
            "Iteration:  86% 398/461 [07:37<01:12,  1.15s/it]\u001b[A\n",
            "Iteration:  87% 399/461 [07:38<01:10,  1.15s/it]\u001b[A\n",
            "Iteration:  87% 400/461 [07:39<01:09,  1.14s/it]\u001b[A\n",
            "Iteration:  87% 401/461 [07:40<01:08,  1.14s/it]\u001b[A\n",
            "Iteration:  87% 402/461 [07:42<01:07,  1.15s/it]\u001b[A\n",
            "Iteration:  87% 403/461 [07:43<01:06,  1.15s/it]\u001b[A\n",
            "Iteration:  88% 404/461 [07:44<01:05,  1.14s/it]\u001b[A\n",
            "Iteration:  88% 405/461 [07:45<01:04,  1.14s/it]\u001b[A\n",
            "Iteration:  88% 406/461 [07:46<01:03,  1.15s/it]\u001b[A\n",
            "Iteration:  88% 407/461 [07:47<01:01,  1.15s/it]\u001b[A\n",
            "Iteration:  89% 408/461 [07:48<01:00,  1.15s/it]\u001b[A\n",
            "Iteration:  89% 409/461 [07:50<00:59,  1.15s/it]\u001b[A\n",
            "Iteration:  89% 410/461 [07:51<00:58,  1.15s/it]\u001b[A\n",
            "Iteration:  89% 411/461 [07:52<00:57,  1.15s/it]\u001b[A\n",
            "Iteration:  89% 412/461 [07:53<00:56,  1.15s/it]\u001b[A\n",
            "Iteration:  90% 413/461 [07:54<00:55,  1.15s/it]\u001b[A\n",
            "Iteration:  90% 414/461 [07:55<00:53,  1.15s/it]\u001b[A\n",
            "Iteration:  90% 415/461 [07:56<00:52,  1.15s/it]\u001b[A\n",
            "Iteration:  90% 416/461 [07:58<00:51,  1.15s/it]\u001b[A\n",
            "Iteration:  90% 417/461 [07:59<00:50,  1.15s/it]\u001b[A\n",
            "Iteration:  91% 418/461 [08:00<00:49,  1.15s/it]\u001b[A\n",
            "Iteration:  91% 419/461 [08:01<00:48,  1.14s/it]\u001b[A\n",
            "Iteration:  91% 420/461 [08:02<00:47,  1.15s/it]\u001b[A\n",
            "Iteration:  91% 421/461 [08:03<00:45,  1.14s/it]\u001b[A\n",
            "Iteration:  92% 422/461 [08:04<00:44,  1.15s/it]\u001b[A\n",
            "Iteration:  92% 423/461 [08:06<00:43,  1.14s/it]\u001b[A\n",
            "Iteration:  92% 424/461 [08:07<00:42,  1.15s/it]\u001b[A\n",
            "Iteration:  92% 425/461 [08:08<00:41,  1.15s/it]\u001b[A\n",
            "Iteration:  92% 426/461 [08:09<00:40,  1.15s/it]\u001b[A\n",
            "Iteration:  93% 427/461 [08:10<00:38,  1.15s/it]\u001b[A\n",
            "Iteration:  93% 428/461 [08:11<00:37,  1.15s/it]\u001b[A\n",
            "Iteration:  93% 429/461 [08:13<00:36,  1.15s/it]\u001b[A\n",
            "Iteration:  93% 430/461 [08:14<00:35,  1.14s/it]\u001b[A\n",
            "Iteration:  93% 431/461 [08:15<00:34,  1.14s/it]\u001b[A\n",
            "Iteration:  94% 432/461 [08:16<00:33,  1.14s/it]\u001b[A\n",
            "Iteration:  94% 433/461 [08:17<00:31,  1.14s/it]\u001b[A\n",
            "Iteration:  94% 434/461 [08:18<00:30,  1.14s/it]\u001b[A\n",
            "Iteration:  94% 435/461 [08:19<00:29,  1.14s/it]\u001b[A\n",
            "Iteration:  95% 436/461 [08:20<00:28,  1.14s/it]\u001b[A\n",
            "Iteration:  95% 437/461 [08:22<00:27,  1.14s/it]\u001b[A\n",
            "Iteration:  95% 438/461 [08:23<00:26,  1.15s/it]\u001b[A\n",
            "Iteration:  95% 439/461 [08:24<00:25,  1.15s/it]\u001b[A\n",
            "Iteration:  95% 440/461 [08:25<00:24,  1.15s/it]\u001b[A\n",
            "Iteration:  96% 441/461 [08:26<00:22,  1.14s/it]\u001b[A\n",
            "Iteration:  96% 442/461 [08:27<00:21,  1.15s/it]\u001b[A\n",
            "Iteration:  96% 443/461 [08:29<00:20,  1.14s/it]\u001b[A\n",
            "Iteration:  96% 444/461 [08:30<00:19,  1.15s/it]\u001b[A\n",
            "Iteration:  97% 445/461 [08:31<00:18,  1.15s/it]\u001b[A\n",
            "Iteration:  97% 446/461 [08:32<00:17,  1.15s/it]\u001b[A\n",
            "Iteration:  97% 447/461 [08:33<00:16,  1.15s/it]\u001b[A\n",
            "Iteration:  97% 448/461 [08:34<00:14,  1.15s/it]\u001b[A\n",
            "Iteration:  97% 449/461 [08:35<00:13,  1.15s/it]\u001b[A\n",
            "Iteration:  98% 450/461 [08:37<00:12,  1.15s/it]\u001b[A\n",
            "Iteration:  98% 451/461 [08:38<00:11,  1.15s/it]\u001b[A\n",
            "Iteration:  98% 452/461 [08:39<00:10,  1.15s/it]\u001b[A\n",
            "Iteration:  98% 453/461 [08:40<00:09,  1.15s/it]\u001b[A\n",
            "Iteration:  98% 454/461 [08:41<00:08,  1.15s/it]\u001b[A\n",
            "Iteration:  99% 455/461 [08:42<00:06,  1.15s/it]\u001b[A\n",
            "Iteration:  99% 456/461 [08:43<00:05,  1.15s/it]\u001b[A\n",
            "Iteration:  99% 457/461 [08:45<00:04,  1.15s/it]\u001b[A\n",
            "Iteration:  99% 458/461 [08:46<00:03,  1.15s/it]\u001b[A\n",
            "Iteration: 100% 459/461 [08:47<00:02,  1.14s/it]\u001b[A\n",
            "Iteration: 100% 460/461 [08:48<00:01,  1.14s/it]\u001b[A\n",
            "Iteration: 100% 461/461 [08:49<00:00,  1.14s/it]\u001b[A\n",
            "Epoch: 100% 1/1 [08:49<00:00, 529.68s/it]\n",
            "03/05/2020 17:55:42 - INFO - __main__ -    global_step = 461, average loss = 0.8073607676717051\n",
            "03/05/2020 17:55:42 - INFO - __main__ -   Saving model checkpoint to output_roberta_clinton\n",
            "03/05/2020 17:55:42 - INFO - transformers.configuration_utils -   Configuration saved in output_roberta_clinton/config.json\n",
            "03/05/2020 17:55:43 - INFO - transformers.modeling_utils -   Model weights saved in output_roberta_clinton/pytorch_model.bin\n",
            "03/05/2020 17:55:43 - INFO - transformers.configuration_utils -   loading configuration file output_roberta_clinton/config.json\n",
            "03/05/2020 17:55:43 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "03/05/2020 17:55:43 - INFO - transformers.modeling_utils -   loading weights file output_roberta_clinton/pytorch_model.bin\n",
            "03/05/2020 17:55:49 - INFO - transformers.tokenization_utils -   Model name 'output_roberta_clinton' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'output_roberta_clinton' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/05/2020 17:55:49 - INFO - transformers.tokenization_utils -   Didn't find file output_roberta_clinton/added_tokens.json. We won't load it.\n",
            "03/05/2020 17:55:49 - INFO - transformers.tokenization_utils -   loading file output_roberta_clinton/vocab.json\n",
            "03/05/2020 17:55:49 - INFO - transformers.tokenization_utils -   loading file output_roberta_clinton/merges.txt\n",
            "03/05/2020 17:55:49 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/05/2020 17:55:49 - INFO - transformers.tokenization_utils -   loading file output_roberta_clinton/special_tokens_map.json\n",
            "03/05/2020 17:55:49 - INFO - transformers.tokenization_utils -   loading file output_roberta_clinton/tokenizer_config.json\n",
            "03/05/2020 17:55:49 - INFO - __main__ -   Evaluate the following checkpoints: ['output_roberta_clinton']\n",
            "03/05/2020 17:55:49 - INFO - transformers.configuration_utils -   loading configuration file output_roberta_clinton/config.json\n",
            "03/05/2020 17:55:49 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "03/05/2020 17:55:49 - INFO - transformers.modeling_utils -   loading weights file output_roberta_clinton/pytorch_model.bin\n",
            "03/05/2020 17:55:54 - INFO - __main__ -   Creating features from dataset file at \n",
            "03/05/2020 17:55:56 - INFO - __main__ -   Saving features into cached file roberta_cached_lm_510_test_text_clinton\n",
            "03/05/2020 17:55:56 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "03/05/2020 17:55:56 - INFO - __main__ -     Num examples = 476\n",
            "03/05/2020 17:55:56 - INFO - __main__ -     Batch size = 4\n",
            "Evaluating: 100% 119/119 [00:44<00:00,  2.67it/s]\n",
            "03/05/2020 17:56:40 - INFO - __main__ -   ***** Eval results  *****\n",
            "03/05/2020 17:56:40 - INFO - __main__ -     perplexity = tensor(1.9786)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIf8165QfqkU",
        "colab_type": "text"
      },
      "source": [
        "#### Similarities/Differences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwDQv5cyfob8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer\n",
        "roberta_obama_model_embedding = RobertaModel.from_pretrained('output_roberta_obama')\n",
        "roberta_obama_tokenizer = RobertaTokenizer.from_pretrained('output_roberta_obama')\n",
        "\n",
        "roberta_clinton_model_embedding = RobertaModel.from_pretrained('output_roberta_clinton')\n",
        "roberta_clinton_tokenizer = RobertaTokenizer.from_pretrained('output_roberta_clinton')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gQ1-5icfogo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# need to come up with a sentence\n",
        "text = \"The United States should consider withdrawing troops from Iraq and Afghanistan and ending the war against terrorism\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2mZ5hzTfomz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "478764e4-5e15-44b9-e67f-9967d0d7c464"
      },
      "source": [
        "visualise_diffs(text, roberta_obama_model_embedding, roberta_obama_tokenizer)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEwCAYAAAB7fzxbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpHmaeY4fouR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "ce4ea298-e2b2-44e3-efa5-750ce41446e4"
      },
      "source": [
        "visualise_diffs(text, roberta_clinton_model_embedding, roberta_clinton_tokenizer)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEwCAYAAAB7fzxbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnJOLmZl1nJOLmZl13P+SMsfrT98WmgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS21MnCD3nOE",
        "colab_type": "text"
      },
      "source": [
        "Obama and Clinton turn out to be not that different in terms of their foreign policy stances on Iraq and Afghanistan although they started off with completely different positions. We know for a fact that Clinton shifted her position from supporting the war to ending it. Overall, the heatmaps show a similar pattern across the two then senators. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46JPmKEzBcJ1",
        "colab_type": "text"
      },
      "source": [
        "### Post-training\n",
        "\n",
        "After you finish training your models, you can download them by right clicking on each of the 8 files inside the name of your output folder. The largest file, pytorch_model.bin, is the file which does most of the heavy lifting but you need the other files to be able to use it later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtcKAEMJD3sX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}